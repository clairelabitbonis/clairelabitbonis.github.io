<!doctype html><html><head><title>DLCV-TP-10* | Voyons...</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png><meta property="og:title" content="DLCV-TP-10* | Voyons..."><meta property="og:description" content="*oui, je compte en binaire. On sait s&rsquo;amuser par ici.
Previously on &ldquo;DLCV Practical sessions&rdquo;&mldr; Il est beau le dataset Répartition des classes Grâce à nous tou·te·s, on a construit un beau dataset qui nous permet de détecter plein de classes d&rsquo;objets très utiles. Le tableau ci-dessous indique le nombre d&rsquo;images et la répartition des différentes classes entre les ensembles de train, validation et test :
Analyse des labels La quantité de labels par image et leur forme varie en fonction des classes annotées."><meta property="og:type" content="article"><meta property="og:url" content="http://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/03_lets_see/"><meta property="og:image" content="http://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/03_lets_see/featured.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-14T10:00:00+09:00"><meta property="article:modified_time" content="2022-12-14T10:00:00+09:00"><meta name=description content="DLCV-TP-10* | Voyons..."><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png alt=Logo>
Claire Labit-Bonis</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20>
</a><a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20>
</a><a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Chercher data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Articles</a></li><div class=subtree><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/>Enseignements</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/3d_perception/>Perception 3D</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/>Travaux pratiques</a><ul><li><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/cc_segmentation/ title=Segmentation>Segmentation</a></li><li><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/ title="Localisation mono.">Localisation mono.</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/>Deep Learning & CV</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/>Cours</a><ul><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/01_why_deep_learning/ title="00 | De l'IA au DL">00 | De l'IA au DL</a></li><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/02_lets_learn_deeply/ title="01 | DD into OD">01 | DD into OD</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/>Travaux pratiques</a><ul class=active><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/>2022-2023</a><ul class=active><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/00_presentation/ title="00 | Présentation">00 | Présentation</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/02_yolo/ title="01 | YOLO !">01 | YOLO !</a></li><li><a class=active href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/03_lets_see/ title="10 | Voyons...">10 | Voyons...</a></li></ul></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/ title=2023-2024>2023-2024</a></li></ul></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/03_lets_see/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/claire_hufa4d5753e97ed9e5e63c04dfec44ec48_673603_120x120_fit_q75_box.jpg alt="Author Image"><h5 class=author-name>Claire Labit-Bonis</h5><p>mercredi 14 décembre 2022</p></div><div class=title><h1>DLCV-TP-10* | Voyons...</h1></div><div class=post-content id=post-content><p>*<em>oui, je compte en binaire. On sait s&rsquo;amuser par ici.</em></p><h2 id=previously-on-dlcv-practical-sessions><strong>Previously on &ldquo;DLCV Practical sessions&rdquo;&mldr;</strong></h2><center><img src=images/previously_on_dlcv.png alt="Previously on DLCV practical sessions" width=70%></center><h2 id=il-est-beau-le-dataset>Il est beau le <em>dataset</em></h2><h3 id=répartition-des-classes>Répartition des classes</h3><p>Grâce à nous tou·te·s, on a construit un beau <em>dataset</em> qui nous permet de détecter plein de classes d&rsquo;objets très utiles.
Le tableau ci-dessous indique le nombre d&rsquo;images et la répartition des différentes classes entre les ensembles de <em>train</em>, <em>validation</em> et <em>test</em> :</p><center><img src=images/dataset.png alt=Dataset width=80%></center><h3 id=analyse-des-labels>Analyse des labels</h3><p>La quantité de labels par image et leur forme varie en fonction des classes annotées. Par exemple, les images de <code>lentilles</code> contiennent beaucoup plus d&rsquo;instances que la classe <code>lunettes</code>, pour un nombre équivalent d&rsquo;images. Par ailleurs, les objets sont majoritairement centrés dans l&rsquo;image même si toutes les positions sont représentées. Leur taille quant à elle diffère d&rsquo;une classe à l&rsquo;autre, bien que les classes <code>lentilles</code> et <code>bague</code> génèrent un point chaud dans le coin en bas à gauche, correspondant aux objets de petite taille.</p><center><img src=images/labels.png alt=Labels width=50%></center><h2 id=metal-une-semaine-pour-tout-changer-metal>&#x1f918; Une semaine pour tout changer &#x1f918;</h2><h3 id=ce-que-vous-navez-pas-vu>Ce que vous n&rsquo;avez pas vu&mldr;</h3><center><img src=images/this_is_fine.png alt="This is fine" width=50%></center><ul><li><mark><strong>encore plus de <em>data processing</em></strong></mark> : après l&rsquo;<em>upload</em> de toutes vos données sur le serveur la semaine dernière, il a fallu les homogénéiser. Certains fichiers contenaient des espaces en trop, les <em>splits</em> n&rsquo;étaient pas toujours formatés de la même manière, l&rsquo;arborescence était parfois à réorganiser, <em>etc</em>.</li><li><mark><em><strong>get ready for training</strong></em></mark> : une fois le <em>dataset</em> formaté, il faut préparer YOLO pour l&rsquo;apprentissage, et notamment écrire le fichier de configuration qui va bien pour un apprentissage sur des données <em>custom</em> :</li></ul><center><img src=images/fichier_config_yolo.png alt=Labels width=80%></center><p> </p><ul><li><mark><strong>la lente agonie des GPUs</strong></mark> : à l&rsquo;heure où j&rsquo;écris ces lignes (14 décembre à ~minuit), nous en sommes à 44 heures d&rsquo;apprentissage, réparties sur les 8 GPUs des serveurs 1 et 2 de l&rsquo;INSA, et chaque GPU a 8Go de mémoire :</li></ul><center><img src=images/gpu1_gpu2.jpg alt="GPU1 et GPU2 sont sur un bateau" height=400px>
<img src=images/nvidia_smi.jpg alt=nvidia-smi height=400px></center>&nbsp;<center><img src=images/tensorboard.png alt=Tensorboard width=65%></center>&nbsp;<ul><li><mark><strong>raté, on recommence &ndash; ou &ldquo;ce qui était prévu mais qui n&rsquo;a pas marché&rdquo;</strong></mark> : le VPN se déconnecte, la connexion SSH se ferme et arrête une liste d&rsquo;apprentissages fièrement lancés pour tourner toute la nuit (on t&rsquo;avait pourtant prévenue de faire un <code>nohup</code> ou d&rsquo;utiliser <code>tmux</code>) ; les classes ne sont pas les bonnes, le <em>batch</em> est trop gros et explose la mémoire, le <em>batch</em> est trop petit et explose le temps, le réseau est trop lourd et fait exploser CUDA.</li></ul><h3 id=configurations-entrainées>Configurations entrainées</h3><p>Les apprentissages faits ces derniers jours couvrent les configurations suivantes (en vert ce qui est fait, en jaune ce qui est en cours, en rouge ce qu&rsquo;il reste à faire) :</p><center><img src=images/trainings.png alt="Configurations d'apprentissage" width=80%></center>&nbsp;<p>Il était prévu de travailler avec différentes tailles de <em>batch</em>, en l&rsquo;occurrence 1 et 32. Finalement, un <em>batch</em> de 1 prend beaucoup trop de temps ; toutes les configurations ont été entrainées avec un <em>batch</em> de 64, voire de 96.</p><p><a href=https://github.com/ultralytics/yolov5#pretrained-checkpoints>Quatre versions</a> de YOLO ont finalement été testées, avec de la plus petite à la plus grande : <code>n</code>, <code>s</code>, <code>m</code>, et <code>l</code>. Le détail de ces architectures est visible soit dans Tensorboard en visualisant le graphe d&rsquo;une configuration correspondant à la version choisie, soit directement dans les fichiers <code>.yaml</code> du dossier <code>models</code> dans le <em>workspace</em> yolov5.</p><h4 id=transfer-learning-fine-tuning-pre-trained-models><em>Transfer learning, fine-tuning, pre-trained models&mldr;</em></h4><p><code>freeze</code> veut dire qu&rsquo;en début d&rsquo;apprentissage, les poids sont initialisés avec un modèle pré-entrainé sur le <em>dataset</em> COCO, et que les poids de certaines courches ne sont pas mis à jour pendant l&rsquo;apprentissage. Dans notre cas, on <em>freeze</em> le <em>backbone</em>. On &ldquo;transfère&rdquo; également la tâche apprise par le réseau pré-entrainé vers une nouvelle tâche, en changeant le nombre et le type des classes à prédire dans notre cas. On dit également qu&rsquo;on &ldquo;affine&rdquo; l&rsquo;extraction des caractéristiques pré-entrainée en mettant à jour tout ou partie des poids pendant l&rsquo;apprentissage.</p><h2 id=fire-and-now-what-fire>&#x1f525; <em>And now, what?</em> &#x1f525;</h2><p>On va de nouveau tirer au sort les configurations sur lesquelles vous vous positionnerez, puisque rien ne s&rsquo;est passé comme prévu pendant les apprentissages. Mais on est agiles, on s&rsquo;adapte. Le plan sera ensuite le suivant :</p><ul><li>vous mettrez en place votre <em>workspace</em>, à savoir : le clone de yolov5 dans votre <em>home directory</em>, ouvert dans VS Code avec un accès en SSH au serveur GPU <code>srv-gei-gpu1</code> grâce à l&rsquo;extension &ldquo;Remote SSH&rdquo; (<code>Ctrl+Shitf+X</code> pour ouvrir le gestionnaire d&rsquo;extensions), ainsi que l&rsquo;activation de l&rsquo;environnement virtuel qui va bien &ndash; cf. partie &ldquo;configuration du <em>workspace</em>&rdquo; ;</li><li>vous pouvez lancer un <code>tensorboard --logdir=/scratch/labi/DLCV/train</code> pour visualiser et comparer les différents apprentissages réalisés ;</li><li>vous pouvez également ouvrir <em>via</em> VS Code en SSH le dossier <code>/scratch/labi/DLCV/train</code> en question, et ainsi accéder à tous les graphiques de performance générés par YOLO pour chaque configuration d&rsquo;apprentissage (matrices de confusion, courbes de précision-rappel, F1-score <em>vs.</em> score de confiance, <em>etc</em>.) ;</li><li>pour évaluer plus en détails votre configuration (notamment sur la base de test), il vous faudra copier le fichier <code>/scratch/labi/DLCV/dataset/dlcv.yaml</code> dans votre dossier <code>path/to/yolov5/data/</code> puis appeler le script <code>val.py</code> avec les paramètres qui vous intéressent. Plusieurs tâches d&rsquo;évaluations sont possibles : <code>train</code>, <code>val</code>, <code>test</code>, <code>speed</code>, <code>study</code>. Par exemple, l&rsquo;évaluation sur base de test pour la configuration ;</li><li>évaluez également les modèles de manière qualitative, c&rsquo;est-à-dire en visualisant directement les résultats, &ldquo;à l&rsquo;oeil&rdquo;. Pour cela, lancez <code>detect.py</code> sur une nouvelle vidéo par exemple (pas besoin d&rsquo;extraire les <em>frames</em>, le script prend aussi des vidéos <code>.mp4</code> en entrée). Le résultat est stocké dans <code>path/to/yolo/runs/detect/exp_name</code>. Vous pouvez changer <code>exp_name</code> avec l&rsquo;option <code>--name</code> (plus pratique pour s&rsquo;y retrouver que d&rsquo;avoir <code>exp1</code>, <code>exp2</code>, <code>exp3</code>&mldr;)</li></ul><p>Ne partez pas ! Vous vous souvenez des quêtes annexes du Bingo de YOLO ? Je vous les remets en mémoire juste au cas où&mldr; parce que maintenant que tout roule, vous pouvez lancer la détection avec des poids COCO, faire du pas à pas en mode <em>debug</em> pour aller chercher la valeur du poids, visualiser la <em>data augmentation</em> faite quand on lance <code>train.py</code>, <em>etc</em>.</p><center><img src=images/le_bingo_de_yolo.png alt="Le bingo de YOLO" width=80%></center><h2 id=annexe--configuration-du-workspace><strong>Annexe</strong> : configuration du <em>workspace</em></h2><p>Cette section vous guide dans la configuration de votre <em>workspace</em> avec les outils dont vous disposez en salle de TP. La configuration proposée se base sur un environnement Ubuntu 20.04, avec l&rsquo;IDE VSCode et la création d&rsquo;un environnement virtuel à l&rsquo;aide de <code>python venv</code>.
Vous êtes évidemment libres d&rsquo;utiliser n&rsquo;importe quel IDE si vous avez d&rsquo;autres préférences, ou d&rsquo;utiliser Anaconda pour créer votre environnement virtuel&mldr; le principal étant que ça marche !</p><p><em><strong>Let&rsquo;s go</strong></em> :</p><hr><ul><li><p>&#x1f525;&#x1f4bb; <mark><strong><em>étape 1</em></mark> : clone de YOLOv5</strong></p><blockquote><p><strong>L&rsquo;infrastructure INSA est bien faite &#x1f60d;</strong></p><p>Votre <em>home directory</em> est synchronisé entre les machines de TP et les serveurs GPU.
Ceci implique que vous pouvez travailler directement dans votre <em>home</em> depuis n&rsquo;importe où, et quand même bénéficier de l&rsquo;environnement matériel de la machine sur laquelle vous vous exécutez.</p><p>Pas besoin de transférer des tonnes de données en SSH depuis votre compte local vers le GPU pour lancer un apprentissage, pas besoin de dupliquer votre <em>clone</em> de YOLOv5 et de vous retrouver avec des espaces de travail différents&mldr; Travaillez directement dans votre <em>home</em>, les modifications que vous y faites suivront.</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e>## Clonage du dépôt Github de la release 6.2 de yolov5 dans votre home directory</span>
</span></span><span style=display:flex><span>login@machine:~$ cd &lt;path/to/workspace&gt;
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;$ git clone -b v6.2 https://github.com/ultralytics/yolov5.git
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;$ cd yolov5
</span></span></code></pre></div><p> </p></li><li><p>&#x1f525;&#x1f4bb; <mark><strong><em>étape 2</em></mark> : configuration de l&rsquo;environnement virtuel</strong></p><p>Ouvrez un nouveau terminal dans VS Code puis créez l&rsquo;environnement virtuel qui va bien :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e>## Configuration de l&#39;environnement virtuel nommé &#39;yolov5env&#39;</span>
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;/yolov5$ python3 -m venv yolov5env <span style=color:#75715e># Création</span>
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;/yolov5$ source yolov5env/bin/activate <span style=color:#75715e># Activation</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/workspace&gt;/yolov5$ python3 -m pip install --upgrade pip <span style=color:#75715e># Mise à jour de pip</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/workspace&gt;/yolov5$ pip3 install -r requirements.txt <span style=color:#75715e># Install libs</span>
</span></span></code></pre></div><p><em>A ce stade, toute l&rsquo;arborescence de YOLOv5 est en place, toutes les librairies sont installées.</em></p></li></ul><hr><ul><li><p>&#x1f525;&#x1f4bb; <mark><strong><em>étape 3</em></mark> : configuration de VS Code</strong></p><p>Assurez-vous ensuite que l&rsquo;extension pour Python est bien installée. Pour cela, accédez à l&rsquo;onglet &ldquo;Extensions&rdquo; <em>via</em> le raccourci <code>Ctrl + Shift + X</code> et cherchez <code>python</code>. Installez l&rsquo;extension si elle ne l&rsquo;est pas déjà :</p><center><p><img src=images/install_extension_python_vscode.png alt="Installation de l&rsquo;extension pour Python"></p></center><p>Sélectionnez ensuite l&rsquo;interpréteur Python de l&rsquo;environnement virtuel que vous avez créé à l&rsquo;étape 1, en utilisant le raccourci <code>Ctrl + Shift + P</code> pour faire apparaître la palette de commande, puis en tapant la commande <code>Python: Select Interpreter</code>. Parmi les choix proposés, cliquez sur celui correspondant à l&rsquo;environnement virtuel <code>yolov5env</code> :</p><center><p><img src=images/select_interpreter_vscode.png alt="Sélection de l&rsquo;interpréteur Python"></p></center></li></ul><hr><ul><li><p>&#x1f525;&#x1f4bb; <mark><strong><em>étape 4</em></mark> : voyons si vous avez suivi&mldr;</strong></p><p>Si tout est correctement configuré, vous pouvez lancer un terminal dans VS Code <em>via</em> <code>Terminal > New Terminal</code> et taper la commande suivante :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@srv-gpu:&lt;path/to/yolov5&gt;$ python detect.py --source <span style=color:#e6db74>&#39;https://ultralytics.com/images/zidane.jpg&#39;</span>
</span></span></code></pre></div><p>Une fois la commande exécutée, vous retrouvez le résultat de l&rsquo;éxecution du modèle YOLOv5-S sur l&rsquo;image <code>zidane.jpg</code> dans le dossier <code>runs/detect/exp</code> :</p><center><p><img src=images/zidane.png alt="Vérification du fonctionnement de YOLOv5"></p></center></li></ul><hr><p>&#x1f525;&#x1f386;&#x1f44d;&#x1f31f; <strong>Well done !</strong></p><h2 id=annexe--lancer-un-apprentissage-une-inférence><strong>Annexe</strong> : lancer un apprentissage, une inférence</h2><blockquote><p><strong>1 acheté = 1 offert</strong></p><p>Il y a DEUX serveurs GPU à l&rsquo;INSA ! accessibles <em>via</em> srv-gei-gpu1 et srv-gei-gpu2. Un serveur contient 4 GPUs. Vous pouvez lancer un apprentissage multi-GPUs sur un même serveur, et des apprentissages distincts en parallèle sur les deux serveurs (ce sont des machines physiques différentes, vous ne pourrez pas faire par exemple un apprentissage multi-GPUs sur 2*4 GPUs, mais vous pourrez lancer un apprentissage parallélisé sur les 4 GPUs du serveur 1, et un autre apprentissage parallélisé sur les 4 GPUs du serveur 2).</p></blockquote><h3 id=la-manière-dont-je-travaille-et-ce-nest-donc-que-mon-point-de-vue>La manière dont je travaille (et ce n&rsquo;est donc que mon point de vue)</h3><p>J&rsquo;ai toujours deux usages différents quand je lance des apprentissages ou une inférence :</p><ul><li>l&rsquo;usage en mode <em>debug</em> qui me permet de naviguer dans le <em>workspace</em>, d&rsquo;exécuter le code pas à pas, d&rsquo;avoir une vue pratique des fichiers que je manipule, etc. Ca me permet de tout mettre en place et d&rsquo;être sûre que tout fonctionne avant de lancer mes scripts ;</li><li>l&rsquo;usage en mode <em>release</em> : je lance tous les apprentissages, inférences, etc. d&rsquo;une traite, la plupart du temps en mode console. Si j&rsquo;ai besoin de monitorer les apprentissages, je lance <code>tensorboard</code> <em>via</em> VS Code en SSH sur srv-ens-calcul ou sur srv-gei-gpu selon la configuration dans laquelle je suis.</li></ul><h3 id=fichiers-nécessaires-pour-un-apprentissage-sur-données-custom>Fichiers nécessaires pour un apprentissage sur données <em>custom</em></h3><p>Pour pouvoir entrainer YOLOv5 comme on l&rsquo;a fait en TP, il faut :</p><ul><li>les données avec la structure <code>&lt;path/to/data>/images/*.jpg</code> et <code>&lt;path/to/data>/labels/*.txt</code> &ndash; pour chaque image, un fichier texte avec son annotation au format YOLO ;</li><li>un fichier <code>train.txt</code> contenant les chemins d&rsquo;accès à toutes les images du <em>split</em> de <em>train</em>. Idem pour <code>val.txt</code> et <code>test.txt</code> ;</li><li>le fichier <code>&lt;dataset>.yaml</code> dans le dossier <code>data</code>, qui contient les chemins vers les précédents fichiers, les noms et nombre de classes :</li></ul><center><img src=images/fichier_config_yolo.png alt=Labels width=80%></center><h3 id=la-configuration-debug>La configuration <em>debug</em></h3><h4 id=si-je-suis-à-lextérieur-de-linsa-><em>Si je suis à l&rsquo;extérieur de l&rsquo;INSA :</em></h4><ol><li>j&rsquo;ouvre VS Code en local sur ma machine ;</li><li>j&rsquo;ouvre une connexion SSH avec srv-ens-calcul.insa-toulouse.fr après avoir installé l&rsquo;extension Remote-SSH (Ctrl+Shift+X pour ouvrir le gestionnaire d&rsquo;extension)</li></ol><h4 id=si-je-suis-à-linsa><em>Si je suis à l&rsquo;INSA</em>*</h4><p>*ou sur montp.insa-toulouse.fr avec le VPN et une interface graphique</p><ol><li>j&rsquo;ouvre VS Code ;</li><li>j&rsquo;ouvre une connexion SSH directement avec srv-gei-gpu1 (ou srv-gei-gpu2) après avoir installé l&rsquo;extension Remote-SSH (Ctrl+Shift+X pour ouvrir le gestionnaire d&rsquo;extension)</li></ol><blockquote><p>Dans le premier cas, je bénéficie de l&rsquo;environnement matériel des postes non-GPU pour exécuter le code en pas à pas. Dans le deuxième cas, je bénéficie directement du matériel du serveur GPU. Peu de différence tant que j&rsquo;exécute en mode <em>debug</em>, le traitement n&rsquo;a pas besoin d&rsquo;être ultra-rapide. Si je veux lancer un apprentissage, je suis obligée d&rsquo;être sur le GPU (ce sera beaucoup trop long sinon).</p></blockquote><h4 id=configurations-de-debug>Configurations de <em>debug</em></h4><p>Pour exécuter <code>train.py</code> ou <code>detect.py</code> en mode <em>debug</em> et ainsi pouvoir faire une exécution pas à pas avec des points d&rsquo;arrêt dans le code, il faut créer le fichier <code>.vscode/launch.json</code> avec les paramètres désirés pour l&rsquo;exécution :</p><center><p><img src=images/debug_config.png alt="Configurations de debug"></p></center><p>Il suffit ensuite de lancer l&rsquo;une ou l&rsquo;autre des configurations depuis l&rsquo;onglet &ldquo;Run and Debug&rdquo; (Ctrl + Shift + D) :</p><center><p><img src=images/train_debug.png alt="Lancement en mode debug"></p></center><h4 id=en-mode-release>En mode <em>release</em></h4><p>Une fois que tout est OK et que je suis prête à lancer mes apprentissages, je procède en mode console :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>claire@mon-poste:~$ ssh &lt;login-insa&gt;@srv-ens-calcul.insa-toulouse.fr
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>base<span style=color:#f92672>)</span> &lt;login-insa&gt;@srv-ens-calcul:~$ ssh srv-gei-gpu1
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>base<span style=color:#f92672>)</span> &lt;login-insa&gt;@srv-gei-gpu1:~$ cd workspace/yolov5/
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>base<span style=color:#f92672>)</span> &lt;login-insa&gt;@srv-gei-gpu1:~/workspace/yolov5$ source yolov5env/bin/activate
</span></span></code></pre></div><p> </p><ul><li><p>&#x1f525; <mark><em><strong>Pour un apprentissage distribué sur plusieurs GPUs</strong></em></mark></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>base<span style=color:#f92672>)</span> &lt;login-insa&gt;@srv-gei-gpu1:~/workspace/yolov5$ python -m torch.distributed.run --nproc_per_node <span style=color:#ae81ff>4</span> train.py --batch-size <span style=color:#ae81ff>96</span> --img-size <span style=color:#ae81ff>320</span> --weights yolov5m.pt --freeze <span style=color:#ae81ff>10</span>  --data data/dlcv.yaml --name dlcv_batch96_img320_yolom_freeze --epochs <span style=color:#ae81ff>50</span> --hyp data/hyps/hyp.scratch-med.yaml --device 0,1,2,3 
</span></span></code></pre></div><blockquote><p><strong>Avantages</strong> : le <em>batch</em> peut être plus gros, l&rsquo;apprentissage ira plus vite, l&rsquo;apprentissage peut être dimensionné pour occuper les 24Go de mémoire à dispo (4*8Go).</p><p><strong>Inconvénients</strong> : un seul apprentissage parallèle est possible en même temps. Aucun autre script utilisant le GPU ne pourra être lancé.</p><ul><li><code>python -m torch.distributed.run --nproc_per_node 4</code> permet de lancer l&rsquo;apprentissage sur 4 GPU ;</li><li>le reste des paramètres a été vu en TP ;</li><li><code>--device 0,1,2,3</code> est nécessaire pour préciser quels GPUs utiliser.
 </li></ul></blockquote></li><li><p>&#x1f525; <mark><em><strong>Pour un apprentissage classique</strong></em></mark></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>base<span style=color:#f92672>)</span> &lt;login-insa&gt;@srv-gei-gpu1:~/workspace/yolov5$ python train.py --batch-size <span style=color:#ae81ff>16</span> --img-size <span style=color:#ae81ff>320</span> --weights yolov5m.pt --freeze <span style=color:#ae81ff>10</span>  --data data/dlcv.yaml --name dlcv_batch16_img320_yolom_freeze --epochs <span style=color:#ae81ff>50</span> --hyp data/hyps/hyp.scratch-med.yaml 
</span></span></code></pre></div><blockquote><p><strong>Avantages</strong> : plusieurs apprentissages classiques peuvent être lancés en même temps.</p><p><strong>Inconvénients</strong> : l&rsquo;apprentissage doit être dimensionné pour tenir au maximum sur la capacité d'1 seul GPU (8Go). Le <em>batch</em> sera plus petit, l&rsquo;apprentissage prendra plus de temps.'</p></blockquote></li></ul><p>Une fois l&rsquo;apprentissage lancé, on peut monitorer l&rsquo;apprentissage en lançant la commande <code>tensorboard --logdir=runs/train</code> depuis le <em>workspace</em> de YOLOv5. L&rsquo;apprentissage est visible à l&rsquo;url localhost:6006 (ou autre port) créée par Tensorboard. On peut également s&rsquo;assurer que le GPU tourne bien avec la commande :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>base<span style=color:#f92672>)</span> &lt;login-insa&gt;@srv-gei-gpu1:~/workspace/yolov5$ nvidia-smi
</span></span></code></pre></div></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/clairelabitbonis/clairelabitbonis.github.io/edit/main/content/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/03_lets_see/index.fr.md title="Améliorez cette page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Améliorez cette page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/02_yolo/ title="DLCV-TP-01 | Le Bingo de YOLO !" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Précédent</div><div class=next-prev-text>DLCV-TP-01 | Le Bingo de YOLO !</div></a></div><div class="col-md-6 next-article"><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/ title="[DLCV] TP 2023-2024" class="btn btn-outline-info"><div>Suivant <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>[DLCV] TP 2023-2024</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table des matières</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#previously-on-dlcv-practical-sessions><strong>Previously on &ldquo;DLCV Practical sessions&rdquo;&mldr;</strong></a></li><li><a href=#il-est-beau-le-dataset>Il est beau le <em>dataset</em></a><ul><li><a href=#répartition-des-classes>Répartition des classes</a></li><li><a href=#analyse-des-labels>Analyse des labels</a></li></ul></li><li><a href=#metal-une-semaine-pour-tout-changer-metal>Une semaine pour tout changer</a><ul><li><a href=#ce-que-vous-navez-pas-vu>Ce que vous n&rsquo;avez pas vu&mldr;</a></li><li><a href=#configurations-entrainées>Configurations entrainées</a><ul><li><a href=#transfer-learning-fine-tuning-pre-trained-models><em>Transfer learning, fine-tuning, pre-trained models&mldr;</em></a></li></ul></li></ul></li><li><a href=#fire-and-now-what-fire><em>And now, what?</em></a></li><li><a href=#annexe--configuration-du-workspace><strong>Annexe</strong> : configuration du <em>workspace</em></a></li><li><a href=#annexe--lancer-un-apprentissage-une-inférence><strong>Annexe</strong> : lancer un apprentissage, une inférence</a><ul><li><a href=#la-manière-dont-je-travaille-et-ce-nest-donc-que-mon-point-de-vue>La manière dont je travaille (et ce n&rsquo;est donc que mon point de vue)</a></li><li><a href=#fichiers-nécessaires-pour-un-apprentissage-sur-données-custom>Fichiers nécessaires pour un apprentissage sur données <em>custom</em></a></li><li><a href=#la-configuration-debug>La configuration <em>debug</em></a><ul><li><a href=#si-je-suis-à-lextérieur-de-linsa-><em>Si je suis à l&rsquo;extérieur de l&rsquo;INSA :</em></a></li><li><a href=#si-je-suis-à-linsa><em>Si je suis à l&rsquo;INSA</em>*</a></li><li><a href=#configurations-de-debug>Configurations de <em>debug</em></a></li><li><a href=#en-mode-release>En mode <em>release</em></a></li></ul></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#about>A propos</a></li><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contactez moi :</h5><ul><li><a href=mailto:clairelabitbonis@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>clairelabitbonis@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Alimenté par
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script><script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>