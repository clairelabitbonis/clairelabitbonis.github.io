<!doctype html><html><head><title>DLCV-TP-00 | Presentation</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png><meta property="og:title" content="DLCV-TP-00 | Presentation"><meta property="og:description" content="Educational objectives The objective of these practical sessions is to touch on all the stages of deep learning engineering, namely:
data acquisition and labelling, convolutional neural networks training, performance evaluation of the learned task, visualization of the obtained results. For this, our starting point will be the object detector widely known and used by the scientific and industrial communities: YOLO (You Only Look Once). We will work with version 5 released in 2020."><meta property="og:type" content="article"><meta property="og:url" content="http://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/"><meta property="og:image" content="http://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/featured.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-04T10:00:00+09:00"><meta property="article:modified_time" content="2022-11-04T10:00:00+09:00"><meta name=description content="DLCV-TP-00 | Presentation"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png alt=Logo>
Claire Labit-Bonis</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-gb"></span>
English</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation><span class="flag-icon flag-icon-fr"></span>
Français</a></div></li><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/ title=Introduction>Introduction</a></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/>Teaching</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/3d_perception/>3D Perception</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/>Practical sessions</a><ul><li><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/cc_segmentation/ title=Segmentation>Segmentation</a></li><li><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/ title="Mono. localization">Mono. localization</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/>Deep Learning & CV</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/>Course</a><ul><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/01_why_deep_learning/ title="00 | From AI to DL">00 | From AI to DL</a></li><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/02_lets_learn_deeply/ title="01 | DD into OD">01 | DD into OD</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/>Practical sessions</a><ul class=active><li><a class=active href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/ title="00 | Presentation">00 | Presentation</a></li></ul></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/claire_hu4fa84bbe5b0b14aeb1f21f53cdd20f7e_312167_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name>Claire Labit-Bonis</h5><p>Friday, November 4, 2022</p></div><div class=title><h1>DLCV-TP-00 | Presentation</h1></div><div class=post-content id=post-content><h2 id=educational-objectives>Educational objectives</h2><p>The objective of these practical sessions is to touch on all the stages of <em>deep learning</em> engineering, namely:</p><ul><li>data <strong>acquisition</strong> and <strong>labelling</strong>,</li><li>convolutional neural networks <strong>training</strong>,</li><li>performance <strong>evaluation</strong> of the learned task,</li><li><strong>visualization</strong> of the obtained results.</li></ul><p>For this, our starting point will be the object detector widely known and used by the scientific and industrial communities: <a href=https://arxiv.org/pdf/1506.02640.pdf>YOLO</a> (You Only Look Once). We will work with <a href=https://github.com/ultralytics/yolov5>version 5</a> released in 2020.</p><blockquote><p>At the end of the course, you will know how to use YOLOv5, how to train it on your own data, and how to evaluate it.</p></blockquote><h2 id=how-the-sessions-go>How the sessions go</h2><blockquote><p><strong>One for all, all for one!</strong></p><p>Because there is strength in numbers, and joy and good humor facilitate learning &ndash; of humans &ndash; the practical sessions will take place in a context that is both collective and individual, not always at the workstations, and always in the interest of understanding. We will all have a role to play, at each step.</p></blockquote><p>We are going to train YOLOv5 to detect several classes of objects, at the rate of one class of object per pair. The <em>dataset</em> that we will build for this will be common to both groups running in parallel (<em>e.g.</em>, groups A1 & A2, groups B1 & B2).</p><p>To do this, we will go through several steps:</p><ul><li>🔥 <mark><strong>step 1 - acquisition</strong></mark>: each binomial will take several video sequences of the class of objects they have chosen from a proposed list, and put it on a common data server for groups A1/A2 and B1/B2 ;</li><li>🔥 <mark><strong>step 2 - annotation</strong></mark>: with the CVAT tool, each binomial will annotate its own image sequences with the chosen object class, so that at the end of the annotation phase, the whole group will have collectively built a multi-class dataset that everyone will benefit from to do their learning;</li><li>🔥 <mark><strong>step 3 - getting to grips with the YOLOv5 code</strong></mark>: at the end of this step, you will know how to apply a pre-trained YOLOv5-S model on COCO to your own images, enter the architecture of the network and identify its different layers and their dimensions, visualize the resulting detection output, etc. To achieve this, a game of &ldquo;where is Charlie?&rdquo; will be proposed to you and will push you to deconstruct the execution of the code step by step. For instance, you will have to answer questions like &ldquo;what is the size of the tensor at the output of layer 17 for an input image of 512x512x3?&rdquo;;</li><li>🔥 <mark><strong>step 4 - training YOLOv5 on our dataset</strong></mark>: the ideal way to analyze the performance of a given set of parameters (input image resolution, network size, batch size, etc.) is to run as many trainings as possible configurations and then compare them to select the best one. We can then display on the same graph different sizes of models, for different resolutions, and compare their speed of execution to the <em>mean average precision</em> that they achieve on a given dataset, for example :</li></ul><center><p><img src=images/perfs_yolov5.png alt="YOLOv5 performance comparison on COCO">
<em>Source: <a href=https://github.com/ultralytics/yolov5>https://github.com/ultralytics/yolov5</a></em></p></center><blockquote><p>A learning process takes several hours. To be able to compare all these configurations, you need either to have several powerful GPU servers that can run several configurations in parallel, or to have a lot of time and be patient&mldr;</p><p><strong>Also, a training session consumes energy.</strong></p><p>It is therefore not possible for each pair to run a training session for each set of parameters and then do a comparative analysis of the results.</p><p><strong>But, once again, there is strength in numbers.</strong></p><p>Each pair will therefore position itself on a given configuration and will launch the associated training on the dataset of the group.</p></blockquote><ul><li>🔥 <mark><strong>step 5 - performance analysis</strong></mark>: once all the training is done, each pair will be able to evaluate the performance of its own configuration, analyze the results quantitatively, <em>i.e.</em>, with numbers, and qualitatively, <em>i.e.</em>, with an &ldquo;eye&rdquo; visualization of the error cases and the cases that work. A comparative evaluation will also be possible, since everyone will have access to the results obtained by the other pairs, through a common <em>leaderboard</em>.</li></ul><h2 id=evaluation-methods>Evaluation methods</h2><p>Each pair will produce a video clip of about 5~10 minutes. Of course, the relevance of the content is more important than the length of the clip, so you are free to decide how much time you need to cover, for instance:</p><ul><li>the statistics of your acquisitions (the chosen class, the different acquisition contexts, the number of annotated images, the annotation strategy, time spent, encountered difficulties&mldr;) ;</li><li>quantitative analysis of your results (performance metrics, number of epochs to converge, speed of execution of the model, distribution of performance over the different classes of objects, potential <em>overfitting</em>, comparison of the metrics to other configurations and interpretation of this comparison&mldr;);</li><li>qualitative analysis of your results (visualization of the model execution, performance according to the acquisition context, according to the quality of the annotation&mldr;);</li><li>other ideas you might have.</li></ul><p>You will have understood that the evaluation of your work will not depend on the performance of your model training (and thus on the configuration that will have been assigned to you), but rather on the analysis that you will be able to do.</p><h2 id=session-tools-and-workspace-configuration>Session tools and workspace configuration</h2><h3 id=shared-file-2022-2023>Shared file 2022-2023</h3><p>So that everyone has the information of which pair is working on which class and which YOLO configuration, a 🔥📃 <a href="https://docs.google.com/spreadsheets/d/1smyGWTv-3chS242o51kwthJtzzOhafJqURbdl5zbyNE/edit?usp=sharing">shared file is available</a> 📃🔥. Sharing is subject to validation, you will have to wait until you have been authorized before you can modify the document.</p><blockquote><p>The videos below guide you in filling this document.</p><p>For confidentiality reasons, the names used are fictitious.</p></blockquote><p><em><strong>Let&rsquo;s go</strong></em> :</p><hr><ul><li>🔥💻 <mark><strong><em>étape 1</em></mark> : configuring work pairs</strong></li></ul><hr><ul><li>🔥💻 <mark><strong><em>étape 2</em></mark> : object classes choice</strong></li></ul><hr><ul><li>🔥💻 <mark><strong><em>étape 3</em></mark> : YOLOv5 configuration choice</strong></li></ul><hr><h3 id=ide-and-yolov5-clone>IDE and YOLOv5 clone</h3><p>This section guides you through the configuration of your workspace with the tools you have in the lab. The proposed configuration is based on an Ubuntu 20.04 environment, with VS Code as an IDE and the creation of a virtual environment using <code>python venv</code>.
You are of course free to use any IDE if you have other preferences, or to use Anaconda to create your virtual environment&mldr; the main thing is that it works !</p><p><em><strong>Let&rsquo;s go</strong></em> :</p><hr><ul><li><p>🔥💻 <mark><strong><em>step 1</em></mark>: setting up the tree</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e>## Cloning the Github repository of yolov5 release 6.2</span>
</span></span><span style=display:flex><span>login@machine:~$ cd &lt;path/to/workspace&gt;
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;$ git clone -b v6.2 https://github.com/ultralytics/yolov5.git
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;$ cd yolov5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Configuration of the virtual environment named &#39;yolov5env&#39;</span>
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;/yolov5$ python3 -m venv yolov5env <span style=color:#75715e># Creation</span>
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;/yolov5$ source yolov5env/bin/activate <span style=color:#75715e># Activation</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/workspace&gt;/yolov5$ python3 -m pip install --upgrade pip <span style=color:#75715e># Upgrade pip</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/workspace&gt;/yolov5$ pip3 install -r requirements.txt <span style=color:#75715e># Install libs</span>
</span></span></code></pre></div><p><em>At this point, the whole YOLOv5 tree is in place, all the libraries are installed.</em></p></li></ul><hr><ul><li><p>🔥💻 <mark><strong><em>step 2</em></mark>: VS Code configuration</strong></p><p>In VS Code, open the <code>yolov5</code> folder from the previous step :</p><center><p><img src=images/open_folder_vscode.png alt="Open workspace in VS Code"></p></center><p>Next, make sure that the extension for Python is installed. To do this, go to the &ldquo;Extensions&rdquo; tab <em>via</em> the <code>Ctrl + Shift + X</code> shortcut and search for <code>python</code>. Install the extension if it is not already installed:</p><center><p><img src=images/install_extension_python_vscode.png alt="Install extension for Python"></p></center><p>Next, select the Python interpreter for the virtual environment you created in step 1, using the shortcut <code>Ctrl + Shift + P</code> to bring up the command palette, then typing the command <code>Python: Select Interpreter</code>. From the choices offered, click on the one corresponding to the virtual environment <code>yolov5env</code> :</p><center><p><img src=images/select_interpreter_vscode.png alt="Select Python Interpreter"></p></center></li></ul><hr><ul><li><p>🔥💻 <mark><strong><em>step 3</em></mark>: let&rsquo;s see if you followed&mldr;</strong></p><p>If everything is set up correctly, you can launch a terminal in VS Code <em>via</em> <code>Terminal > New Terminal</code> and type the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/yolov5&gt;$ python detect.py --source <span style=color:#e6db74>&#39;https://ultralytics.com/images/zidane.jpg&#39;</span>
</span></span></code></pre></div><p>Once the command is executed, you will find the result of the YOLOv5-S model execution on the <code>zidane.jpg</code> image under the <code>runs/detect/exp</code> folder:</p><center><p><img src=images/zidane.png alt="YOLOv5 run check"></p></center></li></ul><hr><p>🔥🎆👍🌟 <strong>Well done !</strong></p><h3 id=cvat-for-annotation>CVAT for annotation</h3><h3 id=data-server-for-the-dataset>Data server for the dataset</h3><h2 id=session-schedule>Session schedule</h2><center><table><thead><tr><th style=text-align:center>Group</th><th style=text-align:center>Dates</th><th style=text-align:center>Rooms</th><th style=text-align:center>Supervisor</th></tr></thead><tbody><tr><td style=text-align:center>A1</td><td style=text-align:center>30/11/2022 \ 12h30 - 15h15<br>07/12/2022 \ 09h30 - 12h15<br>14/12/2022 \ 15h30 - 18h15</td><td style=text-align:center>GEI-111-A<br>GEI-109-A<br>GEI-111-A</td><td style=text-align:center>Claire LABIT-BONIS</td></tr><tr><td style=text-align:center>A2</td><td style=text-align:center>30/11/2022 \ 12h30 - 15h15<br>07/12/2022 \ 09h30 - 12h15<br>14/12/2022 \ 15h30 - 18h15</td><td style=text-align:center>GEI-111-B<br>GEI-109-B<br>GEI-111-B</td><td style=text-align:center>Smail AIT BOUHSAIN</td></tr><tr><td style=text-align:center>B1</td><td style=text-align:center>30/11/2022 \ 15h30 - 18h15<br>07/12/2022 \ 15h30 - 18h15<br>14/12/2022 \ 09h30 - 12h15</td><td style=text-align:center>GEI-111-A<br>GEI-111-A<br>GEI-109-A</td><td style=text-align:center>Claire LABIT-BONIS</td></tr><tr><td style=text-align:center>B2</td><td style=text-align:center>30/11/2022 \ 15h30 - 18h15<br>07/12/2022 \ 15h30 - 18h15<br>14/12/2022 \ 09h30 - 12h15</td><td style=text-align:center>GEI-111-B<br>GEI-111-B<br>GEI-109-B</td><td style=text-align:center>Pierre MARIGO</td></tr></tbody></table></center></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/clairelabitbonis/clairelabitbonis.github.io/edit/main/content/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/02_lets_learn_deeply/ title="DLCV-CM-01 | Deep dive into object detection" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>DLCV-CM-01 | Deep dive into object detection</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a><div class="dropdown languageSelector"><a class="btn dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-gb"></span>
English</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation><span class="flag-icon flag-icon-fr"></span>
Français</a></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#educational-objectives>Educational objectives</a></li><li><a href=#how-the-sessions-go>How the sessions go</a></li><li><a href=#evaluation-methods>Evaluation methods</a></li><li><a href=#session-tools-and-workspace-configuration>Session tools and workspace configuration</a><ul><li><a href=#shared-file-2022-2023>Shared file 2022-2023</a></li><li><a href=#ide-and-yolov5-clone>IDE and YOLOv5 clone</a></li><li><a href=#cvat-for-annotation>CVAT for annotation</a></li><li><a href=#data-server-for-the-dataset>Data server for the dataset</a></li></ul></li><li><a href=#session-schedule>Session schedule</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:clairelabitbonis@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>clairelabitbonis@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script type=text/javascript src=/js/darkreader.js></script>
<script type=text/javascript src=/js/darkmode-darkreader.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script>
<script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>