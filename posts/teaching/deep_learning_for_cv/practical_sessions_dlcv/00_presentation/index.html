<!doctype html><html><head><title>DLCV-TP-00 | Pr√©sentation</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png><meta property="og:title" content="DLCV-TP-00 | Pr√©sentation"><meta property="og:description" content="Objectifs p√©dagogiques L&rsquo;objectif de ces s√©ances de travaux pratiques est de toucher √† toutes les √©tapes de l&rsquo;ing√©nierie du deep learning, √† savoir :
l&rsquo;acquisition et l&rsquo;annotation de donn√©es, l&rsquo;apprentissage de r√©seaux de convolution, l&rsquo;√©valuation des performances de la t√¢che apprise, la visualisation des r√©sultats obtenus. Pour cela, notre point de d√©part sera le d√©tecteur d&rsquo;objets tr√®s largement connu et utilis√© par les communaut√©s scientifique mais aussi industrielle : YOLO (You Only Look Once)."><meta property="og:type" content="article"><meta property="og:url" content="http://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/"><meta property="og:image" content="http://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/featured.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-14T10:00:00+09:00"><meta property="article:modified_time" content="2022-12-14T10:00:00+09:00"><meta name=description content="DLCV-TP-00 | Pr√©sentation"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png alt=Logo>
Claire Labit-Bonis</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Chercher data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Articles</a></li><div class=subtree><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/>Enseignements</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/3d_perception/>Perception 3D</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/>Travaux pratiques</a><ul><li><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/cc_segmentation/ title=Segmentation>Segmentation</a></li><li><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/ title="Localisation mono.">Localisation mono.</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/>Deep Learning & CV</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/>Cours</a><ul><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/01_why_deep_learning/ title="00 | De l'IA au DL">00 | De l'IA au DL</a></li><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/02_lets_learn_deeply/ title="01 | DD into OD">01 | DD into OD</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/>Travaux pratiques</a><ul class=active><li><a class=active href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/ title="00 | Pr√©sentation">00 | Pr√©sentation</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/02_yolo/ title="01 | YOLO !">01 | YOLO !</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/03_lets_see/ title="11 | Voyons...">11 | Voyons...</a></li></ul></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/claire_hu4fa84bbe5b0b14aeb1f21f53cdd20f7e_312167_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name>Claire Labit-Bonis</h5><p>mercredi 14 d√©cembre 2022</p></div><div class=title><h1>DLCV-TP-00 | Pr√©sentation</h1></div><div class=post-content id=post-content><h2 id=objectifs-p√©dagogiques>Objectifs p√©dagogiques</h2><p>L&rsquo;objectif de ces s√©ances de travaux pratiques est de toucher √† toutes les √©tapes de l&rsquo;ing√©nierie du <em>deep learning</em>, √† savoir :</p><ul><li>l&rsquo;<strong>acquisition</strong> et l&rsquo;<strong>annotation</strong> de donn√©es,</li><li>l&rsquo;<strong>apprentissage</strong> de r√©seaux de convolution,</li><li>l&rsquo;<strong>√©valuation</strong> des performances de la t√¢che apprise,</li><li>la <strong>visualisation</strong> des r√©sultats obtenus.</li></ul><p>Pour cela, notre point de d√©part sera le d√©tecteur d&rsquo;objets tr√®s largement connu et utilis√© par les communaut√©s scientifique mais aussi industrielle : <a href=https://arxiv.org/pdf/1506.02640.pdf>YOLO</a> (You Only Look Once). Nous travaillerons avec la <a href=https://github.com/ultralytics/yolov5>version 5</a> sortie en 2020.</p><blockquote><p>A la fin des TPs, vous saurez comment utiliser YOLOv5, comment l&rsquo;entrainer sur vos propres donn√©es, et comment l&rsquo;√©valuer.</p></blockquote><h2 id=d√©roulement-des-s√©ances>D√©roulement des s√©ances</h2><blockquote><p><strong>Un¬∑e pour tou¬∑te¬∑s, tou¬∑te¬∑s pour un¬∑e !</strong></p><p>Parce que l&rsquo;union fait la force, que la joie et la bonne humeur facilitent l&rsquo;apprentissage &ndash; des humains &ndash;, les s√©ances de travaux pratiques se d√©rouleront dans un contexte √† la fois collectif et individuel, pas toujours sur les postes de travail, et toujours dans l&rsquo;int√©r√™t de la compr√©hension. Nous aurons tou¬∑te¬∑s un r√¥le √† jouer, √† chacune des √©tapes.</p></blockquote><p>Nous allons entrainer YOLOv5 √† d√©tecter plusieurs classes d&rsquo;objets, √† raison d&rsquo;une classe d&rsquo;objet par bin√¥me. Le <em>dataset</em> que nous allons construire pour cela sera commun aux deux groupes de TPs qui se d√©roulent en parall√®le (<em>e.g.</em>, groupes A1 & A2, groupes B1 & B2).</p><p>Pour cela, nous passerons par plusieurs √©tapes :</p><ul><li>üî• <mark><strong>√©tape 1 - acquisition</strong></mark> : chaque bin√¥me prendra plusieurs s√©quences vid√©o de la classe d&rsquo;objets qu&rsquo;il aura choisie parmi une liste propos√©e, et la mettra sur un serveur de donn√©es commun aux groupes A1/A2 et B1/B2 ;</li><li>üî• <mark><strong>√©tape 2 - annotation</strong></mark> : avec l&rsquo;outil CVAT, chaque bin√¥me annotera ses propres s√©quences d&rsquo;images avec la classe d&rsquo;objet choisie, de sorte qu&rsquo;√† la fin de la phase d&rsquo;annotation, le groupe entier de TP aura collectivement construit un <em>dataset</em> multi-classes dont tout le monde b√©n√©ficiera pour faire ses apprentissages ;</li><li>üî• <mark><strong>√©tape 3 - prise en main du code de YOLOv5</strong></mark> : √† la fin de cette √©tape, vous saurez appliquer sur vos propres images un mod√®le YOLOv5-S pr√©-entrain√© sur COCO, entrer dans l&rsquo;architecture du r√©seau et identifier ses diff√©rentes couches et leurs dimensions, visualiser la sortie de d√©tection obtenue, etc. Pour parvenir √† cette prise en main, un jeu de &ldquo;o√π est Charlie ?&rdquo; vous sera propos√© et vous poussera √† d√©cortiquer l&rsquo;ex√©cution du code pas √† pas. Vous devrez par exemple r√©pondre √† des questions comme &ldquo;quelle est la taille du tenseur en sortie de la couche 17 pour une image d&rsquo;entr√©e de 512x512x3 ?&rdquo; ;</li><li>üî• <mark><strong>√©tape 4 - apprentissage de YOLOv5 sur notre <em>dataset</em></strong></mark> : l&rsquo;id√©al pour analyser les performances d&rsquo;un jeu de param√®tres donn√© (r√©solution des images d&rsquo;entr√©e, taille du r√©seau, taille de <em>batch</em>, etc.) est de lancer autant d&rsquo;apprentissages que de configurations possibles et de les comparer ensuite pour s√©lectionner celle qui est la meilleure. On peut ensuite afficher sur un m√™me graphe diff√©rentes tailles de mod√®les, pour diff√©rentes r√©solutions, et comparer leur rapidit√© d&rsquo;ex√©cution √† la <em>mean average precision</em> qu&rsquo;ils r√©alisent sur un <em>dataset</em> donn√©, par exemple :</li></ul><center><p><img src=images/perfs_yolov5.png alt="Comparaison des performances de YOLOv5 sur COCO">
<em>Source : <a href=https://github.com/ultralytics/yolov5>https://github.com/ultralytics/yolov5</a></em></p></center><blockquote><p>Un apprentissage dure plusieurs heures. Pour pouvoir comparer toutes ces configurations, il faut soit disposer de plusieurs serveurs GPU puissants qui peuvent faire tourner en parall√®le plusieurs configurations, soit disposer de beaucoup de temps et √™tre patient&mldr;</p><p><strong>Et puis, un apprentissage, √ßa consomme de l&rsquo;√©nergie.</strong></p><p>Il n&rsquo;est donc pas envisageable que chaque bin√¥me lance un apprentissage pour chaque jeu de param√®tres puis fasse une analyse comparative des r√©sultats.</p><p><strong>MAIS !, l&rsquo;union fait la force, une fois de plus.</strong></p><p>Chaque bin√¥me se positionnera donc sur une configuration donn√©e et lancera l&rsquo;apprentissage associ√© sur le <em>dataset</em> du groupe de TP.</p></blockquote><ul><li>üî• <mark><strong>√©tape 5 - analyse des performances</strong></mark> : une fois tous les apprentissages faits, chaque bin√¥me pourra √©valuer les performances de sa propre configuration, analyser les r√©sultats de mani√®re quantitative, <em>i.e.</em>, avec des chiffres, et de mani√®re qualitative, <em>i.e.</em>, avec une visualisation &ldquo;√† l&rsquo;oeil&rdquo; des cas d&rsquo;erreur et des cas qui fonctionnent. Une √©valuation comparative sera √©galement r√©alisable, puisque tout le monde aura acc√®s aux r√©sultats obtenus par les autres bin√¥mes, au travers d&rsquo;un <em>leaderboard</em> commun au groupe.</li></ul><h2 id=modalit√©s-d√©valuation>Modalit√©s d&rsquo;√©valuation</h2><div class="alert alert-danger"><strong><p>La <strong>deadline</strong> est fix√©e au 12 janvier 2023 √† 18h. Vous devez d√©poser sur Moodle :</p><ul><li>votre capsule vid√©o au format <code>.mp4</code> ; elle ne doit pas d√©passer 256 Mo ;</li><li>un fichier <code>.pdf</code> avec vos r√©ponses aux qu√™tes annexes.</li></ul></strong></div><h3 id=capsule-vid√©o>Capsule vid√©o</h3><p>Chaque bin√¥me produira une capsule vid√©o (c&rsquo;est-√†-dire une s√©quence vid√©o) d&rsquo;environ 10 minutes. Bien s√ªr, la pertinence du contenu importe plus que la longueur de la capsule.</p><p>Le tableau d&rsquo;analyse ci-dessous peut vous aider √† structurer votre analyse et vous donne une id√©e des points qui seront √©valu√©s :</p><center><p><img src=images/modalites_evaluation.png alt="Tableau d&amp;rsquo;analyse"></p></center><p>Globalement, on aura :</p><ul><li>les <mark>trois grandes th√©matiques</mark> vues en cours et en TP peuvent donner lieu √† analyse :<ul><li>le <em><strong>dataset</strong></em> (acquisition, annotation),</li><li>l&rsquo;<strong>apprentissage</strong>,</li><li>et l&rsquo;<strong>inf√©rence</strong> (= la d√©tection appliqu√©e √† des donn√©es inconnues ou de test avec le mod√®le entrain√©) ;</li></ul></li><li><mark>trois types d&rsquo;analyse</mark> possibles :<ul><li>l&rsquo;<strong>analyse quantitative</strong> qui consiste √† analyser la r√©partition des classes, le nombre d&rsquo;instances annot√©es, pr√©senter des chiffres, interpr√©ter des courbes de r√©sultats, des temps d&rsquo;ex√©cution du r√©seau en fonction de sa taille, de la r√©solution image, commenter les m√©triques de performance en fonction du <em>split</em>, de la classe, etc.,</li><li>l&rsquo;<strong>analyse qualitative</strong> qui consiste √† analyser visuellement les performances d&rsquo;une configuration de r√©seau donn√©e, par exemple se rendre compte que les petits r√©seaux ont plus de mal √† d√©tecter les petits objets, que la classe <code>velo</code> est mieux d√©tect√©e quand le v√©lo prend toute l&rsquo;image (faire le lien avec l&rsquo;annotation qui a √©t√© faite), ou que les petits objets circulaires et orang√©s sont souvent d√©tect√©s comme des lentilles (exemple : l&rsquo;oeil du pigeon&mldr;),</li><li>le <strong>retour d&rsquo;exp√©rience</strong> : je vous demande votre avis ! Comment vous √™tes-vous r√©parti l&rsquo;annotation dans le bin√¥me ? Entre diff√©rents bin√¥mes qui ont annot√© la m√™me classe ? Quelles difficult√©s avez-vous rencontr√©es ? Quelles questions vous √™tes-vous pos√©es au moment de l&rsquo;acquisition ? Que feriez-vous diff√©remment ? Quelle a √©t√© votre strat√©gie pour choisir vos exemples d&rsquo;inf√©rence ? Comment avez-vous d√©cid√© de la r√©partition entre les diff√©rents <em>splits</em> de <em>train</em>, <em>val</em> et <em>test</em> ?</li></ul></li><li><mark>deux niveaux d&rsquo;analyse</mark> :<ul><li><strong>micro</strong> : vous pouvez analyser votre classe d&rsquo;annotation (m√©triques de performance sur train/val/test, quantit√© annot√©e, type d&rsquo;annotation, nombre d&rsquo;instances par image, strat√©gie d&rsquo;annotation, etc.), les performances fines de votre configuration de r√©seau* (temps d&rsquo;ex√©cution, taille du r√©seau, place m√©moire, nombre de couches, de param√®tres, analyse des courbes F1, PR, etc.),</li><li><strong>macro</strong> : vous <em>vs.</em> la promo enti√®re et les autres configurations. Par exemple, une analyse plus globale qui compare les diff√©rentes courbes F1 des diff√©rents apprentissages &ndash; quelle est l&rsquo;influence de la taille du r√©seau ? Quelle configuration converge le mieux ? Le plus rapidement ? Laquelle g√®re mieux les petits objets ? Quelle classe se distingue des autres et pourquoi ? Quelle a √©t√© la dynamique de groupe globalement ? Quelles discussions avez-vous eues avec les autres bin√¥mes pour vous accorder sur une strat√©gie d&rsquo;annotation ?</li></ul></li></ul><p>*celle tir√©e au sort en TP &ndash; si elle √©tait trop mauvaise, prenez-en une autre, pas grave, il faut pouvoir en dire quelque chose d&rsquo;autre que &ldquo;c&rsquo;est nul et √ßa marche pas&rdquo;&mldr;</p><blockquote><p>Il y a <strong>des millions de choses</strong> √† dire et tout n&rsquo;est pas applicable √† toutes les classes d&rsquo;objets, toutes les configurations ni tous les bin√¥mes. Etre exhaustif vous demanderait une analyse de 45 minutes √† 2 jours (rien que √ßa). Soyez synth√©tiques et pertinents. Ce n&rsquo;est pas une t√¢che simple, mais il y a de la mati√®re √† votre disposition.</p><p><strong>Pas de contrainte sur le format</strong> : une vid√©o d&rsquo;un PPT qui d√©file, une superproduction <em>full</em> 3D pour concurrencer Avatar 2&mldr; l&rsquo;important est d&rsquo;avoir un contenu pertinent, et que √ßa ne soit pas la corv√©e du si√®cle pour vous. Amusez-vous comme on s&rsquo;est amus√©s en cours et en TP, mais vendez-nous une analyse de r√™ve ;)</p></blockquote><h3 id=qu√™tes-annexes>Qu√™tes annexes</h3><p>Le plus pratique pour r√©pondre √† ces qu√™tes annexes est de noter vos r√©ponses dans un rapport, d&rsquo;o√π la contrainte donn√©e plus haut d&rsquo;un <code>.pdf</code> d√©pos√© sur Moodle.</p><p>Je vous laisse aller voir <a href=http://clairelabitbonis.github.io/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/02_yolo/#qu%C3%AAtes-annexes>ici</a> pour le contenu des qu√™tes annexes.</p><p>Les qu√™tes annexes sont annexes, mais elles vous apporteront du bonus si vous les faites. Rappelez-vous : Far Cry, Assassin&rsquo;s Creed, tout √ßa tout √ßa.</p><h2 id=planning-des-s√©ances>Planning des s√©ances</h2><center><table><thead><tr><th style=text-align:center>Groupe</th><th style=text-align:center>Dates</th><th style=text-align:center>Salles</th><th style=text-align:center>Intervenant¬∑e</th></tr></thead><tbody><tr><td style=text-align:center>A1</td><td style=text-align:center>30/11/2022 \ 12h30 - 15h15<br>07/12/2022 \ 09h30 - 12h15<br>14/12/2022 \ 15h30 - 18h15</td><td style=text-align:center>GEI-111-A<br>GEI-109-A<br>GEI-111-A</td><td style=text-align:center>Claire LABIT-BONIS</td></tr><tr><td style=text-align:center>A2</td><td style=text-align:center>30/11/2022 \ 12h30 - 15h15<br>07/12/2022 \ 09h30 - 12h15<br>14/12/2022 \ 15h30 - 18h15</td><td style=text-align:center>GEI-111-B<br>GEI-109-B<br>GEI-111-B</td><td style=text-align:center>Smail AIT BOUHSAIN</td></tr><tr><td style=text-align:center>B1</td><td style=text-align:center>30/11/2022 \ 15h30 - 18h15<br>07/12/2022 \ 15h30 - 18h15<br>14/12/2022 \ 09h30 - 12h15</td><td style=text-align:center>GEI-111-A<br>GEI-111-A<br>GEI-109-A</td><td style=text-align:center>Claire LABIT-BONIS</td></tr><tr><td style=text-align:center>B2</td><td style=text-align:center>30/11/2022 \ 15h30 - 18h15<br>07/12/2022 \ 15h30 - 18h15<br>14/12/2022 \ 09h30 - 12h15</td><td style=text-align:center>GEI-111-B<br>GEI-111-B<br>GEI-109-B</td><td style=text-align:center>Pierre MARIGO</td></tr></tbody></table></center></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/clairelabitbonis/clairelabitbonis.github.io/edit/main/content/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/index.fr.md title="Am√©liorez cette page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Am√©liorez cette page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/02_lets_learn_deeply/ title="DLCV-CM-01 | Deep dive into object detection" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Pr√©c√©dent</div><div class=next-prev-text>DLCV-CM-01 | Deep dive into object detection</div></a></div><div class="col-md-6 next-article"><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/02_yolo/ title="DLCV-TP-01 | Le Bingo de YOLO !" class="btn btn-outline-info"><div>Suivant <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>DLCV-TP-01 | Le Bingo de YOLO !</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table des mati√®res</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#objectifs-p√©dagogiques>Objectifs p√©dagogiques</a></li><li><a href=#d√©roulement-des-s√©ances>D√©roulement des s√©ances</a></li><li><a href=#modalit√©s-d√©valuation>Modalit√©s d&rsquo;√©valuation</a><ul><li><a href=#capsule-vid√©o>Capsule vid√©o</a></li><li><a href=#qu√™tes-annexes>Qu√™tes annexes</a></li></ul></li><li><a href=#planning-des-s√©ances>Planning des s√©ances</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#about>A propos</a></li><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contactez moi :</h5><ul><li><a href=mailto:clairelabitbonis@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>clairelabitbonis@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">¬© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Aliment√© par
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script type=text/javascript src=/js/darkreader.js></script>
<script type=text/javascript src=/js/darkmode-darkreader.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script>
<script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>