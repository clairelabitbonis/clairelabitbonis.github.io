<!doctype html><html><head><title>DLCV2 | Le Bingo de YOLO !</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png><meta property="og:title" content="DLCV2 | Le Bingo de YOLO !"><meta property="og:description" content="Présentation Bonjour, bonjour ! Aujourd&rsquo;hui, plusieurs objectifs :
finir le dataset ; faire le split train/val/test ; développer une application de détection et visualisation dans un flux vidéo ; manipuler un apprentissage de YOLOv8. C&rsquo;est comme une grille de bingo à remplir. Plus vous en validez, mieux c&rsquo;est.
On va le diviser en deux : une quête principale, et des quêtes secondaires (des gamers par ici ?). La quête principale est l&rsquo;objectif à atteindre pour terminer la séance de TP d&rsquo;aujourd&rsquo;hui (si vous ne les avez pas, je ne pourrai pas utiliser vos données pour lancer les apprentissages &#x1f622;), les quêtes secondaires sont à faire s&rsquo;il vous reste du temps."><meta property="og:type" content="article"><meta property="og:url" content="http://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/02_yolo/"><meta property="og:image" content="http://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/02_yolo/featured.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-12-12T10:00:00+09:00"><meta property="article:modified_time" content="2023-12-12T10:00:00+09:00"><meta name=description content="DLCV2 | Le Bingo de YOLO !"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png alt=Logo>
Claire Labit-Bonis</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20>
</a><a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20>
</a><a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Chercher data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Articles</a></li><div class=subtree><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/>Enseignements</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/3d_perception/>Perception 3D</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/>Travaux pratiques</a><ul><li><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/cc_segmentation/ title=Segmentation>Segmentation</a></li><li><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/ title="Localisation mono.">Localisation mono.</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/>Deep Learning & CV</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/>Cours</a><ul><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/01_why_deep_learning/ title="00 | De l'IA au DL">00 | De l'IA au DL</a></li><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/02_lets_learn_deeply/ title="01 | DD into OD">01 | DD into OD</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/>Travaux pratiques</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/>2022-2023</a><ul><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/00_presentation/ title="00 | Présentation">00 | Présentation</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/02_yolo/ title="01 | YOLO !">01 | YOLO !</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/03_lets_see/ title="10 | Voyons...">10 | Voyons...</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/>2023-2024</a><ul class=active><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/00_presentation/ title="00 | Présentation">00 | Présentation</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/01_all_you_need_is_data/ title="01 | All you need is data">01 | All you need is data</a></li><li><a class=active href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/02_yolo/ title="02 | YOLO !">02 | YOLO !</a></li></ul></li></ul></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/02_yolo/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/claire_hufa4d5753e97ed9e5e63c04dfec44ec48_673603_120x120_fit_q75_box.jpg alt="Author Image"><h5 class=author-name>Claire Labit-Bonis</h5><p>mardi 12 décembre 2023</p></div><div class=title><h1>DLCV2 | Le Bingo de YOLO !</h1></div><div class=post-content id=post-content><center><p><img src=images/le_bingo_de_yolo_v2.png alt="Le Bingo de YOLO"></p></center><h2 id=présentation>Présentation</h2><p>Bonjour, bonjour ! Aujourd&rsquo;hui, plusieurs objectifs :</p><ul><li>finir le <em>dataset</em> ;</li><li>faire le <em>split</em> train/val/test ;</li><li>développer une application de détection et visualisation dans un flux vidéo ;</li><li>manipuler un apprentissage de YOLOv8.</li></ul><p>C&rsquo;est comme une grille de bingo à remplir. Plus vous en validez, mieux c&rsquo;est.</p><p>On va le diviser en deux : une quête principale, et des quêtes secondaires (des <em>gamers</em> par ici ?). La quête principale est l&rsquo;objectif à atteindre pour terminer la séance de TP d&rsquo;aujourd&rsquo;hui (si vous ne les avez pas, je ne pourrai pas utiliser vos données pour lancer les apprentissages &#x1f622;), les quêtes secondaires sont à faire s&rsquo;il vous reste du temps.</p><div class="alert alert-warning"><strong>Attention, &ldquo;Labellisation &#9785;&#xfe0f;&rdquo; et &ldquo;L&rsquo;apprentissage dont vous êtes le héros&rdquo; sont <strong>nécessaires</strong> pour que je puisse lancer les apprentissages pour la semaine prochaine. Considérez-les comme la quête principale. Si vous avez le temps, réalisez les quêtes annexes : &ldquo;3,2,1, développez!&rdquo;, &ldquo;Training COCO&rdquo;, &ldquo;Feature visualization&rdquo;.</strong></div><p>Les quêtes annexes sont optionnelles, mais considérées comme des bonus pris en compte lors de l&rsquo;évaluation &#x1f609;</p><h2 id=quête-principale>Quête principale</h2><h3 id=labellisez-frowning_face>Labellisez &#9785;&#xfe0f;</h3><p>La semaine dernière dans CVAT, vous avez tou.te.s annoté votre classe d&rsquo;objets. Nous devons pouvoir récupérer toutes vos données d&rsquo;ici la fin de séance, sinon vos images ne seviront pas à l&rsquo;entrainement (ce serait dommage). Je me suis juste trompée et j&rsquo;ai rajouté la classe <code>gobelet</code> en plus d&rsquo;<code>ecocup</code>, mais ça ne changera rien pour vous, je dois juste faire une bidouille pour rattraper ça.</p><p><img src=images/dataset_cvat.png alt=Dataset></p><div class="alert alert-warning"><strong><p>Attention, en regardant certaines séquences, je me suis rendu compte que beaucoup d&rsquo;annotations s&rsquo;arrêtaient en cours de vidéo, que certaines qui étaient occultées étaient quand même labellisées comme visibles, etc.
Je vous demande donc à tous et toutes de vérifier vos labels avant de les valider pour de bon.</p><p>&#9888;&#xfe0f; Par ailleurs, changement de stratégie ; nous (les encadrant.e.s) nous chargerons nous-mêmes d&rsquo;exporter les tâches et de les <em>uploader</em> sur le serveur GPU. La semaine dernière, certaines erreurs bizarres ont popé lors de la copie sur le serveur, alors on va s&rsquo;éviter du tracas inutile en séance.</p></strong></div><p><strong>Quand votre labellisation est terminée, venez la valider.</strong></p><h3 id=lapprentissage-dont-vous-êtes-le-héros>L&rsquo;apprentissage dont vous êtes le héros</h3><p>Pour celles et ceux qui n&rsquo;ont pas divisé leur <em>dataset</em> en trois fichiers textes <code>train.txt</code>, <code>val.txt</code> et <code>test.txt</code> (cf. TP précédent), c&rsquo;est l&rsquo;heure !</p><p>Vous devez créer trois fichiers : <code>train.txt</code>, <code>val.txt</code> et <code>test.txt</code>. Dans chacun, vous mettrez la liste des chemins d&rsquo;accès vers les <strong><em>images</em></strong> (pas les labels) selon qu&rsquo;elles doivent aller en base d&rsquo;apprentissage, de validation ou de test. Par exemple, dans <code>train.txt</code> :</p><pre><code>./velo/tic_et_tac/4/images/frame_000002.jpg
./velo/tic_et_tac/4/images/frame_000118.jpg
...
./velo/tic_et_tac/2/images/frame_000004.jpg
./velo/tic_et_tac/1/images/frame_000001.jpg
./velo/tic_et_tac/3/images/frame_000256.jpg
</code></pre><div class="alert alert-danger"><strong><p>La répartition de vos données entre les différentes bases est une étape cruciale :</p><ul><li>allez-vous mettre 3 séquences complètes en <code>train</code>, une en <code>val</code> et une en <code>test</code>, au risque d&rsquo;avoir des exemples en test qui sont trop éloignés de ceux de la base d&rsquo;apprentissage ?</li><li>ou bien allez-vous plutôt mettre les débuts de séquence en <code>train</code>, les milieux en <code>val</code>, les fins en <code>test</code>, mais dans ce cas vous biaiserez l&rsquo;apprentissage et obtiendrez des performances étrangement un peu trop hautes ?</li><li>vous pouvez aussi choisir la méthode bourrine et faire un random total sur la répartition&mldr;</li></ul></strong></div><p><strong>Spoiler alert :</strong> la première option a tendance à légèrement <em>overfitter</em>&mldr;</p><p><img src=images/overfitting.png alt=Overfitting></p><div class="alert alert-success"><strong><p><strong>JE N&rsquo;AI QU&rsquo;UNE PHILOSOPHIE, si ça vous intéresse&mldr;</strong></p><p>Personnellement, quand je vois que je commence à passer plus de temps à écrire un script pour faire une tâche automatiquement qu&rsquo;à la faire de manière manuelle et pas du tout intelligente&mldr; Je préfère la faire de manière manuelle et pas du tout intelligente. Par exemple, en <a href=https://stackoverflow.com/questions/1767384/how-can-i-get-a-recursive-full-path-listing-one-line-per-file>listant dans un fichier tous les chemins d&rsquo;accès</a> et en découpant le fichier à la main dans un éditeur de texte. Mais on peut trouver plus bête encore, je pense.</p><p>Maintenant, si vous êtes des pros de Python, bash, que sais-je, faites-vous plaisir. Il faut juste ne pas y passer toute la séance &#x1f609;</p></strong></div><p>Quand vous avez vos trois fichiers, vous me les envoyez par mail (<a href=mailto:clairelabitbonis@gmail.com>clairelabitbonis@gmail.com</a>), avec le nom de chacun des membres du binôme, et votre classe labellisée.</p><h3 id=3-2-1-développez->3, 2, 1, développez !</h3><p>On arrive enfin à YOLO, depuis le temps qu&rsquo;on en parle. La première étape consiste à configurer votre <em>workspace</em> : allez voir l&rsquo;annexe <a href=https://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/02_yolo/#configuration-du-workspace>Configuration du <em>workspace</em></a> (elle porte bien son nom). Une fois que c&rsquo;est fait, voilà les exigences à respecter :</p><ul><li>l&rsquo;application doit détecter des objets dans un flux vidéo ;</li><li>l&rsquo;application doit afficher les boites englobantes de détection sur les images traitées ;</li><li>l&rsquo;application doit afficher l&rsquo;analyse en temps réel, c&rsquo;est-à-dire afficher les détections dès que l&rsquo;analyse est faite ;</li><li>l&rsquo;application doit colorer les boites englobantes de manière unique en fonction de la classe détectée ;</li><li>l&rsquo;application ne doit pas être dépendante du nombre ni du nom des classes détectées ;</li><li>les couleurs choisies pour l&rsquo;affichage des boites doivent être aussi belles qu&rsquo;un arc-en-ciel en plein mois d&rsquo;août &#x1f308;.</li></ul><p>Les contraintes logicielles sont :</p><ul><li>utiliser Python ;</li><li>utiliser OpenCV ;</li><li>utiliser YOLOv8 (ah bon !).</li></ul><p>Pour cette première version et comme les apprentissages sur le <em>dataset</em> de cette année ne sont pas encore faits, vous utiliserez YOLO avec des poids pré-entrainés sur COCO.</p><p>Pour réaliser cette quête, la toile de l&rsquo;Internet est votre amie, avec notamment :</p><ul><li>la <a href=https://docs.ultralytics.com>doc d&rsquo;Ultralytics</a>;</li><li><a href=https://www.google.fr>Google</a>;</li><li>ChatGPT : &#x1f620; bouuuh ChatGPT, en plus il connait même pas YOLOv8 parce que c&rsquo;est trop récent, laissez tomber. <a href=https://chat.openai.com/share/4c573574-bfad-4b17-a918-9db9fc16f0e1>Même s&rsquo;il est très fort</a>. On appelle ça la <em>dissonance cognitive</em> :</li></ul><p><img src=images/dissonance_cognitive.png alt="Dissonance cognitive"></p><h2 id=quêtes-secondaires>Quêtes secondaires</h2><h3 id=training-coco128>Training COCO128</h3><p>Pour pouvoir voir YOLO en action, vous allez lancer un apprentissage sur vos machines, sur CPU. Et afin d&rsquo;éviter de voir vos PC s&rsquo;envoler vers l&rsquo;infini et au-delà, on va le faire sur un tout petit <em>dataset</em> : COCO 128, qui est une extraction du <em>dataset</em> COCO <a href=https://cocodataset.org/#home>Common Objects in Context</a>.</p><p>Toujours, la <a href=https://docs.ultralytics.com/modes/train/#key-features-of-train-mode>doc d&rsquo;Ultralytics</a> vous indique comment faire.
Dans un premier temps, tentez un apprentissage sur 3 <em>epochs</em> pour voir ce qu&rsquo;il se passe quand un apprentissage est terminé.
Tous les éléments liés à l&rsquo;apprentissage que vous venez de lancer auront été enregistrés dans le dossier <code>runs/...</code> quelque chose. Allez voir ce qu&rsquo;il s&rsquo;y passe. Allez voir les métriques qu&rsquo;il sauvegarde, les images qu&rsquo;il produit dans ce dossier.</p><p>Vous pouvez aussi voir comment est écrit le fichier du <em>dataset</em>, il est dans <code>ultralytics/cfg/datasets/coco128.yaml</code>. Vous serez ravis d&rsquo;apprendre que le <em>dataset</em> COCO permet d&rsquo;entrainer YOLO à détecter des brocolis.</p><p>Quand vous lancez votre script de <code>train</code>, vous pouvez ensuite le monitorer avec Tensorboard. Pour cela, ouvrez un autre terminal et entrez <code>tensorboard --logdir=runs</code> pour indiquer qu&rsquo;il faut monitorer le dossier dans lequel YOLO stocke les apprentissages. Il vous ouvre un monitoring sur <a href=https://localhost:6006>https://localhost:6006</a> par défaut.</p><blockquote><p><strong>En mode debug, si vous mettez un point d&rsquo;arrêt</strong>:</p><ul><li><p>dans la fonction <code>_do_train()</code> du fichier <code>ultralytics/engine/trainer.py</code>, vous serez au coeur de la boucle d&rsquo;apprentissage. A la ligne (environ) 336, vous pouvez voir le contenu du <em>batch</em>, vous voyez aussi à ce moment-là le modèle s&rsquo;exécuter sur le <em>batch</em> et renvoyer la <em>loss</em> associée.
<img src=images/train.png alt=Train></p></li><li><p>dans la fonction <code>__init__()</code> de la classe <code>DetectionModel</code> dans <code>ultralytics/nn/tasks.py</code>, vous êtes dans la création du réseau de détection. Regardez les autres fonctions (<code>_predict_augment</code> qui fait la <em>data augmentation</em> au moment de l&rsquo;apprentissage, ou encore la classe <code>BaseDataset</code> dans <code>ultralytics/data/base.py</code> qui construit le <em>dataloader</em> pour faire les <em>batches</em>).</p></li><li><p>dans un module en particulier (une convolution dans <code>ultralytics/nn/modules/conv.py</code>, la tête <code>Detect</code> dans <code>ultralytics/nn/modules/head.py</code>), vous pouvez aussi voir l&rsquo;exécution interne de ces couches au moment où elles sont appelées.</p></li><li><p>dans la fonction <code>forward()</code> de la <code>BboxLoss</code>, dans <code>ultralytics/utils/loss.py</code>, vous aurez le détail du calcul de la fonction d&rsquo;erreur de la détection prédite en fonction du label.</p></li></ul><p>Le <em>debug</em>, c&rsquo;est la vie. N&rsquo;hésitez pas à mettre des points d&rsquo;arrêt sur les traces de l&rsquo;apprentissage pour comprendre ce qui s&rsquo;y joue.</p></blockquote><h3 id=feature-visualization>Feature visualization</h3><p>Quand vous exécutez un modèle sur une image, vous pouvez visualiser les <em>feature maps</em> produites par chaque couche en passant l&rsquo;option <code>visualize=True</code> lors de l&rsquo;inférence. Toutes les <em>feature maps</em> et leurs images sont sauvegardées dans un dossier <code>runs/detect/predict...</code>. Allez voir à quoi ça ressemble. C&rsquo;est &ldquo;intéressant&rdquo;, même s&rsquo;il y a bien longtemps que j&rsquo;ai arrêté d&rsquo;essayer d&rsquo;interpréter des <em>feature maps</em>.</p><p><img src=images/stage21_C2f_features.png alt="Feature maps"></p><h2 id=configuration-du-workspace>Configuration du <em>workspace</em></h2><p>Désolée, les couleurs des scripts <em>bash</em> sont moches.</p><h5 id=si-vous-êtes-sur-vos-machines-personnelles-penguin>Si vous êtes sur vos machines personnelles &#x1f427;</h5><p>C&rsquo;est très simple et vous avez tous les droits, parce que c&rsquo;est vous le.a patron.ne :</p><pre tabindex=0><code>cd &lt;ou_vous_voulez_mettre_yolo&gt;
git clone --recursive https://github.com/ultralytics/ultralytics.git
cd ultralytics
pip install -e .
pip install tensorboard
</code></pre><p>Et hop, c&rsquo;est plié (normalement, parce qu&rsquo;il y a toujours des erreurs auxquelles on s&rsquo;attend pas).</p><h5 id=si-vous-êtes-sur-les-machines-de-linsa-snail>Si vous êtes sur les machines de l&rsquo;INSA &#x1f40c;</h5><p>Il faut installer l&rsquo;environnement virtuel qui va bien pour vous donner les droits d&rsquo;installation des différentes librairies Python :</p><pre tabindex=0><code>(base) labi@srv-tp06:~/Bureau$ git clone --recursive https://github.com/ultralytics/ultralytics.git
Clonage dans &#39;ultralytics&#39;...
remote: Enumerating objects: 20767, done.
remote: Counting objects: 100% (24/24), done.
remote: Compressing objects: 100% (22/22), done.
remote: Total 20767 (delta 4), reused 6 (delta 1), pack-reused 20743
Réception d&#39;objets: 100% (20767/20767), 11.09 Mio | 10.19 Mio/s, fait.
Résolution des deltas: 100% (14636/14636), fait.
Mise à jour des fichiers: 100% (767/767), fait.
(base) labi@srv-tp06:~/Bureau$ cd ultralytics/
(base) labi@srv-tp06:~/Bureau/ultralytics$ python -m venv .env
(base) labi@srv-tp06:~/Bureau/ultralytics$ source .env/bin/activate
(.env) (base) labi@srv-tp06:~/Bureau/ultralytics$ pip install -e .
(.env) (base) labi@srv-tp06:~/Bureau/ultralytics$ pip install tensorboard
</code></pre><p>Une fois dans VSCode, ouvrez le dossier <code>ultralytics</code> et avec la palette de commande <code>Ctrl+Shift+P</code> choisissez l&rsquo;environnement virtuel que vous venez de créer (<code>Python: Sélectionner l'interpréteur</code> > <code>Python 3.11 (.env)</code>).</p><h5 id=si-vous-êtes-sur-vos-machines-personnelles-mais-que-vous-voulez-travailler-sur-les-machines-de-linsa-exploding_head>Si vous êtes sur vos machines personnelles mais que vous voulez travailler sur les machines de l&rsquo;INSA &#x1f92f;</h5><p>Vous pouvez le faire grâce à l&rsquo;extension <code>Ctrl+Shift+X</code> de VSCode qui s&rsquo;appelle <code>Remote-SSH</code>, et en ouvrant depuis votre VSCode local une connexion avec <code>srv-ens-calcul.insa-toulouse.fr</code>. Il faut que le VPN soit activé, <em>of course</em>. La configuration SSH ressemble à ça :</p><pre tabindex=0><code>Host srv-ens-calcul
  Hostname srv-ens-calcul.insa-toulouse.fr
  User &lt;votre_username_INSA&gt;
</code></pre><h5 id=si-vous-êtes-sur-les-machines-de-linsa-et-que-vous-voulez-accéder-à-distance-à-vos-machines-personnelles-clown_face>Si vous êtes sur les machines de l&rsquo;INSA et que vous voulez accéder à distance à vos machines personnelles &#x1f921;</h5><p>C&rsquo;est la même idée dans l&rsquo;autre sens, mais faut pas abuser.</p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/clairelabitbonis/clairelabitbonis.github.io/edit/main/content/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/02_YOLO/index.fr.md title="Améliorez cette page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Améliorez cette page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/01_all_you_need_is_data/ title="DLCV2 | All you need is data" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Précédent</div><div class=next-prev-text>DLCV2 | All you need is data</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table des matières</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#présentation>Présentation</a></li><li><a href=#quête-principale>Quête principale</a><ul><li><a href=#labellisez-frowning_face>Labellisez</a></li><li><a href=#lapprentissage-dont-vous-êtes-le-héros>L&rsquo;apprentissage dont vous êtes le héros</a></li><li><a href=#3-2-1-développez->3, 2, 1, développez !</a></li></ul></li><li><a href=#quêtes-secondaires>Quêtes secondaires</a><ul><li><a href=#training-coco128>Training COCO128</a></li><li><a href=#feature-visualization>Feature visualization</a></li></ul></li><li><a href=#configuration-du-workspace>Configuration du <em>workspace</em></a><ul><li><ul><li></li></ul></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#about>A propos</a></li><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contactez moi :</h5><ul><li><a href=mailto:clairelabitbonis@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>clairelabitbonis@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Alimenté par
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script><script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>