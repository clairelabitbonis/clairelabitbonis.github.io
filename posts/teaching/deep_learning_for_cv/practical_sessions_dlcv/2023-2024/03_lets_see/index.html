<!doctype html><html><head><title>DLCV2.3 | Tadaaaam !</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png><meta property="og:title" content="DLCV2.3 | Tadaaaam !"><meta property="og:description" content="Présentation Bonjour, bonjour ! Aujourd&rsquo;hui, l&rsquo;objectif est d&rsquo;évaluer les performances de YOLOv8 sur le dataset qu&rsquo;on a construit ensemble. On ne sera que deux encadrant.e.s pour les trois groupes, alors ça va être sportif. Mais YOLO, on a pas peur.
Comme indiqué dans le sujet de présentation, vous devrez rendre début janvier une vidéo d&rsquo;une dizaine de minutes qui décrit votre travail de TP, et votre analyse des performances de YOLO sur le dataset."><meta property="og:type" content="article"><meta property="og:url" content="http://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/03_lets_see/"><meta property="og:image" content="http://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/03_lets_see/featured.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-12-21T10:00:00+09:00"><meta property="article:modified_time" content="2023-12-21T10:00:00+09:00"><meta name=description content="DLCV2.3 | Tadaaaam !"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png alt=Logo>
Claire Labit-Bonis</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20>
</a><a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20>
</a><a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Chercher data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Articles</a></li><div class=subtree><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/>Enseignements</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/3d_perception/>Perception 3D</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/>Travaux pratiques</a><ul><li><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/cc_segmentation/ title=Segmentation>Segmentation</a></li><li><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/ title="Localisation mono.">Localisation mono.</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/>Deep Learning & CV</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/>Cours</a><ul><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/01_why_deep_learning/ title="00 | De l'IA au DL">00 | De l'IA au DL</a></li><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/02_lets_learn_deeply/ title="01 | DD into OD">01 | DD into OD</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/>Travaux pratiques</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/>2022-2023</a><ul><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/00_presentation/ title="00 | Présentation">00 | Présentation</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/02_yolo/ title="01 | YOLO !">01 | YOLO !</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2022-2023/03_lets_see/ title="10 | Voyons...">10 | Voyons...</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/>2023-2024</a><ul class=active><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/00_presentation/ title="00 | Présentation">00 | Présentation</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/01_all_you_need_is_data/ title="01 | All you need is data">01 | All you need is data</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/02_yolo/ title="02 | YOLO !">02 | YOLO !</a></li><li><a class=active href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/03_lets_see/ title="03 | Tadaaaam !">03 | Tadaaaam !</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/04_sum_up/ title="04 | On récapitule tout">04 | On récapitule tout</a></li></ul></li></ul></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/03_lets_see/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/claire_hufa4d5753e97ed9e5e63c04dfec44ec48_673603_120x120_fit_q75_box.jpg alt="Author Image"><h5 class=author-name>Claire Labit-Bonis</h5><p>jeudi 21 décembre 2023</p></div><div class=title><h1>DLCV2.3 | Tadaaaam !</h1></div><div class=post-content id=post-content><h2 id=présentation>Présentation</h2><p>Bonjour, bonjour ! Aujourd&rsquo;hui, l&rsquo;objectif est d&rsquo;évaluer les performances de YOLOv8 sur le <em>dataset</em> qu&rsquo;on a construit ensemble. On ne sera que deux encadrant.e.s pour les trois groupes, alors ça va être sportif. Mais YOLO, on a pas peur.</p><p>Comme indiqué dans le sujet de <a href=https://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/00_presentation/>présentation</a>, vous devrez rendre début janvier une vidéo d&rsquo;une dizaine de minutes qui décrit votre travail de TP, et votre analyse des performances de YOLO sur le <em>dataset</em>.</p><p>Pour cela, vous allez aujourd&rsquo;hui procéder à :</p><ul><li>une <strong>analyse quantitative</strong> des performances : interpréter des courbes de précision/rappel, des matrices de confusion, des temps de calcul, des tailles de réseau ;</li><li>une <strong>analyse qualitative</strong> des performances : afficher les détections sur la base de <em>train</em>, de <em>test</em>, sur une nouvelle vidéo, sur de nouvelles images. Voir dans quels cas ça marche, dans quels cas ça marche pas ;</li></ul><p>Ces analyses doivent être faites à un niveau <strong>micro</strong> où vous allez vous préoccuper de votre propre classe d&rsquo;objet (celle que vous avez annotée) et comparer les différentes configurations de modèles, mais aussi à un niveau <strong>macro</strong> où vous allez vous comparer aux autres classes.</p><div class="alert alert-danger"><strong>Il n&rsquo;y a pas que ces analyses qui sont demandées pour le rendu. Il faut aussi parler du <em>dataset</em>, de votre propre expérience, etc. Regardez bien le sujet de <a href=https://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/00_presentation/>présentation</a>.</strong></div><p>Le sujet d&rsquo;aujourd&rsquo;hui est divisé en deux parties :</p><ul><li><a href=#il-est-tres-beau-le-dataset>partie 1</a> : un compte-rendu du <em>dataset</em> ainsi qu&rsquo;une présentation des différents entrainements de YOLO, pour que vous puissiez avoir une vue globale des données, et si j&rsquo;ai eu le temps je les aurai comparées au <em>dataset</em> de l&rsquo;année précédente (on se voit dans seulement quelques heures, alors je pense honnêtement qu&rsquo;on est laaaaaarge au niveau du temps qu&rsquo;il me reste pour rédiger des choses&mldr;) ;</li><li><a href=#et-donc-%C3%A7a-marche>partie 2</a> : la description des scripts que j&rsquo;ai écrits pour vous faciliter la vie, parce que je suis sympa.</li></ul><h2 id=il-est-tres-beau-le-dataset>Il est TRES beau le <em>dataset</em></h2><h3 id=répartition-des-classes>Répartition des classes</h3><p>Ensemble, on a construit un TRES beau <em>dataset</em> qui nous permet de détecter plein de classes d&rsquo;objets très utiles, bravo. On pourra par exemple automatiquement dire si on regarde un clip de JuL, de Damso, si on se balade en forêt à l&rsquo;automne, ou si Dwayne Johnson se cache derrière un buisson - et ça, c&rsquo;est vraiment super.</p><p>Le tableau ci-dessous indique le nombre d&rsquo;images et la répartition des différentes classes entre les ensembles de <em>train</em>, <em>validation</em> et <em>test</em> :</p><center><img src=images/dataset_2023-2024.png alt=Dataset width=80%></center><p>Et en voilà une petite illustration &#x2764;&#xfe0f; :</p><center><img src=images/medley_dlcv_2023-2024.png alt=Dataset width=80%></center><h3 id=analyse-des-labels>Analyse des labels</h3><p>La quantité de labels par image et leur forme varie en fonction des classes annotées. Par exemple, les images de <code>prise</code> contiennent beaucoup plus d&rsquo;instances que la classe <code>ballon</code>, pour un nombre équivalent d&rsquo;images. Par ailleurs, et en mettant ça en regard de l&rsquo;année passée, les objets ont globalement été plus répartis sur l&rsquo;ensemble de l&rsquo;image, là où ceux de l&rsquo;année dernière étaient très au centre (figures en bas à gauche). Ma première intuition serait de dire que cette année on s&rsquo;est accordés plus de libertés que l&rsquo;année dernière sur le fait que les objets devaient être bien centrés, ne pas dépasser, etc. Un peu plus en mode <em>yolo</em> quoi.</p><p>On voit aussi que les labels de l&rsquo;année dernière étaient en majorité verticaux, là où cette année on est un peu plus sur des boîtes horizontales (figures en bas à droite).</p><center><img src=images/labels_2022-2023.png alt=Labels width=45%>
<img src=images/labels_2023-2024.jpg alt=Labels width=45.1%></center><h2 id=et-donc-ça-marche->Et donc, ça marche ?</h2><p>Pour le savoir, vous pourrez utiliser deux scripts (que j&rsquo;ai codés, donc ils sont certainement buggés, c&rsquo;est OOOOKKAAAAAY. N&rsquo;hésitez pas à les modifier pour qu&rsquo;ils répondent parfaitement à votre besoin ! J&rsquo;ai même mis des commentaires&mldr;) :</p><ul><li><a href=files/qualitative_dlcv.py>qualitative_dlcv.py</a> : permet de visualiser l&rsquo;application d&rsquo;un réseau sur différents types de sources ;</li><li><a href=files/quantitative_dlcv.py>quantitative_dlcv.py</a> : permet de générer des métriques de performances pour une configuration donnée.</li></ul><p>En parlant de configurations, cette semaine j&rsquo;ai pu entrainer <a href=https://docs.ultralytics.com/models/yolov8/#supported-tasks-and-modes>plusieurs versions de YOLO (N, S, M, L)</a> qui diffèrent par leur nombre de filtres par couche et autres paramètres qui en font des réseaux plus ou moins gros (vous pouvez aller voir le fichier <code>ultralytics/cfg/models/v8/yolov8.yaml</code> pour plus de détails), avec des résolutions d&rsquo;entrée différentes (de 320 à 1980), et en figeant ou non le <em>backbone</em> avant l&rsquo;apprentissage. Au final, on obtient les configurations suivantes :</p><center><img src=images/configurations.png alt=Labels width=60%></center><p>Vous pouvez télécharger le <a href="https://drive.google.com/file/d/1phuoYoy2C7jfXZXp0cSHeISskYDgtWRr/view?usp=drive_link">fichier zip</a> qui contient tous les <em>runs</em> correspondant à ces apprentissages. Ce dossier, <code>runs</code>, est à placer à la racine d&rsquo;<code>ultralytics</code>. Chaque sous-dossier contient une configuration donnée, accompagnée de ses courbes d&rsquo;apprentissage, de ses matrices de confusion et de ses poids (dans <code>weights</code>).</p><p>Vous pouvez également télécharger le fichier <a href=files/dlcv_2023-2024.yaml>dlcv_2023-2024.yaml</a> dont vous avez besoin pour lancer l&rsquo;évaluation quantitative, (paramètre <code>--data</code>, que l&rsquo;on retrouve aussi à l&rsquo;apprentissage). Ce fichier est à placer dans <code>ultralytics/cfg/datasets</code>, à côté des autres configurations de <em>datasets</em>.</p><p>Si vous voulez exécuter les scripts en mode <code>debug</code>, je vous donne le fichier <a href=files/launch.json>.json</a> qui va bien. Comme ça vous pouvez directement modifier les <em>args</em> dans le fichier de configuration. Il est à placer dans le dossier <code>.vscode</code>.</p><h3 id=analyse-qualitative>Analyse qualitative</h3><p>Le script <code>qualitative_dlcv.py</code> prend en entrées 4 paramètres :</p><ul><li><code>weights</code> qui sera le chemin vers le fichier <code>.pt</code> issu de l&rsquo;apprentissage que vous voulez évaluer ;</li><li><code>source</code> pour indiquer l&rsquo;entrée que vous donnerez au réseau. Vous pouvez mettre soit <code>folder</code>, <code>txt_file</code>, <code>video</code> ou <code>webcam</code> pour indiquer que vous donnerez un dossier avec des images <code>.jpg</code> à l&rsquo;intérieur, un fichier similaire à <code>train.txt</code> avec les chemins vers les images, une vidéo ou une webcam ;</li><li><code>path</code> nécessaire dans le cas où vous avez mis <code>video</code>, <code>folder</code> ou <code>txt_file</code> ;</li><li><code>display_size</code> : la taille à laquelle l&rsquo;image sera retaillée avant d&rsquo;être passée au réseau et affichée.</li></ul><p>Quelques exemples d&rsquo;appels :</p><pre tabindex=0><code>python qualitative_dlcv.py --weights runs/detect/yolov8s_640_freeze/weights/best.pt --source folder --path /scratch/labi/DLCV/2023-2024/dataset --display_size 640

python qualitative_dlcv.py --weights runs/detect/yolov8m_320_nofreeze/weights/best.pt --source video --path video.mp4 --display_size 320
</code></pre><p>Si vous voulez avoir accès au <em>dataset</em> complet dans <code>/scratch/labi/DLCV/2023-2024/dataset</code>, il faut que vous soyez en <a href=https://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/02_yolo/#si-vous-%C3%AAtes-sur-vos-machines-personnelles-mais-que-vous-voulez-travailler-sur-les-machines-de-linsa-exploding_head>Remote-SSH</a> sur le <code>srv-gei-gpu2</code>. Si vous restez en local, vous devez avoir un dossier qui contient des images, ou une vidéo, ou un fichier texte avec les chemins vers les images, ou&mldr; une webcam.</p><h3 id=analyse-quantitative>Analyse quantitative</h3><p>Pour l&rsquo;analyse quantitative, vous avez dans tous les sous-dossiers de <code>runs</code> les matrices de confusion et courbes en tout genre qui vous permettront d&rsquo;analyser les performances des configurations entrainées.</p><p>Vous pouvez également utiliser le script <code>quantitative_dlcv.py</code> qui prend en entrées plusieurs paramètres.</p><p>Exemple : j&rsquo;ai une vidéo de test avec ses labels, exportés depuis CVAT, et je veux effectuer une analyse quantitative dessus.</p><p>D&rsquo;abord, l&rsquo;arborescence des fichiers et le <code>.txt</code> associé (ignorez les dossiers &ldquo;preparation&rdquo; et &ldquo;videos_for_cvat&rdquo;, j&rsquo;ai juste pas eu le temps de les enlever&mldr;) :</p><center><img src=images/arbo_videotest_img.png alt=Labels width=30%>
<img src=images/arbo_videotest_labels.png alt=Labels width=25.2%>
<img src=images/videotest_txt.png alt=Labels width=80%></center><p>Si vous créez un nouveau <code>.json</code> pour tester ce <em>dataset</em> en particulier (à placer dans <code>ultralytics/cfg/datasets</code>), et que vous exécutez le script avec les bons paramètres, vous aurez une évaluation d&rsquo;un modèle donné sur ce <em>dataset</em>-là :</p><center><img src=images/quantitative_evaluation.png alt=Labels width=100%></center><h3 id=pour-info>Pour info</h3><p>Je vous ai mis un <a href=files/train_dlcv.py>script d&rsquo;apprentissage</a> formaté de la même manière, si ça peut vous être utile (a priori pas d&rsquo;apprentissage aujourd&rsquo;hui).</p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/clairelabitbonis/clairelabitbonis.github.io/edit/main/content/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/03_lets_see/index.fr.md title="Améliorez cette page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Améliorez cette page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/02_yolo/ title="DLCV2.2 | Le Bingo de YOLO !" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Précédent</div><div class=next-prev-text>DLCV2.2 | Le Bingo de YOLO !</div></a></div><div class="col-md-6 next-article"><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/2023-2024/04_sum_up/ title="DLCV2.4 | On récapitule tout" class="btn btn-outline-info"><div>Suivant <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>DLCV2.4 | On récapitule tout</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table des matières</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#présentation>Présentation</a></li><li><a href=#il-est-tres-beau-le-dataset>Il est TRES beau le <em>dataset</em></a><ul><li><a href=#répartition-des-classes>Répartition des classes</a></li><li><a href=#analyse-des-labels>Analyse des labels</a></li></ul></li><li><a href=#et-donc-ça-marche->Et donc, ça marche ?</a><ul><li><a href=#analyse-qualitative>Analyse qualitative</a></li><li><a href=#analyse-quantitative>Analyse quantitative</a></li><li><a href=#pour-info>Pour info</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#about>A propos</a></li><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contactez moi :</h5><ul><li><a href=mailto:clairelabitbonis@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>clairelabitbonis@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Alimenté par
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script><script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>