<!doctype html><html><head><title>DLCV 2ème édition | Présentation</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png><meta property="og:title" content="DLCV 2ème édition | Présentation"><meta property="og:description" content="&#x1f57a; Eeeeet zééé bartiiii pour la deuxième édition du cours de Deep Learning en 5-SDBD. L&rsquo;année dernière a été un franc succès, alors j&rsquo;ai hâte qu&rsquo;on s&rsquo;y remette cette année &#x1f604; Objectifs pédagogiques L&rsquo;objectif de ces séances de travaux pratiques est de toucher à toutes les étapes de l&rsquo;ingénierie du deep learning, à savoir :
l&rsquo;acquisition et l&rsquo;annotation de données, l&rsquo;apprentissage de réseaux de convolution, l&rsquo;évaluation des performances de la tâche apprise, la visualisation des résultats obtenus."><meta property="og:type" content="article"><meta property="og:url" content="http://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2023-2024/00_presentation/"><meta property="og:image" content="http://clairelabitbonis.github.io/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2023-2024/00_presentation/feature_d.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-12-05T10:00:00+09:00"><meta property="article:modified_time" content="2023-12-05T10:00:00+09:00"><meta name=description content="DLCV 2ème édition | Présentation"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png alt=Logo>
Claire Labit-Bonis</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20>
</a><a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20>
</a><a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Chercher data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Articles</a></li><div class=subtree><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/>Enseignements</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/3d_perception/>Perception 3D</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/>Travaux pratiques</a><ul><li><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/cc_segmentation/ title=Segmentation>Segmentation</a></li><li><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/ title="Localisation mono.">Localisation mono.</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/>Deep Learning & CV</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/deep_learning_for_cv/condensed/>Condensé</a><ul><li><a href=/posts/teaching/deep_learning_for_cv/condensed/00_presentation/ title="00 | Présentation">00 | Présentation</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/insa/>INSA</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/deep_learning_for_cv/insa/course_dlcv/>Cours</a><ul><li><a href=/posts/teaching/deep_learning_for_cv/insa/course_dlcv/01_why_deep_learning/ title="00 | De l'IA au DL">00 | De l'IA au DL</a></li><li><a href=/posts/teaching/deep_learning_for_cv/insa/course_dlcv/02_lets_learn_deeply/ title="01 | DD into OD">01 | DD into OD</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/>Travaux pratiques</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2022-2023/>2022-2023</a><ul><li><a href=/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2022-2023/00_presentation/ title="00 | Présentation">00 | Présentation</a></li><li><a href=/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2022-2023/02_yolo/ title="01 | YOLO !">01 | YOLO !</a></li><li><a href=/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2022-2023/03_lets_see/ title="10 | Voyons...">10 | Voyons...</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2023-2024/>2023-2024</a><ul class=active><li><a class=active href=/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2023-2024/00_presentation/ title="00 | Présentation">00 | Présentation</a></li><li><a href=/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2023-2024/01_all_you_need_is_data/ title="01 | All you need is data">01 | All you need is data</a></li><li><a href=/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2023-2024/02_yolo/ title="02 | YOLO !">02 | YOLO !</a></li><li><a href=/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2023-2024/03_lets_see/ title="03 | Tadaaaam !">03 | Tadaaaam !</a></li><li><a href=/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2023-2024/04_sum_up/ title="04 | On récapitule tout">04 | On récapitule tout</a></li></ul></li></ul></li></ul></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2023-2024/00_presentation/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/claire_hufa4d5753e97ed9e5e63c04dfec44ec48_673603_120x120_fit_q75_box.jpg alt="Author Image"><h5 class=author-name>Claire Labit-Bonis</h5><p>mardi 5 décembre 2023</p></div><div class=title><h1>DLCV 2ème édition | Présentation</h1></div><div class=post-content id=post-content><div class="alert alert-success"><strong>&#x1f57a; Eeeeet zééé bartiiii pour la deuxième édition du cours de Deep Learning en 5-SDBD. L&rsquo;année dernière a été un franc succès, alors j&rsquo;ai hâte qu&rsquo;on s&rsquo;y remette cette année &#x1f604;</strong></div><h2 id=objectifs-pédagogiques>Objectifs pédagogiques</h2><p>L&rsquo;objectif de ces séances de travaux pratiques est de toucher à toutes les étapes de l&rsquo;ingénierie du <em>deep learning</em>, à savoir :</p><ul><li>l&rsquo;<strong>acquisition</strong> et l&rsquo;<strong>annotation</strong> de données,</li><li>l&rsquo;<strong>apprentissage</strong> de réseaux de convolution,</li><li>l&rsquo;<strong>évaluation</strong> des performances de la tâche apprise,</li><li>la <strong>visualisation</strong> des résultats obtenus.</li></ul><p>Pour cela, notre point de départ sera le détecteur d&rsquo;objets très largement connu et utilisé par les communautés scientifique mais aussi industrielle : <a href=https://arxiv.org/pdf/1506.02640.pdf>YOLO</a> (You Only Look Once). L&rsquo;année dernière, nous avions utilisé la <a href=https://github.com/ultralytics/yolov5>version 5</a> sortie en 2020. Cette année, nous nous attaquons à la <a href=https://github.com/ultralytics/ultralytics>version 8</a> sortie en 2022. Tout va trop vite, YOLOv8 est déjà dépassé par <a href=https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md>YOLO-NAS</a>, mais, eh, on peut pas être partout.</p><blockquote><p>A la fin des TPs, vous saurez comment utiliser YOLOv8, comment l&rsquo;entrainer sur vos propres données, et comment l&rsquo;évaluer.</p></blockquote><blockquote><p>L&rsquo;année dernière, on nous a dit &ldquo;les messages en bleu on croyait que ça servait à rien alors on les a pas lus&rdquo;. Du coup j&rsquo;ai essayé d&rsquo;autres balises, mais elles sont moches. Je vais essayer de limiter les messages bleus.</p></blockquote><h2 id=déroulement-des-séances>Déroulement des séances</h2><p><strong>Un·e pour tou·te·s, tou·te·s pour un·e !</strong></p><p>Parce que l&rsquo;union fait la force, que la joie et la bonne humeur facilitent l&rsquo;apprentissage &ndash; des humains &ndash;, les séances de travaux pratiques se dérouleront dans un contexte à la fois collectif et individuel, pas toujours sur les postes de travail, et toujours dans l&rsquo;intérêt de la compréhension. Nous aurons tou·te·s un rôle à jouer, à chacune des étapes.</p><p>Nous allons entrainer YOLOv8 à détecter plusieurs classes d&rsquo;objets, à raison d&rsquo;une classe d&rsquo;objet par binôme. Le <em>dataset</em> que nous allons construire pour cela sera commun aux trois groupes de TPs qui se déroulent en parallèle et/ou en différé selon les problématiques d&rsquo;emploi du temps, de disponibilité de salle, d&rsquo;organisation. Bref, trois groupes. Même <em>dataset</em>.</p><p>Pour cela, nous passerons par plusieurs étapes :</p><ul><li>&#x1f525; <mark><strong>étape 1 - acquisition</strong></mark> : chaque binôme prendra plusieurs séquences vidéo de la classe d&rsquo;objets qu&rsquo;il aura choisie parmi une liste proposée, et la mettra sur un serveur de données commun à tous les groupes ;</li><li>&#x1f525; <mark><strong>étape 2 - annotation</strong></mark> : avec l&rsquo;outil CVAT, chaque binôme annotera ses propres séquences d&rsquo;images avec la classe d&rsquo;objet choisie, de sorte qu&rsquo;à la fin de la phase d&rsquo;annotation, le groupe entier de TP aura collectivement construit un <em>dataset</em> multi-classes dont tout le monde bénéficiera pour faire ses apprentissages ;</li><li>&#x1f525; <mark><strong>étape 3 - prise en main du code de YOLOv8</strong></mark> : à la fin de cette étape, vous saurez appliquer sur vos propres images un modèle YOLOv8-S pré-entrainé sur COCO, entrer dans l&rsquo;architecture du réseau, visualiser les <em>feature maps</em>, etc.</li><li>&#x1f525; <mark><strong>étape 4 - apprentissage de YOLOv8 sur notre <em>dataset</em></strong></mark> : l&rsquo;idéal pour analyser les performances d&rsquo;un jeu de paramètres donné (résolution des images d&rsquo;entrée, taille du réseau, taille de <em>batch</em>, etc.) est de lancer autant d&rsquo;apprentissages que de configurations possibles et de les comparer ensuite pour sélectionner celle qui est la meilleure. On peut ensuite afficher sur un même graphe différentes tailles de modèles, pour différentes résolutions, et comparer leur rapidité d&rsquo;exécution à la <em>mean average precision</em> qu&rsquo;ils réalisent sur un <em>dataset</em> donné, par exemple :</li></ul><center><p><img src=images/perfs_yolov5.png alt="Comparaison des performances de YOLOv5 sur COCO">
<em>Source : <a href=https://github.com/ultralytics/yolov5>https://github.com/ultralytics/yolov5</a></em></p></center><ul><li>&#x1f525; <mark><strong>étape 5 - analyse des performances</strong></mark> : une fois tous les apprentissages faits, chaque binôme pourra évaluer les performances de sa propre configuration, analyser les résultats de manière quantitative, <em>i.e.</em>, avec des chiffres, et de manière qualitative, <em>i.e.</em>, avec une visualisation &ldquo;à l&rsquo;oeil&rdquo; des cas d&rsquo;erreur et des cas qui fonctionnent. Une évaluation comparative sera également réalisable, puisque tout le monde aura accès à toutes les configurations entrainées.</li></ul><blockquote><p><strong>Un apprentissage dure plusieurs heures.</strong></p><p>Pour pouvoir comparer toutes ces configurations, il faut soit disposer de plusieurs serveurs GPU puissants qui peuvent faire tourner en parallèle plusieurs configurations, soit disposer de beaucoup de temps et être patient&mldr;</p><p><strong>Un apprentissage consomme de l&rsquo;énergie.</strong></p><p>Il n&rsquo;est donc pas envisageable que chaque binôme lance un apprentissage pour chaque jeu de paramètres puis fasse une analyse comparative des résultats.</p><p><strong>MAIS ! On peut rendre le problème moins complexe.</strong></p><p>Nous vous fournirons un sous-ensemble du <em>dataset</em> pour que vous puissiez apprendre à manipuler une base d&rsquo;apprentissage, de validation et de test. Vous pourrez lancer un mini-apprentissage en local sur CPU pour quelques <em>epochs</em>.</p><p>Pour l&rsquo;évaluation des résultats, chaque binôme se positionnera sur une configuration tirée au sort (résolution d&rsquo;image/taille de réseau/poids figés ou non) et pourra analyser sa configuration et les performances de sa classe d&rsquo;objets par rapport aux autres configurations et autres classes d&rsquo;objets.</p></blockquote><h2 id=modalités-dévaluation>Modalités d&rsquo;évaluation</h2><div class="alert alert-danger"><strong><p>La <strong>deadline</strong> est fixée au 12 janvier 2024 à 20h. Vous devez déposer <a href="https://moodle.insa-toulouse.fr/course/view.php?id=1154&amp;section=4">sur Moodle</a> :</p><ul><li>votre capsule vidéo au format <code>.mp4</code>.</li></ul><p>Le ne doit pas dépasser 256Mo.</p></strong></div><h3 id=capsule-vidéo>Capsule vidéo</h3><p>Chaque binôme produira une capsule vidéo (c&rsquo;est-à-dire une séquence vidéo) d&rsquo;environ 10 minutes. Bien sûr, la pertinence du contenu importe plus que la longueur de la capsule, même si la consigne de temps est à respecter.</p><blockquote><p>Si les 256 Mo sont trop difficiles à atteindre, donnez-nous un lien <a href=https://youtu.be/eBGIQ7ZuuiU>Youtube vers votre vidéo par exemple</a>. Ou Drive. Ou autre. On est à l&rsquo;ère du numérique, on trouvera une solution.</p></blockquote><p>Le tableau d&rsquo;analyse ci-dessous peut vous aider à structurer votre analyse et vous donne une idée des points qui seront évalués :</p><center><p><img src=images/modalites_evaluation.png alt="Tableau d&rsquo;analyse"></p></center><p>Globalement, on aura :</p><ul><li>les <mark>trois grandes thématiques</mark> vues en cours et en TP peuvent donner lieu à analyse :<ul><li>le <em><strong>dataset</strong></em> (acquisition, annotation),</li><li>l&rsquo;<strong>apprentissage</strong>,</li><li>et l&rsquo;<strong>inférence</strong> (= la détection appliquée à des données inconnues ou de test avec le modèle entrainé) ;</li></ul></li><li><mark>trois types d&rsquo;analyse</mark> possibles :<ul><li>l&rsquo;<strong>analyse quantitative</strong> qui consiste à analyser la répartition des classes, le nombre d&rsquo;instances annotées, présenter des chiffres, interpréter des courbes de résultats, des temps d&rsquo;exécution du réseau en fonction de sa taille, de la résolution image, commenter les métriques de performance en fonction du <em>split</em>, de la classe, etc.,</li><li>l&rsquo;<strong>analyse qualitative</strong> qui consiste à analyser visuellement les performances d&rsquo;une configuration de réseau donnée, par exemple se rendre compte que les petits réseaux ont plus de mal à détecter les petits objets, que la classe <code>velo</code> l&rsquo;année dernière était mieux détectée quand le vélo prenait toute l&rsquo;image (faire le lien avec l&rsquo;annotation qui a été faite), ou que les petits objets circulaires et orangés étaient souvent détectés comme des lentilles (exemple : oeil de pigeon&mldr;),</li><li>le <strong>retour d&rsquo;expérience</strong> : je vous demande votre avis ! Comment vous êtes-vous réparti l&rsquo;annotation dans le binôme ? Entre différents binômes qui ont annoté la même classe ? Quelles difficultés avez-vous rencontrées ? Quelles questions vous êtes-vous posées au moment de l&rsquo;acquisition ? Que feriez-vous différemment ? Quelle a été votre stratégie pour choisir vos exemples d&rsquo;inférence ? Comment avez-vous décidé de la répartition entre les différents <em>splits</em> de <em>train</em>, <em>val</em> et <em>test</em> ?</li></ul></li><li><mark>deux niveaux d&rsquo;analyse</mark> :<ul><li><strong>micro</strong> : vous pouvez analyser votre classe d&rsquo;annotation (métriques de performance sur train/val/test, quantité annotée, type d&rsquo;annotation, nombre d&rsquo;instances par image, stratégie d&rsquo;annotation, etc.), les performances fines de votre configuration de réseau* (temps d&rsquo;exécution, taille du réseau, place mémoire, nombre de couches, de paramètres, analyse des courbes F1, PR, etc.),</li><li><strong>macro</strong> : vous <em>vs.</em> la promo entière et les autres configurations. Par exemple, une analyse plus globale qui compare les différentes courbes F1 des différents apprentissages &ndash; quelle est l&rsquo;influence de la taille du réseau ? Quelle configuration converge le mieux ? Le plus rapidement ? Laquelle gère mieux les petits objets ? Quelle classe se distingue des autres et pourquoi ? Quelle a été la dynamique de groupe globalement ? Quelles discussions avez-vous eues avec les autres binômes pour vous accorder sur une stratégie d&rsquo;annotation ?</li></ul></li></ul><p>*celle tirée au sort en TP &ndash; si elle était trop mauvaise, prenez-en une autre, pas grave, il faut pouvoir en dire quelque chose d&rsquo;autre que &ldquo;c&rsquo;est nul et ça marche pas&rdquo;&mldr;</p><blockquote><p>Il y a <strong>des millions de choses</strong> à dire et tout n&rsquo;est pas applicable à toutes les classes d&rsquo;objets, toutes les configurations ni tous les binômes. Etre exhaustif vous demanderait une analyse allant de 45 minutes à 2 jours (rien que ça). Soyez synthétiques et pertinents. Ce n&rsquo;est pas une tâche simple, mais il y a de la matière à votre disposition.</p></blockquote><blockquote><p><strong>Pas de contrainte sur le format</strong> : une vidéo d&rsquo;un PPT qui défile, une superproduction <em>full</em> 3D pour concurrencer Avatar 2 (à condition d&rsquo;avoir aussi un meilleur scénario&mldr;). L&rsquo;important est d&rsquo;avoir un contenu pertinent, et que ça ne soit pas la corvée du siècle pour vous. Amusez-vous, vendez-nous une analyse de rêve.</p></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/clairelabitbonis/clairelabitbonis.github.io/edit/main/content/posts/teaching/deep_learning_for_cv/INSA/practical_sessions_dlcv/2023-2024/00_presentation/index.fr.md title="Améliorez cette page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Améliorez cette page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2022-2023/03_lets_see/ title="DLCV-TP-10* | Voyons..." class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Précédent</div><div class=next-prev-text>DLCV-TP-10* | Voyons...</div></a></div><div class="col-md-6 next-article"><a href=/posts/teaching/deep_learning_for_cv/insa/practical_sessions_dlcv/2023-2024/01_all_you_need_is_data/ title="DLCV2.1 | All you need is data" class="btn btn-outline-info"><div>Suivant <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>DLCV2.1 | All you need is data</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table des matières</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#objectifs-pédagogiques>Objectifs pédagogiques</a></li><li><a href=#déroulement-des-séances>Déroulement des séances</a></li><li><a href=#modalités-dévaluation>Modalités d&rsquo;évaluation</a><ul><li><a href=#capsule-vidéo>Capsule vidéo</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#about>A propos</a></li><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contactez moi :</h5><ul><li><a href=mailto:clairelabitbonis@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>clairelabitbonis@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Alimenté par
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script><script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>