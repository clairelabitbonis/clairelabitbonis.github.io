<!doctype html><html><head><title>3DP-TP-01 | Monocular localization with iterative PnL</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png><meta property="og:title" content="3DP-TP-01 | Monocular localization with iterative PnL"><meta property="og:description" content="Iterative estimation of a camera extrinsic parameters."><meta property="og:type" content="article"><meta property="og:url" content="http://clairelabitbonis.github.io/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/"><meta property="og:image" content="http://clairelabitbonis.github.io/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/featured.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-07-16T08:06:25+06:00"><meta property="article:modified_time" content="2022-07-16T08:06:25+06:00"><meta name=description content="Iterative estimation of a camera extrinsic parameters."><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png alt=Logo>
Claire Labit-Bonis</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-gb"></span>
English</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/fr/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization><span class="flag-icon flag-icon-fr"></span>
Français</a></div></li><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/ title=Introduction>Introduction</a></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/>Teaching</a><ul class=active><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/3d_perception/>3D Perception</a><ul class=active><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/teaching/3d_perception/practical_sessions_3d_perception/>Practical sessions</a><ul class=active><li><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/cc_segmentation/ title=Segmentation>Segmentation</a></li><li><a class=active href=/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/ title="Mono. localization">Mono. localization</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/deep_learning_for_cv/>Deep Learning & CV</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/>Course</a><ul><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/01_why_deep_learning/ title="00 | From AI to DL">00 | From AI to DL</a></li><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/02_lets_learn_deeply/ title="01 | Let's learn deeply">01 | Let's learn deeply</a></li><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/03_all_you_need_is_data/ title="10 | Data is life">10 | Data is life</a></li><li><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/04_deep_learning_culture/ title="11 | Deep culture">11 | Deep culture</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/>Practical sessions</a><ul><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/ title="00 | Presentation">00 | Presentation</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/01_unity_is_strength/ title="01 | Unity is strength">01 | Unity is strength</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/02_yolo/ title="10 | YOLO!">10 | YOLO!</a></li><li><a href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/03_lets_see/ title="03 | Let's see...">03 | Let's see...</a></li></ul></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/claire_hu4fa84bbe5b0b14aeb1f21f53cdd20f7e_312167_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name>Claire Labit-Bonis</h5><p>Saturday, July 16, 2022</p></div><div class=title><h1>3DP-TP-01 | Monocular localization with iterative PnL</h1></div><div class=post-content id=post-content><blockquote><p>This exercise consists in filling empty functions, or parts of the <code>localisation.py</code> file:</p><ul><li><a href=#anchor-step-1>step 1</a> : <code>perspective_projection</code> and <code>transform_and_draw_model</code>,</li><li><a href=#anchor-step-2>step 2</a> : <code>calculate_normal_vector</code> and <code>calculate_error</code></li><li><a href=#anchor-step-3>step 3</a> : <code>partial_derivatives</code>,</li><li><a href=#anchor-step-4>step 4</a> : transformation towards a second point of view.</li></ul><p>This post describes the role of each function.</p></blockquote><div class="alert alert-danger"><strong>Do not dive headfirst into the code ! The first part &ldquo;goal description&rdquo; is just a global and theoretical presentation of the subject. Each step and its associated functions are described in details within their respective parts: <a href=#anchor-step-1>step 1</a>, <a href=#anchor-step-2>step 2</a>, <a href=#anchor-step-3>step 3</a>, <a href=#anchor-step-4>step 4</a>.</strong></div><h2 id=contents-download>Contents download</h2><p>Practical session files can be downloaded through your course on Moodle, or at <a href=files/files.zip>this link</a>.</p><h2 id=anchor-step-0>Goal description</h2><p>This exercise aims at finding the optimal transformation between a 3D model expressed in centimeters in an <em>object</em> coordinate system \(\mathcal{R_o} (X,Y,Z)\) (sometimes called <em>world</em> coordinate system \(\mathcal{R_w}\)) in order to draw it as an overlay in a 2D pixel image defined in its <em>image</em> coordinate system \(\mathcal{R_i} (u,v)\). The 3D model was downloaded from <a href=https://free3d.com>free3d</a> and modified within <a href=https://www.blender.org/>Blender</a> for the needs of this exercise. Points and edges were respectively exported <em>via</em> a Python script to the <code>pikachu.xyz</code> and <code>pikachu.edges</code> files.</p><p><img src=images/blender_pikachu.png alt="3D model made in Blender" class=center><div style=margin-top:1rem></div></p><p><code>pikachu.xyz</code> contains 134 lines and 3 columns, corresponding to the 134 model points and their three \(x, y, z\) coordinates.</p><p><code>pikachu.edges</code> contains 246 lines and 2 columns, corresponding to the 246 model edges and their 2 ends indices (for instance, the first line in <code>pikachu.edges</code> describes the first edge: its first point coordinates are at index 2 in the <code>pikachu.xyz</code> file, and its second point is at index 0).</p><p><img src=images/edges_xyz.png alt="Coordinates and edges files" class=center><div style=margin-top:1rem></div></p><p>By &ldquo;transformation&rdquo;, we mean scene rotation and translation along the \(x\), \(y\) and \(z\) axes for each point in \(\mathcal{R_o}\).</p><p>In our case, we want to perform augmented reality by positioning Pikachu&rsquo;s model over the sky-blue cube of the image below, and making it perfectly match with the virtual cube on which it is standing.</p><p><img src=images/final_objective.png alt="Goal to be reached" class=center><div style=margin-top:1rem></div></p><details><summary><strong>Intrinsic parameters. <span style=color:pink>Click to expand</span></strong></summary><p>We use the pinhole camera model allowing to perform this operation through two successive transformations: \(\mathcal{R_o} \rightarrow \mathcal{R_c}\) and \(\mathcal{R_c} \rightarrow \mathcal{R_i}\). Apart from \(\mathcal{R_o}\) and \(\mathcal{R_i}\) coordinate systems, we then must also considerate the camera frame \(\mathcal{R_c}\).</p><blockquote><p>The resource <em><a href=http://www.optique-ingenieur.org/fr/cours/OPI_fr_M04_C01/co/Grain_OPI_fr_M04_C01_2.html>Modélisation et calibrage d&rsquo;une caméra</a></em> describes the pinhole camera model in details, and can help understanding the exercise.</p></blockquote><p><img src=images/goal.png alt="Coordinate systems transformation" class=center><div style=margin-top:1rem></div></p><blockquote><p>Wikipedia defines the <a href=https://en.wikipedia.org/wiki/Pinhole_camera_model>pinhole model</a> as describing <em>the mathematical relationship between the coordinates of a point in three-dimensional space and its projection onto the image plane of an ideal pinhole camera, where the camera aperture is described as a point and no lenses are used to focus light</em>.</p></blockquote><p>The change between \(\mathcal{R_c}\) and \(\mathcal{R_i}\) coordinate systems is done thanks to the <strong>intrinsic</strong> camera parameters. These \((\alpha_u, \alpha_v, u_0, v_0)\) coefficients are then stored in a homogeneous transformation matrix \(K_{i \leftarrow c}\) so that we can describe the relation \(p_i = K_{i \leftarrow c}.P_c\) as:</p><p>$$
\begin{bmatrix}
u\\v\\1
\end{bmatrix}_{\mathcal{R}_i} = s.
\begin{bmatrix}
\alpha_u & 0 & u_0\\
0 & \alpha_v & v_0\\
0 & 0 & 1
\end{bmatrix} .
\begin{bmatrix}
X\\
Y\\
Z
\end{bmatrix}_{\mathcal{R}_c}
$$</p><p>with:</p><ul><li>\(p_i\) (on the left) the pixel point within the 2D image frame \(\mathcal{R_i}\),</li><li>\(P_c\) (on the right) the point in centimeters within the 3D camera frame \(\mathcal{R_c}\),</li><li>\(s = \frac{1}{Z}\),</li><li>\(\alpha_u = k_x f\), \(\alpha_v = k_y f\):<ul><li>\(k_x = k_y\) the sensor number of pixels per millimeter along \(x\) and \(y\) directions &ndash; the equality being true only if the pixels are square,</li><li>\(f\) the focal distance.</li></ul></li><li>\(u_0\) and \(v_0\) the image centers in pixels within \(\mathcal{R}_i\).</li></ul><p>In our case, the image was captured by a Canon EOS 700D sensor of size \(22.3\times14.9 mm\). The image size being \(720\times480 px\) and the focal distance \(18mm\), we deduce the parameters \(\alpha_u = 581.1659\), \(\alpha_v = 579.8657\), \(u_0 = 360\) and \(v_0 = 240\) stored in the <code>calibration_parameters.txt</code> file.</p></details><details><summary><strong>Extrinsic parameters. <span style=color:pink>Click to expand</span></strong></summary><p>To reach our goal <em>i.e.</em>, display our 3D model in the 2D image in pixels, we have to estimate the \(\mathcal{R_o}\rightarrow\mathcal{R_c}\) transformation coefficients and establish the relation \(P_c=M_{c\leftarrow o}.P_o\), with:</p><ul><li><p>\(M_{c\leftarrow o}=\big[ R_{\alpha\beta\gamma} | T \big]\) the homogeneous transformation matrix of the Euler angles and \(x\), \(y\) and \(z\) translation of the coordinate system.</p></li><li><p>\(R_{\alpha\beta\gamma}\) the rotation matrix resulting from the successive applications &ndash; and thus the multiplication between them &ndash; of the rotation matrices \(R_\gamma\), \(R_\beta\) and \(R_\alpha\):</p></li></ul><p>$$R_\alpha = \begin{bmatrix}1 & 0 & 0\\0 & \cos \alpha & -\sin \alpha\\0 & \sin \alpha & \cos \alpha\end{bmatrix}$$</p><p>$$R_\beta = \begin{bmatrix}\cos \beta & 0 & -\sin \beta\\0 & 1 & 0\\\sin \beta & 0 & \cos \beta\end{bmatrix}$$</p><p>$$R_\gamma = \begin{bmatrix}\cos \gamma & -\sin \gamma & 0\\\sin \gamma & \cos \gamma & 0\\0 & 0 & 1\end{bmatrix}$$</p><p>With the correct extrinsic parameters \((\alpha, \beta, \gamma, t_x, t_y, t_z)\), the 3D model in its coordinate system \(\mathcal{R_o}\) can be transformed into \(\mathcal{R_c}\) then \(\mathcal{R_i}\), while respecting the following relationship for each point \(P_o\):</p><p>$$ p_i = K_{i \leftarrow c} M_{c\leftarrow o} P_o$$</p><p>$$\begin{bmatrix}
u\\
v\\
1
\end{bmatrix}_{\mathcal{R}_i} = s.
\begin{bmatrix}
\alpha_u & 0 & u_0\\
0 & \alpha_v & v_0\\
0 & 0 & 1
\end{bmatrix} \bigodot
\begin{bmatrix}
r_{11} & r_{12} & r_{13} & t_x\\
r_{21} & r_{22} & r_{23} & t_y\\
r_{31} & r_{32} & r_{33} & t_z\\
0 & 0 & 0 & 1
\end{bmatrix} .
\begin{bmatrix}
X\\
Y\\
Z\\
1
\end{bmatrix}_{\mathcal{R}_o}
$$</p><div class="alert alert-warning"><strong>The \(\bigodot\) operator is the multiplication between terms after having removed the homogeneous coordinate from \(M_{c\leftarrow o} P_o\). It is only here so that matrices shapes match.</strong></div></details></br><blockquote><p>The instrinsic parameters are known; in order to find the extrinsic parameters, we use a localization method (<em>i.e.</em> of parameters estimation) called <strong>Perspective-n-Lines</strong>.</p></blockquote><h3 id=extrinsic-parameters-estimation>Extrinsic parameters estimation</h3><p>Our starting point is an initial coarse estimate of \((\alpha, \beta, \gamma, t_x, t_y, t_z)\). This initial estimate is more or less accurate depending on our <em>a priori</em> knowledge of the scene, the object and the position of the camera.</p><p>In our case, the initial parameters have values \(alpha = -2.1\), \(beta = 0.7\), \(gamma = 2.7\), \(t_x = 3.1\), \(t_y = 1.3\) and \(t_z = 18\), <em>i.e.,</em> we know before we even start that the 3D model will have to undergo a rotation of angles \((-2.1, 0.7, 2.7)\) around its three axes, and that it should shift approximately \(3cm\) in \(x\), \(1.5cm\) in \(y\) and \(18cm\) in \(z\).</p><blockquote><p>This <em>a priori</em> knowledge of the environment comes from the fact that one is able, as a human being, to evaluate the distance of the real object from the sensor only by looking at the image.</p></blockquote><h3 id=use-of-the-programme>Use of the programme</h3><p>When the script <code>localisation.py</code> is launched, two figures open: one represents a real scene captured by the camera, the other represents the virtual model to be transformed and whose system origin is materialised by a red dot. As the objective is to copy the box on which the Pikachu rests on top of the sky-blue cube on the table, you must first select five edges belonging to the real box (by <em>clicking</em> the mouse on the edges ends), then select the corresponding edges in the same order on the 3D model (by <em>clicking</em> the mouse in the middle of the edges). The number of segments to be selected (by default 5), is fixed from the start in the variable <code>nb_segments</code>.</p><p><img src=images/edge_selection.png alt="Edge selection in the image and in the 3D model" class=center><div style=margin-top:1rem></div></p><p>In this way, it will be possible to calculate the error on the estimation of extrinsic parameters by comparing the distance between the edges selected in the image and those of the transformed model. This error criterion is described at <a href=#anchor-step-2>step 2</a>.</p><h2 id=anchor-step-1><em><strong>Step 1</strong></em>: Display the pattern in \(\mathcal{R_i}\)</h2><p>In the main program, the points of the model are stored in <code>model3d_Ro</code>, a matrix of size \([246\times6]\) corresponding to the 246 edges of the model, each defined by the 6 coordinates of its two points \(P_1(x_1, y_1, z_1)\) and \(P_2(x_2, y_2, z_2)\). The transformation matrix \(M_{c\leftarrow o}\) is stored in <code>extrinsic_matrix</code> and the intrinsic parameters of the camera are stored in <code>intrinsic_matrix</code>.</p><p>To visualise the 3D model in the 2D image and get an idea of the accuracy of our estimate, we have to code the <code>tranform_and_draw_model</code> function which allows to apply the \(K_{i \leftarrow c} M_{c\leftarrow o}\) transformation at each point \(P_o\) of a set of edges <code>edges_Ro</code>, with an <code>intrinsic</code> matrix, and an <code>extrinsic</code> matrix to finally display the result in a <code>fig_axis</code> figure:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform_and_draw_model</span>(edges_Ro, intrinsic, extrinsic, fig_axis):
</span></span><span style=display:flex><span>    <span style=color:#75715e># ********************************************************************* #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># TO BE COMPLETED.                                                      #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># FUNCTIONS YOU WILL HAVE TO USE :                                      #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   - perspective_projection                                            #  </span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   - transform_point_with_matrix                                       #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Input:                                                                #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   edges_Ro : ndarray[Nx6]                                             #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#             N = number of edges in the model                          #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#             6 = (X1, Y1, Z1, X2, Y2, Z2) the P1 and P2 point          #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                 coordinates for each edge                             #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   intrinsic : ndarray[3x3] - camera intrinsic parameters              #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   extrinsic : ndarray[4x4] - camera extrinsic parameters              #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   fig_axis : figure used for display                                  #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Output:                                                               #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   No return - the function only calculates and displays the           #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   transformed points (u1, v1) and (u2, v2)                            #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ********************************************************************* #</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Part to replace #</span>
</span></span><span style=display:flex><span>    u_1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((edges_Ro<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>],<span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    u_2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((edges_Ro<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>],<span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    v_1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((edges_Ro<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>],<span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    v_2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((edges_Ro<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>],<span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#75715e>############### </span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> range(edges_Ro<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>        fig_axis<span style=color:#f92672>.</span>plot([u_1[p], u_2[p]], [v_1[p], v_2[p]], <span style=color:#e6db74>&#39;k&#39;</span>)
</span></span></code></pre></div><p>The objective is to store in the points <code>[u_1, v_1]</code> and <code>[u_2, v_2]</code> the coordinates \((u, v)\) of the points \(P_1\) and \(P_2\) of the edges after transformation.</p><p>This function can be divided into two sub-steps:</p><ul><li><p>the transformation \(\mathcal{R_o} \rightarrow \mathcal{R_c}\) of the points \(P_o\) using the function <code>transform_point_with_matrix</code> provided in the <code>matTools</code> library:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>P_c <span style=color:#f92672>=</span> matTools<span style=color:#f92672>.</span>transform_point_with_matrix(extrinsic, P_o)
</span></span></code></pre></div></li><li><p>the projection \(\mathcal{R_c} \rightarrow \mathcal{R_i}\) of the newly obtained \(P_c\) points using the function <code>perspective_projection</code> which must be completed :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>perspective_projection</span>(intrinsic, P_c):
</span></span><span style=display:flex><span>    <span style=color:#75715e># ***************************************************** #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># TO BE COMPLETED.                                      #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Useful function:                                      #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   np.dot                                              #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Input:                                                #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   intrinsic : ndarray[3x3] - intrinsic parameters     #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   P_c : ndarray[Nx3],                                 #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#         N = number of points to transform             #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#         3 = (X, Y, Z) the points coordinates          #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Output:                                               #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   u, v : two ndarray[N] containing the Ri coordinates #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#          of the transformed Pc points                 #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ***************************************************** #</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    u, v <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span> <span style=color:#75715e># Part to replace</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> u, v
</span></span></code></pre></div></li></ul><p>Once <code>perspective_projection</code> and <code>transform_and_draw_model</code> have been completed, the launch of the overall programme will project the model in the image, with the extrinsic parameters defined at the outset.</p><p><img src=images/first_rough_estimate.png alt="First projection of the model in the image" class=center><div style=margin-top:1rem></div></p><blockquote><p>The transformation applied to these points is obviously not very good, the model does not match the box as one would wish. In order to be able to differentiate a &ldquo;bad&rdquo; estimate of the extrinsic parameters from a &ldquo;good&rdquo; one, and thus be able to automatically propose a new estimate closer to our objective, we must determine an error criterion characterising the distance we are from this optimal objective.</p></blockquote><h2 id=anchor-step-2><em><strong>Step 2</strong></em>: Determine an error criterion</h2><p>The error criterion is used to assess the accuracy of our estimate. The greater the error, the poorer our extrinsic parameters. Once we have determined this error criterion, we can integrate it into an optimisation loop aimed at minimising it, and thus have the best possible extrinsic parameters for our 2D/3D matching objective.</p><p>As an example, the figure below illustrates this optimisation loop for the projection of the virtual box on the real cube. At iteration \(0\), the parameters are coarse, the error is large. By changing the parameters the error will be reduced until ideally zero error and optimal transformation for a perfect projection of the 3D model into the 2D image is achieved.</p><p><img src=images/extrinsic_optim.gif alt="Optimisation of extrinsic parameters" class=center><div style=margin-top:1rem></div></p><p>In 2D/3D segment matching, the error criterion to be minimised is the scalar product of the normal to the interpretation plane for the matching. In other words, the objective is to transform the points of the model so that the segments selected in the image and those selected in the model belong to the same plane expressed in the camera frame \(\mathcal{R_c}\).</p><p><img src=images/interpretation_plan.png alt="Interpretation plan relating to line matching" class=center><div style=margin-top:1rem></div></p><p>As shown in the figure above, the <em>interpretation plane</em> can be defined as being formed by the segments \(\mathcal{l_{i \rightarrow c}^{j,1}}\) and \(\mathcal{l_{i \rightarrow c}^{j,2}}\). In this notation, \(j\) corresponds to the number of edges selected when the program is launched (5 by default). For each selected edge \(j\) and expressed in the <em>image</em> frame \(\mathcal{R_i}\), there are two segments \(l^1\) and \(l^2\). The \(l^1\) segment is composed of the two ends \(P_{i \rightarrow c}^0\) and \(P_{i \rightarrow c}^1\); the \(l^2\) segment is composed of the two ends \(P_{i \rightarrow c}^1\) and \(P_{i \rightarrow c}^2\). Each of the \(P_{i \rightarrow c}\) is a point in the <em>image</em> frame \(\mathcal{R_i}\) transformed into the <em>camera</em> frame \(\mathcal{R_c}\). They are known: they are the ones that have been selected by mouse click.</p><p>Once these segments have been calculated, we can calculate the normal to the interpretation plane \(N_c^j\). As a reminder:</p><p>$$N = \frac{l^1 \wedge l^2}{||l^1 \wedge l^2||}$$</p><p>In the segment selection function <code>utils.select_segments()</code>, as you make edge selections, the normals are calculated and stored in the <code>normal_vectors</code> matrix thanks to the <code>calculate_normal_vector</code> function which must be completed in the <code>localisation.py</code> file:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>calculate_normal_vector</span>(p1_Ri, p2_Ri, intrinsic):
</span></span><span style=display:flex><span>    <span style=color:#75715e># ********************************************************* #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># TO BE COMPLETED.                                          #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Useful functions:                                         #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   np.dot, np.cross, np.linalg.norm, np.linalg.inv         #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Input:                                                    #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   p1_Ri : list[3]                                         #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#           3 = (u, v, 1) of the first selected point       #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   p2_Ri : list[3]                                         #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#           3 = (u, v, 1) of the second selected point      #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   intrinsic : ndarray[3x3] of intrinsic                   #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Output:                                                   #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   normal_vector : ndarray[3] containing the normal to     #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                   L1_c and L2_c segments, deducted from   #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                   the selected image points               #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ********************************************************* #</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    normal_vector <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((len(p1_Ri),)) <span style=color:#75715e># Part to replace</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> normal_vector
</span></span></code></pre></div><p>Then, for each segment \(j \in [1, &mldr;, 5]\), the distance between the plane linked to the image segments and the points of the 3D model can be calculated using the scalar product of the \(N_c^j\) and the \(P_{o\rightarrow c}^j\). If the scalar product is null, both vectors are orthogonal and the point \(P_{o\rightarrow c}^j\) belongs to the same plane as \(P_{i \rightarrow c}^j\). The larger the scalar product, the further the extrinsic parameters move away from the correct transformation.</p><p>To summarize, the parameter optimization criterion is \(\sum_{j=1}^{2n} F^j(X)^2\) (the square removes negative values), with \(F^j(X) = N^{j ; \text{mod} ; 2}.P_
{o\rightarrow c}^{j ; \text{mod} ; 2, 1|2}\), depending on whether one is on the point \(P^{1}\) or \(P^{2}\).</p><blockquote><p>In the sum that runs through the points from \(j\) to \(2n\), \(j ; \text{mod} ; 2\) is the segment index for the current point. In other words, we cumulate the \((N^i.P^{i, 1})^2\) and \((N^i.P^{i, 2})^2\) for all \(i\) segments.</p></blockquote><div class="alert alert-warning"><strong>Here, \(X\) designates the set of parameters \((\alpha, \beta, \gamma, t_x, t_y, t_z)\) and not a coordinate.</strong></div><p>Remember that every \(P_{o\rightarrow c} = \big[ R_{\alpha\beta\gamma} | T \big]. P_o\) ; the value of the error thus actually depends on the value of the extrinsic parameters. Each time the optimization loop is run, changing the weights of these parameters will influence the \(F(X)\) criterion.</p><p>The function <code>calculate_error</code> calculates the error criterion. It takes as input the number of selected edges <code>nb_segments</code>, the <code>normal_vectors</code>, and the <code>Rc_segments</code> selected and then transformed by the matrix of extrinsic parameters and expressed in \(\mathcal{R_c}\).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>calculate_error</span>(nb_segments, normal_vectors, segments_Rc):
</span></span><span style=display:flex><span>    <span style=color:#75715e># ***************************************************************** #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># TO BE COMPLETED.                                                  #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Input:                                                            #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   nb_segments : default 5 = number of selected segments           #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   normal_vectors : ndarray[Nx3] - normal vectors to the           #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                    interpretation plane of selected segments      #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                    N = number of segments                         #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                    3 = (X,Y,Z) normal coordinates in Rc           #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   segments_Rc : ndarray[Nx6] = selected segments in Ro            #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                 and transformed in Rc                             #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                 N = number of segments                            #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                 6 = (X1, Y1, Z1, X2, Y2, Z2) of points P1 and     #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                 P2 of the transformed edges in Rc                 #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Output:                                                           #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   err : float64 - cumulated error of observed vs. expected dist.  #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ***************************************************************** #</span>
</span></span><span style=display:flex><span>    err <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> range(nb_segments):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Part to replace with the error calculation</span>
</span></span><span style=display:flex><span>        err <span style=color:#f92672>=</span> err <span style=color:#f92672>+</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    err <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sqrt(err <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> nb_segments)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> err
</span></span></code></pre></div><blockquote><p>From a mathematical point of view, the sum is written for \(j\) ranging from \(1\) to \(2n\), <em>i.e.,</em> the number of points. From an implementation point of view and given the construction of our variables, it is simpler to express the criterion through a sum running along the edges: \(\sum_{j=1}^{n} F^j(X)\) with \(F^{j}(X) = F^{j, 1}(X)^2 + F^{j, 2}(X)^2\).</p><p>Thus, \(F^{j, 1}(X) = N^j.P_{o\rightarrow c}^{j, 1}\) and \(F^{j, 2}(X) = N^j.P_{o\rightarrow c}^{j, 2}\).</p></blockquote><h2 id=anchor-step-3><em><strong>Step 3</strong></em>: Estimation of parameters by ordinary least squares method</h2><p>At each iteration \(k\), we look for a set of parameters \(X_{k}(\alpha, \beta, \gamma, t_x, t_y, t_z)\) such as the criterion \(F(X)\) is equal to the criterion for the previous parameter set \(X_0\) (before update) incremented by a delta weighted by the Jacobian of the function.</p><p>We thus seek to solve a system of the form :</p><p>$$F(X) \approx F(X_0) + J \Delta X$$</p><blockquote><p>The Jacobian \(J\) contains on its lines the partial derivatives of the \(F\) function for each selected point and according to each of the \(X\) parameters. It reflects the trend of the criterion (ascending/descending) and the speed at which it increases or decreases according to the value of each of its parameters.</p></blockquote><p>Since our objective is to reach a criterion \(F(X) = 0\), we translate the problem to be solved by:</p><p>$$
\begin{align}
0 &= F(X_0) + J \Delta X \\\
\Leftrightarrow \qquad -F(X_0) &= J\Delta X
\end{align}
$$</p><p>The advantage of working with successive increments is that the increment values are so small in relation to the last iteration that the variation of these parameters can be approximated as 0. As a reminder, the error criterion is expressed as follows for each segment \(j\):</p><p>$$
\begin{align}
F^{j, 1}(X) &= N^j.P_{o\rightarrow c}^{j, 1} \text{ with } P_{o\rightarrow c}^{j, 1} = \big[ R_{\alpha\beta\gamma
} | T \big] . P_o^{j, 1}\\\
F^{j, 2}(X) &= N^{j}.P_{o\rightarrow c}^{j, 2} \text{ with } P_{o\rightarrow c}^{j, 2} = \big[ R_{\alpha\beta\gamma
} | T \big] . P_o^{j, 2}
\end{align}
$$</p><p>Because of always having \(\Delta X \approx 0\), the calculation of the Jacobian matrix is considerably simplified, since the partial derivatives \((\frac{\partial F^j}{\partial \alpha}, \frac{\partial F^j}{\partial \beta}, \frac{\partial F^j}{\partial \gamma}, \frac{\partial F^j}{\partial t_x}, \frac{\partial F^j}{\partial t_y}, \frac{\partial F^j}{\partial t_z})\) are the same at each iteration.</p><p>The detail of the derivation is given for the first parameter \(\alpha\), the following are to be demonstrated :</p><p>$$\begin{align}
\frac{\partial F^j}{\partial \alpha} &= N^j . [R_\gamma . R_\beta . \frac{\partial R_\alpha}{\partial \alpha} | T] . P_o^j\\\
\end{align}
$$</p><div class="alert alert-info"><strong><p>\(R_\gamma\) and \(R_\beta\) disappear from the derivation because for \(\gamma \approx 0\) and \(\beta \approx 0\), we have:</p><p>$$
R_{\gamma \approx 0} = \begin{bmatrix}
\cos 0 & -\sin 0 & 0\\\
\sin 0 & \cos 0 & 0\\\
0 & 0 & 1 \end{bmatrix} = \begin{bmatrix}
1 & 0 & 0\\\
0 & 1 & 0\\\
0 & 0 & 1 \end{bmatrix} = I_3
$$</p><p>$$
R_{\beta \approx 0} = \begin{bmatrix}
\cos 0 & 0 & -\sin 0\\\
0 & 1 & 0\\\
\sin 0 & 0 & \cos 0 \end{bmatrix} = \begin{bmatrix}
1 & 0 & 0\\\
0 & 1 & 0\\\
0 & 0 & 1 \end{bmatrix} = I_3
$$</p><p>\(T\) disappears since \(t_x\), \(t_y\), \(t_z \approx 0\).</p><p><em>Reminder of derivation rules: \((\text{constant } a)&rsquo; \rightarrow 0 \text{, } (\sin)&rsquo; \rightarrow \cos \text{, } (\cos)&rsquo; \rightarrow -\sin\).</em></p></strong></div><p>$$
\begin{align}
\frac{\partial F^j}{\partial \alpha} &= N^j . [I_3 . I_3 . \frac{\partial R_\alpha}{\partial \alpha} | 0] .
P_o^j\\\
&= N^j . \frac{\partial \begin{bmatrix}1 & 0 & 0\\\ 0 & \cos (\alpha \approx 0) & -\sin (\alpha \approx 0)\\\ 0 & \sin (\alpha \approx 0) & \cos (\alpha \approx 0) \end{bmatrix}}{\partial \alpha} . \begin{bmatrix}X^j\\\ Y^j\\\ Z^j\end{bmatrix}_o\\\
&= N^j . \begin{bmatrix}0 & 0 & 0\\\ 0 & -\sin (\alpha \approx 0) & -\cos (\alpha \approx 0)\\\ 0 & \cos (\alpha \approx 0) & -\sin (\alpha \approx 0) \end{bmatrix} . \begin{bmatrix}X^j\\\ Y^j\\\ Z^j\end{bmatrix}_o\\\
&= N^j . \begin{bmatrix}0 & 0 & 0\\\ 0 & 0 & -1\\\ 0 & 1 & 0 \end{bmatrix} . \begin{bmatrix}X^j\\\ Y^j\\\ Z^j\end{bmatrix}_o\\\
&= N^j . \begin{bmatrix}0\\\ -Z^j\\\ Y^j\end{bmatrix}_o\\\
\end{align}
$$</p><p>The reasoning is the same for \(\frac{\partial F^j}{\partial \beta}, \frac{\partial F^j}{\partial \gamma}, \frac{\partial F^j}{\partial t_x}, \frac{\partial F^j}{\partial t_y}, \frac{\partial F^j}{\partial t_z}\) and we get:</p><p>$$
\frac{\partial F^j}{\partial \beta} = N^j . \begin{bmatrix}Z^j\\\ 0\\\ -X^j\end{bmatrix}_o \text{, }
\frac{\partial F^j}{\partial \gamma} = N^j . \begin{bmatrix}-Y^j\\\ X^j\\\ 0\end{bmatrix}_o
$$
$$
\frac{\partial F^j}{\partial t_x} = N_x^j \text{, }
\frac{\partial F^j}{\partial t_y} = N_y^j \text{, }
\frac{\partial F^j}{\partial t_z} = N_z^j
$$</p><p>Each of these partial derivatives is to be implemented in the <code>partial_derivatives</code> function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>partial_derivatives</span>(normal_vector, P_c):
</span></span><span style=display:flex><span>    <span style=color:#75715e># ********************************************************************* #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># TO BE COMPLETED.                                                      #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Input:                                                                #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   normal_vector : ndarray[3] contains the normal of the segment       #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                   to which P_c belongs                                #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   P_c : ndarray[3] the object point transformed to Rc                 #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Output:                                                               #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   partial_derivative : ndarray[6] partial derivative of the criterion #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                        for each extrinsic parameter                   #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   crit_X0 : float64 - criterion value for the current parameters,     #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                       which will be used as initial value before      #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#                       the update and recalculation of the error       #</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ********************************************************************* #</span>
</span></span><span style=display:flex><span>    X, Y, Z <span style=color:#f92672>=</span> P_c[<span style=color:#ae81ff>0</span>], P_c[<span style=color:#ae81ff>1</span>], P_c[<span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span>    partial_derivative <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Part to replace #######</span>
</span></span><span style=display:flex><span>    partial_derivative[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    partial_derivative[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    partial_derivative[<span style=color:#ae81ff>2</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    partial_derivative[<span style=color:#ae81ff>3</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    partial_derivative[<span style=color:#ae81ff>4</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    partial_derivative[<span style=color:#ae81ff>5</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#########################</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ********************************************************************</span>
</span></span><span style=display:flex><span>    crit_X0 <span style=color:#f92672>=</span> normal_vector[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>*</span> X <span style=color:#f92672>+</span> normal_vector[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>*</span> Y <span style=color:#f92672>+</span> normal_vector[<span style=color:#ae81ff>2</span>] <span style=color:#f92672>*</span> Z
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> partial_derivative, crit_X0
</span></span></code></pre></div><p>The problem can thus be formalised as:</p><p>$$
F = J \Delta X \text{ with } F = \begin{bmatrix}-F^1(X_0) \\\ \vdots \\\ -F^{2n}(X_0)\end{bmatrix}_{2n\times 1}
$$
$$
\text{ and } J = \begin{bmatrix}\frac{\partial F^1}{\partial \alpha} & \frac{\partial F^1}{\partial \beta} & \frac{\partial F^1}{\partial \gamma} & \frac{\partial F^1}{\partial t_x} & \frac{\partial F^1}{\partial t_y} & \frac{\partial F^1}{\partial t_z}\\\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\\ \frac{\partial F^{2n}}{\partial \alpha} & \frac{\partial F^{2n}}{\partial \beta} & \frac{\partial F^{2n}}{\partial \gamma} & \frac{\partial F^{2n}}{\partial t_x} & \frac{\partial F^{2n}}{\partial t_y} & \frac{\partial F^{2n}}{\partial t_z}\end{bmatrix}_{2n\times 6}
$$
$$
\text{ and } \Delta X = \begin{bmatrix}\Delta \alpha \\\ \vdots \\\ \Delta t_z\end{bmatrix}_{6 \times 1}
$$</p><blockquote><p>\(2n\) is the number of selected points, \(n\) is the number of edges (one edge = two ends).</p></blockquote><p>\(F\) is known, \(J\) is known, all that remains is to estimate \(\Delta X\) so that the equality \(F = J \Delta X\) is &ldquo;as true as possible&rdquo;. This is done by minimising the distance between the two sides of equality:</p><p>$$
\begin{align}
\min_{\Delta X} ||F - J\Delta X||^2 & \\\
\Leftrightarrow \qquad \frac{\partial ||F - J\Delta X||^2}{\partial \Delta X} &= 0
\end{align}
$$</p><blockquote><p>Indeed, if the derivative of the function to be minimized is 0, then the curve of the function has definitely reached a minimum.</p></blockquote><div class="alert alert-info"><strong><p><em>Reminder of matrices operations:</em></p><p>\((A+B)^T = A^T + B^T\)</p><p>\((AB)^T = B^T A^T\)</p><p>\(A\times B \neq B\times A\)</p><p>\(A^2 = A^T A\)</p></strong></div><p>By developing \((F - J \Delta X)^2\), we arrive at :</p><p>$$
\begin{align}
(F - J \Delta X)^2 & = (F - J \Delta X)^T(F - J \Delta X)\\\
&= (F^T - \Delta X^T J^T)(F - J \Delta X)\\\
&= F^TF - F^TJ\Delta X - \Delta X^T J^T F + \Delta X^T J^T J \Delta X
\end{align}
$$</p><p>We use matrices properties to show \(F^TJ\Delta X = ((J\Delta X)^T F)^T = (\Delta X^T J^T F)^T\). Let&rsquo;s consider matrices shapes for both sides of this equality:
$$
\begin{align}
F^TJ\Delta X &\rightarrow [1\times2n][2n\times 6][6\times 1] \rightarrow [1\times 1]\\\
(\Delta X^T J^T F)^T &\rightarrow [1\times6][6\times 2n][2n\times 1] \rightarrow [1\times 1]
\end{align}
$$</p><p>Given that each of these terms is a \([1\times 1]\) matrix in the end, we can write \((\Delta X^T J^T F)^T = \Delta X^T J^T F\), and thus \(F^TJ\Delta X = \Delta X^T J^T F\).</p><p>We deduce from this:
$$
\begin{align}
(F - J \Delta X)^2 & = F^TF - 2\Delta X^T J^T F + \Delta X^T J^T J \Delta X\\\
\end{align}
$$</p><div class="alert alert-info"><strong><p><em>Some rules of derivation:</em></p><p>\(\frac{\partial AX}{\partial X} = A^T\), \(\frac{\partial X^TA^T}{\partial X} = A^T\), \(\frac{\partial X^TAX}{\partial X} = 2AX\).</p></strong></div><p>$$
\begin{align}
\frac{\partial (F - J \Delta X)^2}{\partial \Delta X} & = -2J^T F + 2 J^T J \Delta X\\\
&= -J^T F + J^T J \Delta X\\\
\Leftrightarrow \qquad J^T F &= J^T J \Delta X \\\
(J^TJ)^{-1} J^T F &= \Delta X \\\
\end{align}
$$</p><p><strong>In a nutshell:</strong></p><ul><li>minimizing the distance between \(F\) and \(J\Delta X\) means that \(\frac{\partial (F - J \Delta X)^2}{\partial \Delta X} = 0\)</li><li>the solution is \(\Delta X = (J^TJ)^{-1} J^T F\).</li></ul><blockquote><p>The matrix \(J^+ = (J^TJ)^{-1} J^T\) is called the <em>pseudo-inverse</em> of \(J\).</p></blockquote><p>In the Python code, we can thus implement the parameter update in the optimization loop. \(\Delta X\) corresponds to the variable named <code>delta_solution</code>. We can then pass <code>delta_solution</code> to the function <code>matTools.construct_matrix_from_vec</code> which returns a matrix of \(X\) increments in the same form as <code>extrinsic</code> (the extrinsic parameter matrix).</p><p>Each element of <code>extrinsic</code> is incremented by multiplying it by <code>delta_extrinsic</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># ********************************************************************* #</span>
</span></span><span style=display:flex><span><span style=color:#75715e># TO BE COMPLETED.                                                      #</span>
</span></span><span style=display:flex><span><span style=color:#75715e># delta_solution = ...                                                  #</span>
</span></span><span style=display:flex><span><span style=color:#75715e># delta_extrinsic = matTools.construct_matrix_from_vec(delta_solution)  #</span>
</span></span><span style=display:flex><span><span style=color:#75715e># extrinsic = ...                                                       #</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ********************************************************************* #</span>
</span></span></code></pre></div><p>Once the optimisation loop is operational, the model transformation can be visualised without the virtual box by removing the first 12 points of <code>model3D_Ro</code> :</p><p><img src=images/final_objective_without_box.png alt="Transformation of the model after estimation of the camera pose" class=center><div style=margin-top:1rem></div></p><h2 id=anchor-step-4><em><strong>Step 4</strong></em>: projection of the estimated pose on the image taken from a different point of view</h2><p>A second photo has been captured from a different point of view, and the passage matrix between the first and second views is stored and loaded at the beginning of the programme from the corresponding <code>.txt</code> file. This allows the model to be re-projected into the image from the second camera, and the result to be displayed:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>fig5 <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>figure(<span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>ax5 <span style=color:#f92672>=</span> fig5<span style=color:#f92672>.</span>add_subplot(<span style=color:#ae81ff>111</span>)
</span></span><span style=display:flex><span>ax5<span style=color:#f92672>.</span>set_xlim(<span style=color:#ae81ff>0.720</span>)
</span></span><span style=display:flex><span>ax5<span style=color:#f92672>.</span>set_ylim(<span style=color:#ae81ff>480</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>imshow(image_2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># To be completed with the passage matrix from Ro to Rc2</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Ro -&gt; Rc and Rc -&gt; Rc2</span>
</span></span><span style=display:flex><span>Ro_to_Rc2 <span style=color:#f92672>=</span> <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>transform_and_draw_model(model3D_Ro[<span style=color:#ae81ff>12</span>:], intrinsic_matrix, Ro_to_Rc2, ax5)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show(block <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><p><img src=images/left_to_right.png alt="Projection in the second image" class=center><div style=margin-top:1rem></div></p><h2 id=bonus>Bonus for the end</h2><p>Thanks to the estimated extrinsic parameter matrix, and as long as the origin of the object reference does not change, elements can be added to the 3D scene and projected into the image in an identical way. <code>model3D_Ro_final</code> contains the points of a scene with Pikachu and a dinosaur in it.</p><p><img src=images/pika_dino_blender.png alt="Adding elements to the 3D scene" class=center><div style=margin-top:1rem></div></p><p>The last lines of the display part are uncommented to get the final result:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>fig6 <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>figure(<span style=color:#ae81ff>6</span>)
</span></span><span style=display:flex><span>ax6, lines <span style=color:#f92672>=</span> utils<span style=color:#f92672>.</span>plot_3d_model(model3D_Ro_final, fig6)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig7 <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>figure(<span style=color:#ae81ff>7</span>)
</span></span><span style=display:flex><span>ax7 <span style=color:#f92672>=</span> fig7<span style=color:#f92672>.</span>add_subplot(<span style=color:#ae81ff>111</span>)
</span></span><span style=display:flex><span>ax7<span style=color:#f92672>.</span>set_xlim(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>720</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>imshow(image)
</span></span><span style=display:flex><span>transform_and_draw_model(model3D_Ro_final[<span style=color:#ae81ff>12</span>:], intrinsic_matrix, extrinsic_matrix, ax7)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show(block<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p><img src=images/pika_dino.png alt="New projected scene" class=center><div style=margin-top:1rem></div></p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/clairelabitbonis/clairelabitbonis.github.io/edit/main/content/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/teaching/3d_perception/practical_sessions_3d_perception/cc_segmentation/ title="3DP-TP-00 | 3D segmentation and object shape checking based on RGB-D sensor with CloudCompare" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>3DP-TP-00 | 3D segmentation and object shape checking based on RGB-D sensor with CloudCompare</div></a></div><div class="col-md-6 next-article"><a href=/posts/teaching/deep_learning_for_cv/course_dlcv/01_why_deep_learning/ title="DLCV-CM-00 | From A.I. to deep learning" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>DLCV-CM-00 | From A.I. to deep learning</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a><div class="dropdown languageSelector"><a class="btn dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-gb"></span>
English</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/fr/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization><span class="flag-icon flag-icon-fr"></span>
Français</a></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#contents-download>Contents download</a></li><li><a href=#anchor-step-0>Goal description</a><ul><li><a href=#extrinsic-parameters-estimation>Extrinsic parameters estimation</a></li><li><a href=#use-of-the-programme>Use of the programme</a></li></ul></li><li><a href=#anchor-step-1><em><strong>Step 1</strong></em>: Display the pattern in \(\mathcal{R_i}\)</a></li><li><a href=#anchor-step-2><em><strong>Step 2</strong></em>: Determine an error criterion</a></li><li><a href=#anchor-step-3><em><strong>Step 3</strong></em>: Estimation of parameters by ordinary least squares method</a></li><li><a href=#anchor-step-4><em><strong>Step 4</strong></em>: projection of the estimated pose on the image taken from a different point of view</a></li><li><a href=#bonus>Bonus for the end</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:clairelabitbonis@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>clairelabitbonis@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script type=text/javascript src=/js/darkreader.js></script>
<script type=text/javascript src=/js/darkmode-darkreader.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script>
<script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>