<!doctype html><html lang=fr><head><meta name=generator content="Hugo 0.77.0"><title>Claire Labit-Bonis</title><meta name=description content="Portfolio et site personnel de Claire Labit-Bonis."><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_2.png><meta property="og:title" content="Claire Labit-Bonis"><meta property="og:type" content="website"><meta property="og:description" content="Portfolio et site personnel de Claire Labit-Bonis."><meta property="og:image" content="http://clairelabitbonis.gitlab.io/images/author/claire.png"><meta property="og:url" content="http://clairelabitbonis.gitlab.io/"><link rel=stylesheet href=/css/sections/home.css><link rel=stylesheet href=/css/sections/about.css><link rel=stylesheet href=/css/sections/skills.css><link rel=stylesheet href=/css/sections/experiences.css><link rel=stylesheet href=/css/sections/education.css><link rel=stylesheet href=/css/sections/projects.css><link rel=stylesheet href=/css/sections/recent-posts.css><link rel=stylesheet href=/css/sections/achievements.css><link rel=stylesheet href=/css/sections/accomplishments.css><link rel=stylesheet href=/css/sections/publications.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#top-navbar data-offset=100><nav class="navbar navbar-expand-xl top-navbar initial-navbar" id=top-navbar><div class=container><a class=navbar-brand href=/fr><img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_2.png id=logo alt=Logo>
Claire Labit-Bonis</a>
<button class="navbar-toggler navbar-dark" id=navbar-toggler type=button data-toggle=collapse data-target=#top-nav-items aria-label=menu>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=#home>Accueil</a></li><li class=nav-item><a class=nav-link href=#about>A propos</a></li><li class=nav-item><a class=nav-link href=#publications>Publications</a></li><li class=nav-item><a class=nav-link id=blog-link href=/fr/posts>Articles</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-fr"></span>Français</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/><span class="flag-icon flag-icon-gb"></span>English</a>
<a class="dropdown-item nav-link languages-item" href=/fr/><span class="flag-icon flag-icon-fr"></span>Français</a></div></li><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><div class="container-fluid home" id=home><style>#homePageBackgroundImageDivStyled{background-image:url(/images/site/background_huacdfba78b1d98609715cf728e6076ae9_462363_500x0_resize_q75_box.jpg)}@media(min-width:500px) and (max-width:800px){#homePageBackgroundImageDivStyled{background-image:url(/images/site/background_huacdfba78b1d98609715cf728e6076ae9_462363_800x0_resize_q75_box.jpg)}}@media(min-width:801px) and (max-width:1200px){#homePageBackgroundImageDivStyled{background-image:url(/images/site/background_huacdfba78b1d98609715cf728e6076ae9_462363_1200x0_resize_q75_box.jpg)}}@media(min-width:1201px) and (max-width:1500px){#homePageBackgroundImageDivStyled{background-image:url(/images/site/background_huacdfba78b1d98609715cf728e6076ae9_462363_1500x0_resize_q75_box.jpg)}}@media(min-width:1501px){#homePageBackgroundImageDivStyled{background-image:url(/images/site/background.jpg)}}</style><span class=on-the-fly-behavior></span><div id=homePageBackgroundImageDivStyled class="background container-fluid"></div><div class="container content text-center"><img src=/images/author/claire_hu4fa84bbe5b0b14aeb1f21f53cdd20f7e_312167_148x148_fit_box_2.png class="rounded-circle mx-auto d-block img-fluid" alt="Author Image"><h1 class=greeting>Bonjour, moi c'est Claire</h1><div class=typing-carousel><span id=ityped class=ityped></span><span class=ityped-cursor></span></div><ul id=typing-carousel-data><li>Je suis ingénieure en I.A. et vision par ordinateur</li><li>Je suis enseignante</li><li>J'adore découvrir de nouvelles choses</li><li>Je travaille sur l'apprentissage automatique</li><li>J'aime travailler sur des projets amusants</li></ul><a href=#about aria-label="Lire Suite - Claire"><i class="arrow bounce fa fa-chevron-down"></i></a></div></div><div class="container-fluid section-holder d-flex bg-white"><div class="container anchor p-lg-5 about-section" id=about><div class="row pt-sm-2 pt-md-4 align-self-center"><div class=col-sm-6><h3 class=p-1>Claire Labit-Bonis</h3><h5 class=p-1>Ingénieure I.A.
chez <a href=https://www.actia.com title=ACTIA target=_blank rel=noopener>ACTIA</a></h5><p class="p-1 text-justify">Je suis architecte logiciel spécialisée en intelligence artificielle appliquée à la vision par ordinateur. J&rsquo;ai obtenu mon doctorat en 2021 au sein des équipes du <a href=https://www.laas.fr>LAAS-CNRS</a> et d'<a href=https://www.actia.com/fr/>ACTIA</a>, en travaillant sur la détection et le suivi multi-objets pour des cas d&rsquo;utilisation dans les transports publics.</p><div class="text-container ml-auto"><ul class="social-link d-flex"><li><a href=https://www.linkedin.com/in/clairelabitbonis title=LinkedIn target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=mailto:clairelabitbonis@gmail.com title=E-mail target=_blank rel=noopener><i class="fa-solid fa-envelope"></i></a></li></ul></div></div><div class="col-sm-6 pt-5 pl-md-4 pl-sm-3 pt-sm-0"><div class=row><div class="col-6 col-lg-4 p-2"><div class="circular-progress green"><span class=circular-progress-left><span class="circular-progress-bar circular-progress-percentage-100"></span></span><span class=circular-progress-right><span class=circular-progress-bar></span></span><div class=circular-progress-value>Implication</div></div></div><div class="col-6 col-lg-4 p-2"><div class="circular-progress pink"><span class=circular-progress-left><span class="circular-progress-bar circular-progress-percentage-100"></span></span><span class=circular-progress-right><span class=circular-progress-bar></span></span><div class=circular-progress-value>Bonne volonté</div></div></div><div class="col-6 col-lg-4 p-2"><div class="circular-progress yellow"><span class=circular-progress-left><span class="circular-progress-bar circular-progress-percentage-100"></span></span><span class=circular-progress-right><span class=circular-progress-bar></span></span><div class=circular-progress-value>Curiosité</div></div></div></div></div></div></div></div><div class="container-fluid section-holder d-flex bg-dimmed"><div class="container-fluid anchor pb-5 publications-section" id=publications><h1 class=text-center><span id=publications></span>Publications</h1><div class="container ml-auto text-center"><div class="btn-group flex-wrap" role=pub-group id=publication-filter-buttons><button type=button class="btn btn-dark pub-filtr-control" data-filter=pub-all>
All</button>
<button type=button class="btn btn-dark pub-filtr-control" data-filter=pub-computervision>
Computer Vision</button>
<button type=button class="btn btn-dark pub-filtr-control" data-filter=pub-mot>
Multi-Object Tracking</button>
<button type=button class="btn btn-dark pub-filtr-control" data-filter=pub->
Visual Object Tracking</button>
<button type=button class="btn btn-dark pub-filtr-control" data-filter=pub-deeplearning>
Deep Learning</button>
<button type=button class="btn btn-dark pub-filtr-control" data-filter=pub-siamesenetworks>
Siamese Networks</button>
<button type=button class="btn btn-dark pub-filtr-control" data-filter=pub-passengercounting>
Passenger Counting</button>
<button type=button class="btn btn-dark pub-filtr-control" data-filter=pub-similaritylearning>
Similarity Learning</button>
<button type=button class="btn btn-dark pub-filtr-control" data-filter=pub-crosscorrelation>
Cross-correlation</button>
<button type=button class="btn btn-dark pub-filtr-control" data-filter=pub->
Telepresence</button>
<button type=button class="btn btn-dark pub-filtr-control" data-filter=pub-prototyping>
Prototyping</button></div></div><div class="container filtr-publications"><div class=row id=publication-card-holder><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-computervision,pub-deeplearning,pub-mot,pub-crosscorrelation><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">Intercorrélation basée apprentissage profond pour le suivi multi-cibles</h5><div class=sub-title><span><a href=https://caprfiap2022.sciencesconf.org/>Reconnaissance des Formes, Image, Apprentissage et Perception (RFIAP’22)</a></span>
<span class=ml-2>July 2022</span></div><div class=authors><span class=mr-2>Pierre Marigo</span>
<span class=mr-2>Jérôme Thomas</span>
<span class=mr-2><a href=.>Claire Labit-Bonis</a></span>
<span class=mr-2><a href=https://homepages.laas.fr/lerasle/>Frédéric Lerasle</a></span></div></div><div class=card-body><p>Tracking multiple moving targets in a video stream requires localizing and re-identifying them in varied and sometimes cluttered scenes. We present a tracking-by-detection method with multi-hypothesis visual pose estimation based on deep learning cross-correlation. In addition to a single all-in-one neural network for target detection and tracking, we also put forward a strategy for managing trajectories by characterizing their states. We demonstrate the quality of our approach and the gains made by comparing ourselves to the literature on the MOT17 benchmark through a quantitative and qualitative analysis.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">Computer Vision</span>
<span class="btn badge btn-info ml-1 p-2">Deep Learning</span>
<span class="btn badge btn-info ml-1 p-2">Multi-Object Tracking</span>
<span class="btn badge btn-info ml-1 p-2">Cross-correlation</span></div><div class=pdf-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=/pdf/sections/publications/intercorrelation-basee-apprentissage.pdf target=_blank rel=noopener role=button>PDF</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-computervision,pub-deeplearning,pub-mot,pub-passengercounting><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">Visual and automatic bus passenger counting based on a deep tracking-by-detection system</h5><div class=sub-title><span><a href=https://hal.laas.fr/>HAL LAAS - Open archives</a></span>
<span class=ml-2>July 2021</span></div><div class=authors><span class=mr-2><a href=.>Claire Labit-Bonis</a></span>
<span class=mr-2>Jérôme Thomas</span>
<span class=mr-2><a href=https://homepages.laas.fr/lerasle/>Frédéric Lerasle</a></span></div></div><div class=card-body><p>In this paper, we address the industrial constraints of automatic passenger counting in city buses through a deep architecture able to deal with images taken from low cost 2D cameras placed above the doorstep, from a zenithal point of view. The challenge is then to handle highly variable scenes due to passengers appearance (hair color, hats, height), bus population density at rush hour and changes in scene illumination. The scientific breakthrough related to deep learning applied to computer vision as well as the system embedding requirements for this task motivate us to integrate in this context a lightweight convolutional multiobject tracker which was especially designed for embedded applications and performed well on the MOT Challenge. We here evaluate it in an industrial context on our large scale in-situ dataset, labelled for detection, multi-target tracking and counting, and present a complete and embedded counting system meeting the requirements of our application.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">Computer Vision</span>
<span class="btn badge btn-info ml-1 p-2">Deep Learning</span>
<span class="btn badge btn-info ml-1 p-2">Multi-Object Tracking</span>
<span class="btn badge btn-info ml-1 p-2">Passenger Counting</span></div><div class=pdf-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=/pdf/sections/publications/visual-bus-passenger.pdf target=_blank rel=noopener role=button>PDF</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-computervision,pub-deeplearning,pub-mot,pub-siamesenetworks,pub-passengercounting><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">Compact and siamese multi-object tracking-by-detection applied to on-board passenger counting in city buses</h5><div class=sub-title><span>PhD thesis supervised by Frédéric Lerasle and Jérôme Thomas, defended at the University of Toulouse 3</span>
<span class=ml-2>Jun, 21 - 2021</span></div><div class=authors><span class=mr-2><a href=.>Claire Labit-Bonis</a></span></div></div><div class=card-body><p>In this thesis, we answer the industrial constraints of passenger counting in city buses through a deep architecture able to deal with images taken from low cost 2D sensors placed above the doorstep, from a zenithal point of view. The challenge is then to handle highly variable scenes due to passenger appearance (hair color, hats, height), bus population density at rush hour and changes in scene illumination. The scientific breakthrough related to deep learning applied to computer vision as well as the system embedding requirements motivate us to propose a unified and lightweight convolutional architecture in the context of multi-object tracking-by-detection for trajectory reconstruction. We evaluate our approach compared to the literature on a public reference dataset, the MOT Challenge, but also in an industrial context on our large \textit{in-situ} database, labeled for detection, multi-target tracking and counting. We show state-of-the-art results on the public database both in tracking accuracy and speed execution, but also present a complete and embedded counting system respecting the industrial constraints defined by the specifications.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">Computer Vision</span>
<span class="btn badge btn-info ml-1 p-2">Deep Learning</span>
<span class="btn badge btn-info ml-1 p-2">Multi-Object Tracking</span>
<span class="btn badge btn-info ml-1 p-2">Siamese Networks</span>
<span class="btn badge btn-info ml-1 p-2">Passenger Counting</span></div><div class=pdf-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=http://www.theses.fr/2021TOU30070 target=_blank rel=noopener role=button>Détails</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-computervision,pub-deeplearning,pub-mot,pub-vot,pub-siamesenetworks><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">Compact and discriminative multi-object tracking with siamese CNNs</h5><div class=sub-title><span><a href=https://www.micc.unifi.it/icpr2020>IEEE International Conference on Pattern Recognition (ICPR’2020)</a></span>
<span class=ml-2>January 2021</span></div><div class=authors><span class=mr-2><a href=.>Claire Labit-Bonis</a></span>
<span class=mr-2>Jérôme Thomas</span>
<span class=mr-2><a href=https://homepages.laas.fr/lerasle/>Frédéric Lerasle</a></span></div></div><div class=card-body><p>Following the tracking-by-detection paradigm, multiple object tracking deals with challenging scenarios, occlusions or even missing detections; the priority is often given to quality measures instead of speed, and a good trade-off between the two is hard to achieve. Based on recent work, we propose a fast, lightweight tracker able to predict targets position and reidentify them at once, when it is usually done with two sequential steps. To do so, we combine a bounding box regressor with a target-oriented appearance learner in a newly designed and unified architecture. This way, our tracker can infer the targets’ image pose but also provide us with a confidence level about target identity. Most of the time, it is also common to filter out the detector outputs with a preprocessing step, throwing away precious information about what has been seen in the image. We propose a tracks management strategy able to balance efficiently between detection and tracking outputs and their associated likelihoods. Simply put, we spotlight a full siamese based single object tracker able to predict both position and appearance features at once with a lightweight and all-in-one architecture, within a balanced overall multi-target management strategy. We demonstrate the efficiency and speed of our system w.r.t the literature on the well-known MOT17 challenge benchmark, and bring to the fore qualitative evaluations as well as state-of-the-art quantitative results.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">Computer Vision</span>
<span class="btn badge btn-info ml-1 p-2">Deep Learning</span>
<span class="btn badge btn-info ml-1 p-2">Multi-Object Tracking</span>
<span class="btn badge btn-info ml-1 p-2">Visual Object Tracking</span>
<span class="btn badge btn-info ml-1 p-2">Siamese Networks</span></div><div class=pdf-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=/pdf/sections/publications/reg-and-reid.pdf target=_blank rel=noopener role=button>PDF</a>
<a class="btn btn-outline-info ml-1 pl-2 mb-2" href=/pdf/sections/publications/reg-and-reid_slides.pdf target=_blank rel=noopener role=button>Slides</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-computervision,pub-deeplearning,pub-mot,pub-similaritlearning,pub-siamesenetworks,pub-passengercounting><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">Fast tracking-by-detection of bus passengers with Siamese CNNs</h5><div class=sub-title><span><a href=http://www.cs.albany.edu/AVSS2019/>IEEE Advanced Video Signal-based Surveillance (AVSS’19)</a></span>
<span class=ml-2>September 2019</span></div><div class=authors><span class=mr-2><a href=.>Claire Labit-Bonis</a></span>
<span class=mr-2>Jérôme Thomas</span>
<span class=mr-2><a href=https://homepages.laas.fr/lerasle/>Frédéric Lerasle</a></span></div></div><div class=card-body><p>Knowing the exact number of passengers among the citybus fleets allows public transport operators to optimally distribute their vehicles into the traffic. However, interpreting overcrowded scenarios, at rush hour, with day/night illumination changes can be tricky. Based on the visual tracking-by-detection paradigm, we benefit from video stream information provided by cameras placed above doors to infer people trajectories and deduce the number of enterings/leavings at every bus stop. In this way a person detector estimates the location of the passengers in each image, a tracker matches detections between successive frames based on different cues such as appearance or motion, and infers trajectories over time. This paper proposes a fast and embeddable framework that performs person detection using relevant state-of-the-art CNN detectors, and couple the best one (in our applicative context) with a newly designed Siamese network for real-time tracking/data association purposes. Evaluations on our own large scale in-situ dataset are very promising in terms of performances and real-time constraint expected for on-board processing.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">Computer Vision</span>
<span class="btn badge btn-info ml-1 p-2">Deep Learning</span>
<span class="btn badge btn-info ml-1 p-2">Multi-Object Tracking</span>
<span class="btn badge btn-info ml-1 p-2">Similarity Learning</span>
<span class="btn badge btn-info ml-1 p-2">Siamese Networks</span>
<span class="btn badge btn-info ml-1 p-2">Passenger Counting</span></div><div class=pdf-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=/pdf/sections/publications/fast-siamese.pdf target=_blank rel=noopener role=button>PDF</a>
<a class="btn btn-outline-info ml-1 pl-2 mb-2" href=/pdf/sections/publications/fast-siamese_slides.pdf target=_blank rel=noopener role=button>Slides</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-computervision,pub-deeplearning,pub-mot,pub-passengercounting><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">Suivi de passagers de bus par apprentissage profond</h5><div class=sub-title><span><a href=https://rfiap2018.ign.fr/>Reconnaissance des Formes, Image, Apprentissage et Perception (RFIAP’18)</a></span>
<span class=ml-2>June 2018</span></div><div class=authors><span class=mr-2><a href=.>Claire Labit-Bonis</a></span>
<span class=mr-2>Jérôme Thomas</span>
<span class=mr-2><a href=https://homepages.laas.fr/lerasle/>Frédéric Lerasle</a></span>
<span class=mr-2><a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=Xt4u35wAAAAJ">Francisco Madrigal</a></span></div></div><div class=card-body><p>In this paper we present a comparative study of tracking-by-detection approaches applied to passenger counting in city buses. A detector targets passengers at each frame, a tracker then matches detections together through time to produce trajectories. We compare three deep learning detectors still under-explored in our context, and couple them with a real time tracker for global evaluation on our large scale in situ dataset. The results we present are very encouraging in terms of detection, tracking rate and speed expected for our embedded perspectives.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">Computer Vision</span>
<span class="btn badge btn-info ml-1 p-2">Deep Learning</span>
<span class="btn badge btn-info ml-1 p-2">Multi-Object Tracking</span>
<span class="btn badge btn-info ml-1 p-2">Siamese Networks</span>
<span class="btn badge btn-info ml-1 p-2">Passenger Counting</span></div><div class=pdf-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=/pdf/sections/publications/bus-passenger-counting.pdf target=_blank rel=noopener role=button>PDF</a>
<a class="btn btn-outline-info ml-1 pl-2 mb-2" href=/pdf/sections/publications/bus-passenger-counting_slides.pdf target=_blank rel=noopener role=button>Slides</a></div></div></div></div><div class="col-12 p-2 pub-filtr-item" data-category=pub-all,pub-telepresence,pub-prototyping><div class="card mt-3"><div class=card-header><h5 class="card-title mb-0">WithU - un robot low-cost de téléprésence</h5><div class=sub-title><span><a href=https://dumas.ccsd.cnrs.fr/IHM-2015>Conférence francophone sur l’Interaction Homme-Machine (IHM’15)</a></span>
<span class=ml-2>October 2015</span></div><div class=authors><span class=mr-2>Yorian Delmas</span>
<span class=mr-2><a href=.>Claire Labit-Bonis</a></span>
<span class=mr-2>Jérémy Ouanély</span>
<span class=mr-2>Yannick Traoré</span>
<span class=mr-2>Aurélien Veillard</span>
<span class=mr-2><a href=https://gepettoweb.laas.fr/index.php/Members/MichelTa%C3%AFx>Michel Taïx</a></span>
<span class=mr-2><a href=https://www.irit.fr/~Philippe.Truillet/index.html>Philippe Truillet</a></span></div></div><div class=card-body><p>Being able to interact with a remote environment, to &ldquo;act as if you were there&rdquo; without having to move, is made possible by techniques that have emerged and evolved since the 1960s. From simple teleconferencing allowing to participate in meetings on the other side of the world, to the telemanipulation of medical robots in meticulous surgical operations, telepresence allows a user to be virtually present in a remote location. We show here the design and development of a working prototype with low-cost tools.</p></div><div class=card-footer><div class=tags><span class="btn badge btn-info ml-1 p-2">Telepresence</span>
<span class="btn badge btn-info ml-1 p-2">Prototyping</span></div><div class=pdf-btn><a class="btn btn-outline-info ml-1 pl-2 mb-2" href=/pdf/sections/publications/withu.pdf target=_blank rel=noopener role=button>PDF</a>
<a class="btn btn-outline-info ml-1 pl-2 mb-2" href=/pdf/sections/publications/withu_slides.pdf target=_blank rel=noopener role=button>Slides</a></div></div></div></div></div></div></div></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.gitlab.io/fr/#about>A propos</a></li><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.gitlab.io/fr/#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contactez moi :</h5><ul><li><a href=mailto:clairelabitbonis@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span><span>clairelabitbonis@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Alimenté par
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=/js/itype.min.js></script><script src=/js/github-button.js></script><script src=/js/home.js></script><script src=/js/jquery.filterizr.min.js></script></body></html>