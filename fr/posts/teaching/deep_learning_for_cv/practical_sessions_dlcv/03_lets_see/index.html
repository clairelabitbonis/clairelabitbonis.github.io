<!doctype html><html><head><title>DLCV-TP-11 | Voyons...</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png><meta property="og:title" content="DLCV-TP-11 | Voyons..."><meta property="og:description" content="Previously on &ldquo;DLCV Practical sessions&rdquo;&mldr; Il est beau le dataset R√©partition des classes Gr√¢ce √† nous tou¬∑te¬∑s, on a construit un beau dataset qui nous permet de d√©tecter plein de classes d&rsquo;objets tr√®s utiles. Le tableau ci-dessous indique le nombre d&rsquo;images et la r√©partition des diff√©rentes classes entre les ensembles de train, validation et test :
Analyse des labels La quantit√© de labels par image et leur forme varie en fonction des classes annot√©es."><meta property="og:type" content="article"><meta property="og:url" content="http://clairelabitbonis.github.io/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/03_lets_see/"><meta property="og:image" content="http://clairelabitbonis.github.io/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/03_lets_see/featured.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-14T10:00:00+09:00"><meta property="article:modified_time" content="2022-12-14T10:00:00+09:00"><meta name=description content="DLCV-TP-11 | Voyons..."><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/fr><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png alt=Logo>
Claire Labit-Bonis</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/fr/search><input type=text name=keyword placeholder=Chercher data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/fr/posts data-filter=all>Articles</a></li><div class=subtree><li><a href=/fr/posts/introduction/ title=Introduction>Introduction</a></li><li><i class="fas fa-minus-circle"></i><a class=active href=/fr/posts/teaching/>Enseignements</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/fr/posts/teaching/3d_perception/>Perception 3D</a><ul><li><i class="fas fa-plus-circle"></i><a href=/fr/posts/teaching/3d_perception/practical_sessions_3d_perception/>Travaux pratiques</a><ul><li><a href=/fr/posts/teaching/3d_perception/practical_sessions_3d_perception/cc_segmentation/ title=Segmentation>Segmentation</a></li><li><a href=/fr/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/ title="Localisation mono.">Localisation mono.</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/fr/posts/teaching/deep_learning_for_cv/>Deep Learning & CV</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/fr/posts/teaching/deep_learning_for_cv/course_dlcv/>Cours</a><ul><li><a href=/fr/posts/teaching/deep_learning_for_cv/course_dlcv/01_why_deep_learning/ title="00 | De l'IA au DL">00 | De l'IA au DL</a></li><li><a href=/fr/posts/teaching/deep_learning_for_cv/course_dlcv/02_lets_learn_deeply/ title="01 | DD into OD">01 | DD into OD</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/>Travaux pratiques</a><ul class=active><li><a href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/ title="00 | Pr√©sentation">00 | Pr√©sentation</a></li><li><a href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/02_yolo/ title="01 | YOLO !">01 | YOLO !</a></li><li><a class=active href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/03_lets_see/ title="11 | Voyons...">11 | Voyons...</a></li></ul></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/03_lets_see/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/claire_hu4fa84bbe5b0b14aeb1f21f53cdd20f7e_312167_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name>Claire Labit-Bonis</h5><p>mercredi 14 d√©cembre 2022</p></div><div class=title><h1>DLCV-TP-11 | Voyons...</h1></div><div class=post-content id=post-content><h2 id=previously-on-dlcv-practical-sessions><strong>Previously on &ldquo;DLCV Practical sessions&rdquo;&mldr;</strong></h2><center><img src=images/previously_on_dlcv.png alt="Previously on DLCV practical sessions" width=70%></center><h2 id=il-est-beau-le-dataset>Il est beau le <em>dataset</em></h2><h3 id=r√©partition-des-classes>R√©partition des classes</h3><p>Gr√¢ce √† nous tou¬∑te¬∑s, on a construit un beau <em>dataset</em> qui nous permet de d√©tecter plein de classes d&rsquo;objets tr√®s utiles.
Le tableau ci-dessous indique le nombre d&rsquo;images et la r√©partition des diff√©rentes classes entre les ensembles de <em>train</em>, <em>validation</em> et <em>test</em> :</p><center><img src=images/dataset.png alt=Dataset width=80%></center><h3 id=analyse-des-labels>Analyse des labels</h3><p>La quantit√© de labels par image et leur forme varie en fonction des classes annot√©es. Par exemple, les images de <code>lentilles</code> contiennent beaucoup plus d&rsquo;instances que la classe <code>lunettes</code>, pour un nombre √©quivalent d&rsquo;images. Par ailleurs, les objets sont majoritairement centr√©s dans l&rsquo;image m√™me si toutes les positions sont repr√©sent√©es. Leur taille quant √† elle diff√®re d&rsquo;une classe √† l&rsquo;autre, bien que les classes <code>lentilles</code> et <code>bague</code> g√©n√®rent un point chaud dans le coin en bas √† gauche, correspondant aux objets de petite taille.</p><center><img src=images/labels.png alt=Labels width=50%></center><h2 id=-une-semaine-pour-tout-changer->ü•∑ Une semaine pour tout changer ü•∑</h2><h3 id=ce-que-vous-navez-pas-vu>Ce que vous n&rsquo;avez pas vu&mldr;</h3><center><img src=images/deadline.png alt=Labels width=50%></center><ul><li><mark><strong>encore plus de <em>data processing</em></strong></mark> : apr√®s l&rsquo;<em>upload</em> de toutes vos donn√©es sur le serveur la semaine derni√®re, il a fallu les homog√©n√©iser. Certains fichiers contenaient des espaces en trop, les <em>splits</em> n&rsquo;√©taient pas toujours format√©s de la m√™me mani√®re, l&rsquo;arborescence √©tait parfois √† r√©organiser, <em>etc</em>.</li><li><mark><em><strong>get ready for training</strong></em></mark> : une fois le <em>dataset</em> format√©, il faut pr√©parer YOLO pour l&rsquo;apprentissage, et notamment √©crire le fichier de configuration qui va bien pour un apprentissage sur des donn√©es <em>custom</em> :</li></ul><center><img src=images/fichier_config_yolo.png alt=Labels width=80%></center><p>¬†</p><ul><li><mark><strong>la lente agonie des GPUs</strong></mark> : √† l&rsquo;heure o√π j&rsquo;√©cris ces lignes (14 d√©cembre √† ~minuit), nous en sommes √† 44 heures d&rsquo;apprentissage, r√©parties sur les 8 GPUs des serveurs 1 et 2 de l&rsquo;INSA, et chaque GPU a 8Go de m√©moire :</li></ul><center><img src=images/gpu1_gpu2.jpg alt="GPU1 et GPU2 sont sur un bateau" height=400px>
<img src=images/nvidia_smi.jpg alt=nvidia-smi height=400px></center>
&nbsp;
<center><img src=images/tensorboard.png alt=Tensorboard width=65%></center>
&nbsp;<ul><li><mark><strong>rat√©, on recommence &ndash; ou &ldquo;ce qui √©tait pr√©vu mais qui n&rsquo;a pas march√©&rdquo;</strong></mark> : le VPN se d√©connecte, la connexion SSH se ferme et arr√™te une liste d&rsquo;apprentissages fi√®rement lanc√©s pour tourner toute la nuit (on t&rsquo;avait pourtant pr√©venue de faire un <code>nohup</code> ou d&rsquo;utiliser <code>tmux</code>) ; les classes ne sont pas les bonnes, le <em>batch</em> est trop gros et explose la m√©moire, le <em>batch</em> est trop petit et explose le temps, le r√©seau est trop lourd et fait exploser CUDA.</li></ul><h3 id=configurations-entrain√©es>Configurations entrain√©es</h3><p>Les apprentissages faits ces derniers jours couvrent les configurations suivantes (en vert ce qui est fait, en jaune ce qui est en cours, en rouge ce qu&rsquo;il reste √† faire) :</p><center><img src=images/trainings.png alt="Configurations d'apprentissage" width=80%></center>
&nbsp;<p>Il √©tait pr√©vu de travailler avec diff√©rentes tailles de <em>batch</em>, en l&rsquo;occurrence 1 et 32. Finalement, un <em>batch</em> de 1 prend beaucoup trop de temps ; toutes les configurations ont √©t√© entrain√©es avec un <em>batch</em> de 64, voire de 96.</p><p><a href=https://github.com/ultralytics/yolov5#pretrained-checkpoints>Quatre versions</a> de YOLO ont finalement √©t√© test√©es, avec de la plus petite √† la plus grande : <code>n</code>, <code>s</code>, <code>m</code>, et <code>l</code>. Le d√©tail de ces architectures est visible soit dans Tensorboard en visualisant le graphe d&rsquo;une configuration correspondant √† la version choisie, soit directement dans les fichiers <code>.yaml</code> du dossier <code>models</code> dans le <em>workspace</em> yolov5.</p><h4 id=transfer-learning-fine-tuning-pre-trained-models><em>Transfer learning, fine-tuning, pre-trained models&mldr;</em></h4><p><code>freeze</code> veut dire qu&rsquo;en d√©but d&rsquo;apprentissage, les poids sont initialis√©s avec un mod√®le pr√©-entrain√© sur le <em>dataset</em> COCO, et que les poids de certaines courches ne sont pas mis √† jour pendant l&rsquo;apprentissage. Dans notre cas, on <em>freeze</em> le <em>backbone</em>. On &ldquo;transf√®re&rdquo; √©galement la t√¢che apprise par le r√©seau pr√©-entrain√© vers une nouvelle t√¢che, en changeant le nombre et le type des classes √† pr√©dire dans notre cas. On dit √©galement qu&rsquo;on affine l&rsquo;extraction des caract√©ristiques pr√©-entrain√© en mettant √† jour tout ou partie des poids pendant l&rsquo;apprentissage.</p><h2 id=-and-now-what->üî• <em>And now, what?</em> üî•</h2><p>On va de nouveau tirer au sort les configurations sur lesquelles vous vous positionnerez, puisque rien ne s&rsquo;est pass√© comme pr√©vu pendant les apprentissages. Mais on est agiles, on s&rsquo;adapte. Le plan sera ensuite le suivant :</p><ul><li>vous mettrez en place votre <em>workspace</em>, √† savoir : le clone de yolov5 dans votre <em>home directory</em>, ouvert dans VS Code avec un acc√®s en SSH au serveur GPU <code>srv-gei-gpu1</code> gr√¢ce √† l&rsquo;extension &ldquo;Remote SSH&rdquo; (<code>Ctrl+Shitf+X</code> pour ouvrir le gestionnaire d&rsquo;extensions), ainsi que l&rsquo;activation de l&rsquo;environnement virtuel qui va bien &ndash; cf. partie &ldquo;configuration du <em>workspace</em>&rdquo; ;</li><li>vous pouvez lancer un <code>tensorboard --logdir=/scratch/labi/DLCV/train</code> pour visualiser et comparer les diff√©rents apprentissages r√©alis√©s ;</li><li>vous pouvez √©galement ouvrir <em>via</em> VS Code en SSH le dossier <code>/scratch/labi/DLCV/train</code> en question, et ainsi acc√©der √† tous les graphiques de performance g√©n√©r√©s par YOLO pour chaque configuration d&rsquo;apprentissage (matrices de confusion, courbes de pr√©cision-rappel, F1-score <em>vs.</em> score de confiance, <em>etc</em>.) ;</li><li>pour √©valuer plus en d√©tails votre configuration (notamment sur la base de test), il vous faudra copier le fichier <code>/scratch/labi/DLCV/dataset/dlcv.yaml</code> dans votre dossier <code>path/to/yolov5/data/</code> puis appeler le script <code>val.py</code> avec les param√®tres qui vous int√©ressent. Plusieurs t√¢ches d&rsquo;√©valuations sont possibles : <code>train</code>, <code>val</code>, <code>test</code>, <code>speed</code>, <code>study</code>. Par exemple, l&rsquo;√©valuation sur base de test pour la configuration</li><li>√©valuez √©galement les mod√®les de mani√®re qualitative, c&rsquo;est-√†-dire en visualisant directement les r√©sultats, &ldquo;√† l&rsquo;oeil&rdquo;. Pour cela, lancez <code>detect.py</code> sur une nouvelle vid√©o par exemple (pas besoin d&rsquo;extraire les <em>frames</em>, le script prend aussi des vid√©os <code>.mp4</code> en entr√©e). Le r√©sultat est stock√© dans <code>path/to/yolo/runs/detect/exp_name</code>. Vous pouvez changer <code>exp_name</code> avec l&rsquo;option <code>--name</code> (plus pratique pour s&rsquo;y retrouver que d&rsquo;avoir <code>exp1</code>, <code>exp2</code>, <code>exp3</code>&mldr;)</li></ul><p>Ne partez pas ! Vous vous souvenez des qu√™tes annexes du Bingo de YOLO ? Je vous les remets en m√©moire juste au cas o√π&mldr; parce que maintenant que tout roule, vous pouvez lancer la d√©tection avec des poids COCO, faire du pas √† pas en mode <em>debug</em> pour aller chercher la valeur du poids, visualiser la <em>data augmentation</em> faite quand on lance <code>train.py</code>, <em>etc</em>.</p><center><img src=images/le_bingo_de_yolo.png alt="Le bingo de YOLO" width=80%></center><h2 id=annexe--configuration-du-workspace><strong>Annexe</strong> : configuration du <em>workspace</em></h2><p>Cette section vous guide dans la configuration de votre <em>workspace</em> avec les outils dont vous disposez en salle de TP. La configuration propos√©e se base sur un environnement Ubuntu 20.04, avec l&rsquo;IDE VSCode et la cr√©ation d&rsquo;un environnement virtuel √† l&rsquo;aide de <code>python venv</code>.
Vous √™tes √©videmment libres d&rsquo;utiliser n&rsquo;importe quel IDE si vous avez d&rsquo;autres pr√©f√©rences, ou d&rsquo;utiliser Anaconda pour cr√©er votre environnement virtuel&mldr; le principal √©tant que √ßa marche !</p><p><em><strong>Let&rsquo;s go</strong></em> :</p><hr><ul><li><p>üî•üíª <mark><strong><em>√©tape 1</em></mark> : clone de YOLOv5</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e>## Clonage du d√©p√¥t Github de la release 6.2 de yolov5 dans votre home directory</span>
</span></span><span style=display:flex><span>login@machine:~$ cd &lt;path/to/workspace&gt;
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;$ git clone -b v6.2 https://github.com/ultralytics/yolov5.git
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;$ cd yolov5
</span></span></code></pre></div></li><li><p>üî•üíª <mark><strong><em>√©tape 2</em></mark> : ouvrez une session SSH dans VSCode pour vous connecter √† srv-gei-gpu1</strong></p><p>Pour cela, il vous faut d&rsquo;abord installer l&rsquo;extension <code>Remote SSH</code> <em>via</em> le gestionnaire d&rsquo;extensions (raccourci Ctrl+Shift+X). Ensuite, tout en bas √† gauche vous pouvez ouvrir une session SSH et vous placer dans le dossier YOLOv5, en cliquant sur l&rsquo;ic√¥ne avec deux fl√®ches.</p></li><li><p>üî•üíª <mark><strong><em>√©tape 3</em></mark> : configuration de l&rsquo;environnement virtuel</strong></p><p>Ouvrez un nouveau terminal dans VS Code puis cr√©ez l&rsquo;environnement virtuel qui va bien :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e>## Configuration de l&#39;environnement virtuel nomm√© &#39;yolov5env&#39;</span>
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;/yolov5$ python3 -m venv yolov5env <span style=color:#75715e># Cr√©ation</span>
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;/yolov5$ source yolov5env/bin/activate <span style=color:#75715e># Activation</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/workspace&gt;/yolov5$ python3 -m pip install --upgrade pip <span style=color:#75715e># Mise √† jour de pip</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/workspace&gt;/yolov5$ pip3 install -r requirements.txt <span style=color:#75715e># Install libs</span>
</span></span></code></pre></div><p><em>A ce stade, toute l&rsquo;arborescence de YOLOv5 est en place, toutes les librairies sont install√©es.</em></p></li></ul><hr><ul><li><p>üî•üíª <mark><strong><em>√©tape 4</em></mark> : configuration de VS Code</strong></p><p>Assurez-vous ensuite que l&rsquo;extension pour Python est bien install√©e. Pour cela, acc√©dez √† l&rsquo;onglet &ldquo;Extensions&rdquo; <em>via</em> le raccourci <code>Ctrl + Shift + X</code> et cherchez <code>python</code>. Installez l&rsquo;extension si elle ne l&rsquo;est pas d√©j√† :</p><center><p><img src=images/install_extension_python_vscode.png alt="Installation de l&amp;rsquo;extension pour Python"></p></center><p>S√©lectionnez ensuite l&rsquo;interpr√©teur Python de l&rsquo;environnement virtuel que vous avez cr√©√© √† l&rsquo;√©tape 1, en utilisant le raccourci <code>Ctrl + Shift + P</code> pour faire appara√Ætre la palette de commande, puis en tapant la commande <code>Python: Select Interpreter</code>. Parmi les choix propos√©s, cliquez sur celui correspondant √† l&rsquo;environnement virtuel <code>yolov5env</code> :</p><center><p><img src=images/select_interpreter_vscode.png alt="S√©lection de l&amp;rsquo;interpr√©teur Python"></p></center></li></ul><hr><ul><li><p>üî•üíª <mark><strong><em>√©tape 5</em></mark> : voyons si vous avez suivi&mldr;</strong></p><p>Si tout est correctement configur√©, vous pouvez lancer un terminal dans VS Code <em>via</em> <code>Terminal > New Terminal</code> et taper la commande suivante :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/yolov5&gt;$ python detect.py --source <span style=color:#e6db74>&#39;https://ultralytics.com/images/zidane.jpg&#39;</span>
</span></span></code></pre></div><p>Une fois la commande ex√©cut√©e, vous retrouvez le r√©sultat de l&rsquo;√©xecution du mod√®le YOLOv5-S sur l&rsquo;image <code>zidane.jpg</code> dans le dossier <code>runs/detect/exp</code> :</p><center><p><img src=images/zidane.png alt="V√©rification du fonctionnement de YOLOv5"></p></center></li></ul><hr><p>üî•üéÜüëçüåü <strong>Well done !</strong></p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/clairelabitbonis/clairelabitbonis.github.io/edit/main/content/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/03_lets_see/index.fr.md title="Am√©liorez cette page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Am√©liorez cette page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/02_yolo/ title="DLCV-TP-01 | Le Bingo de YOLO !" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Pr√©c√©dent</div><div class=next-prev-text>DLCV-TP-01 | Le Bingo de YOLO !</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table des mati√®res</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#previously-on-dlcv-practical-sessions><strong>Previously on &ldquo;DLCV Practical sessions&rdquo;&mldr;</strong></a></li><li><a href=#il-est-beau-le-dataset>Il est beau le <em>dataset</em></a><ul><li><a href=#r√©partition-des-classes>R√©partition des classes</a></li><li><a href=#analyse-des-labels>Analyse des labels</a></li></ul></li><li><a href=#-une-semaine-pour-tout-changer->ü•∑ Une semaine pour tout changer ü•∑</a><ul><li><a href=#ce-que-vous-navez-pas-vu>Ce que vous n&rsquo;avez pas vu&mldr;</a></li><li><a href=#configurations-entrain√©es>Configurations entrain√©es</a><ul><li><a href=#transfer-learning-fine-tuning-pre-trained-models><em>Transfer learning, fine-tuning, pre-trained models&mldr;</em></a></li></ul></li></ul></li><li><a href=#-and-now-what->üî• <em>And now, what?</em> üî•</a></li><li><a href=#annexe--configuration-du-workspace><strong>Annexe</strong> : configuration du <em>workspace</em></a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/fr/#about>A propos</a></li><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/fr/#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contactez moi :</h5><ul><li><a href=mailto:clairelabitbonis@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>clairelabitbonis@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">¬© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Aliment√© par
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script type=text/javascript src=/js/darkreader.js></script>
<script type=text/javascript src=/js/darkmode-darkreader.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script>
<script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>