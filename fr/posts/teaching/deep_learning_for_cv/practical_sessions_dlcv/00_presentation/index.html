<!doctype html><html><head><title>DLCV-TP-00 | Pr√©sentation</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png><meta property="og:title" content="DLCV-TP-00 | Pr√©sentation"><meta property="og:description" content="Objectifs p√©dagogiques L&rsquo;objectif de ces s√©ances de travaux pratiques est de toucher √† toutes les √©tapes de l&rsquo;ing√©nierie du deep learning, √† savoir :
l&rsquo;acquisition et l&rsquo;annotation de donn√©es, l&rsquo;apprentissage de r√©seaux de convolution, l&rsquo;√©valuation des performances de la t√¢che apprise, la visualisation des r√©sultats obtenus. Pour cela, notre point de d√©part sera le d√©tecteur d&rsquo;objets tr√®s largement connu et utilis√© par les communaut√©s scientifique mais aussi industrielle : YOLO (You Only Look Once)."><meta property="og:type" content="article"><meta property="og:url" content="http://clairelabitbonis.github.io/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/"><meta property="og:image" content="http://clairelabitbonis.github.io/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/featured.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-04T10:00:00+09:00"><meta property="article:modified_time" content="2022-11-04T10:00:00+09:00"><meta name=description content="DLCV-TP-00 | Pr√©sentation"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/fr><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png alt=Logo>
Claire Labit-Bonis</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-fr"></span>
Fran√ßais</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation><span class="flag-icon flag-icon-gb"></span>
English</a></div></li><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/fr/search><input type=text name=keyword placeholder=Chercher data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/fr/posts data-filter=all>Articles</a></li><div class=subtree><li><a href=/fr/posts/introduction/ title=Introduction>Introduction</a></li><li><i class="fas fa-minus-circle"></i><a class=active href=/fr/posts/teaching/>Enseignements</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/fr/posts/teaching/3d_perception/>Perception 3D</a><ul><li><i class="fas fa-plus-circle"></i><a href=/fr/posts/teaching/3d_perception/practical_sessions_3d_perception/>Travaux pratiques</a><ul><li><a href=/fr/posts/teaching/3d_perception/practical_sessions_3d_perception/cc_segmentation/ title=Segmentation>Segmentation</a></li><li><a href=/fr/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/ title="Localisation mono.">Localisation mono.</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/fr/posts/teaching/deep_learning_for_cv/>Deep Learning & CV</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/fr/posts/teaching/deep_learning_for_cv/course_dlcv/>Cours</a><ul><li><a href=/fr/posts/teaching/deep_learning_for_cv/course_dlcv/01_why_deep_learning/ title="00 | De l'IA au DL">00 | De l'IA au DL</a></li><li><a href=/fr/posts/teaching/deep_learning_for_cv/course_dlcv/02_lets_learn_deeply/ title="01 | DD into OD">01 | DD into OD</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/>Travaux pratiques</a><ul class=active><li><a class=active href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/ title="00 | Pr√©sentation">00 | Pr√©sentation</a></li><li><a href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/02_yolo/ title="01 | YOLO !">01 | YOLO !</a></li><li><a href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/03_lets_see/ title="11 | Voyons...">11 | Voyons...</a></li></ul></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/claire_hu4fa84bbe5b0b14aeb1f21f53cdd20f7e_312167_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name>Claire Labit-Bonis</h5><p>vendredi 4 novembre 2022</p></div><div class=title><h1>DLCV-TP-00 | Pr√©sentation</h1></div><div class=post-content id=post-content><h2 id=objectifs-p√©dagogiques>Objectifs p√©dagogiques</h2><p>L&rsquo;objectif de ces s√©ances de travaux pratiques est de toucher √† toutes les √©tapes de l&rsquo;ing√©nierie du <em>deep learning</em>, √† savoir :</p><ul><li>l&rsquo;<strong>acquisition</strong> et l&rsquo;<strong>annotation</strong> de donn√©es,</li><li>l&rsquo;<strong>apprentissage</strong> de r√©seaux de convolution,</li><li>l&rsquo;<strong>√©valuation</strong> des performances de la t√¢che apprise,</li><li>la <strong>visualisation</strong> des r√©sultats obtenus.</li></ul><p>Pour cela, notre point de d√©part sera le d√©tecteur d&rsquo;objets tr√®s largement connu et utilis√© par les communaut√©s scientifique mais aussi industrielle : <a href=https://arxiv.org/pdf/1506.02640.pdf>YOLO</a> (You Only Look Once). Nous travaillerons avec la <a href=https://github.com/ultralytics/yolov5>version 5</a> sortie en 2020.</p><blockquote><p>A la fin des TPs, vous saurez comment utiliser YOLOv5, comment l&rsquo;entrainer sur vos propres donn√©es, et comment l&rsquo;√©valuer.</p></blockquote><h2 id=d√©roulement-des-s√©ances>D√©roulement des s√©ances</h2><blockquote><p><strong>Un¬∑e pour tou¬∑te¬∑s, tou¬∑te¬∑s pour un¬∑e !</strong></p><p>Parce que l&rsquo;union fait la force, que la joie et la bonne humeur facilitent l&rsquo;apprentissage &ndash; des humains &ndash;, les s√©ances de travaux pratiques se d√©rouleront dans un contexte √† la fois collectif et individuel, pas toujours sur les postes de travail, et toujours dans l&rsquo;int√©r√™t de la compr√©hension. Nous aurons tou¬∑te¬∑s un r√¥le √† jouer, √† chacune des √©tapes.</p></blockquote><p>Nous allons entrainer YOLOv5 √† d√©tecter plusieurs classes d&rsquo;objets, √† raison d&rsquo;une classe d&rsquo;objet par bin√¥me. Le <em>dataset</em> que nous allons construire pour cela sera commun aux deux groupes de TPs qui se d√©roulent en parall√®le (<em>e.g.</em>, groupes A1 & A2, groupes B1 & B2).</p><p>Pour cela, nous passerons par plusieurs √©tapes :</p><ul><li>üî• <mark><strong>√©tape 1 - acquisition</strong></mark> : chaque bin√¥me prendra plusieurs s√©quences vid√©o de la classe d&rsquo;objets qu&rsquo;il aura choisie parmi une liste propos√©e, et la mettra sur un serveur de donn√©es commun aux groupes A1/A2 et B1/B2 ;</li><li>üî• <mark><strong>√©tape 2 - annotation</strong></mark> : avec l&rsquo;outil CVAT, chaque bin√¥me annotera ses propres s√©quences d&rsquo;images avec la classe d&rsquo;objet choisie, de sorte qu&rsquo;√† la fin de la phase d&rsquo;annotation, le groupe entier de TP aura collectivement construit un <em>dataset</em> multi-classes dont tout le monde b√©n√©ficiera pour faire ses apprentissages ;</li><li>üî• <mark><strong>√©tape 3 - prise en main du code de YOLOv5</strong></mark> : √† la fin de cette √©tape, vous saurez appliquer sur vos propres images un mod√®le YOLOv5-S pr√©-entrain√© sur COCO, entrer dans l&rsquo;architecture du r√©seau et identifier ses diff√©rentes couches et leurs dimensions, visualiser la sortie de d√©tection obtenue, etc. Pour parvenir √† cette prise en main, un jeu de &ldquo;o√π est Charlie ?&rdquo; vous sera propos√© et vous poussera √† d√©cortiquer l&rsquo;ex√©cution du code pas √† pas. Vous devrez par exemple r√©pondre √† des questions comme &ldquo;quelle est la taille du tenseur en sortie de la couche 17 pour une image d&rsquo;entr√©e de 512x512x3 ?&rdquo; ;</li><li>üî• <mark><strong>√©tape 4 - apprentissage de YOLOv5 sur notre <em>dataset</em></strong></mark> : l&rsquo;id√©al pour analyser les performances d&rsquo;un jeu de param√®tres donn√© (r√©solution des images d&rsquo;entr√©e, taille du r√©seau, taille de <em>batch</em>, etc.) est de lancer autant d&rsquo;apprentissages que de configurations possibles et de les comparer ensuite pour s√©lectionner celle qui est la meilleure. On peut ensuite afficher sur un m√™me graphe diff√©rentes tailles de mod√®les, pour diff√©rentes r√©solutions, et comparer leur rapidit√© d&rsquo;ex√©cution √† la <em>mean average precision</em> qu&rsquo;ils r√©alisent sur un <em>dataset</em> donn√©, par exemple :</li></ul><center><p><img src=images/perfs_yolov5.png alt="Comparaison des performances de YOLOv5 sur COCO">
<em>Source : <a href=https://github.com/ultralytics/yolov5>https://github.com/ultralytics/yolov5</a></em></p></center><blockquote><p>Un apprentissage dure plusieurs heures. Pour pouvoir comparer toutes ces configurations, il faut soit disposer de plusieurs serveurs GPU puissants qui peuvent faire tourner en parall√®le plusieurs configurations, soit disposer de beaucoup de temps et √™tre patient&mldr;</p><p><strong>Et puis, un apprentissage, √ßa consomme de l&rsquo;√©nergie.</strong></p><p>Il n&rsquo;est donc pas envisageable que chaque bin√¥me lance un apprentissage pour chaque jeu de param√®tres puis fasse une analyse comparative des r√©sultats.</p><p><strong>MAIS !, l&rsquo;union fait la force, une fois de plus.</strong></p><p>Chaque bin√¥me se positionnera donc sur une configuration donn√©e et lancera l&rsquo;apprentissage associ√© sur le <em>dataset</em> du groupe de TP.</p></blockquote><ul><li>üî• <mark><strong>√©tape 5 - analyse des performances</strong></mark> : une fois tous les apprentissages faits, chaque bin√¥me pourra √©valuer les performances de sa propre configuration, analyser les r√©sultats de mani√®re quantitative, <em>i.e.</em>, avec des chiffres, et de mani√®re qualitative, <em>i.e.</em>, avec une visualisation &ldquo;√† l&rsquo;oeil&rdquo; des cas d&rsquo;erreur et des cas qui fonctionnent. Une √©valuation comparative sera √©galement r√©alisable, puisque tout le monde aura acc√®s aux r√©sultats obtenus par les autres bin√¥mes, au travers d&rsquo;un <em>leaderboard</em> commun au groupe.</li></ul><h2 id=modalit√©s-d√©valuation>Modalit√©s d&rsquo;√©valuation</h2><p>Chaque bin√¥me produira une capsule vid√©o (c&rsquo;est-√†-dire une s√©quence vid√©o) d&rsquo;environ 5~10 minutes. Bien s√ªr, la pertinence du contenu importe plus que la longueur de la capsule ; libre √† vous donc de d√©cider du temps qu&rsquo;il vous faut pour aborder, par exemple :</p><ul><li>les statistiques de vos acquisitions (la classe choisie, les diff√©rents contextes d&rsquo;acquisition, le nombre d&rsquo;images annot√©es, la strat√©gie d&rsquo;annotation, le temps pass√©, les difficult√©s rencontr√©es&mldr;) ;</li><li>l&rsquo;analyse quantitative de vos r√©sultats (m√©triques de performance, nombre d&rsquo;<em>epochs</em> pour converger, rapidit√© d&rsquo;ex√©cution du mod√®le, r√©partition de la performance sur les diff√©rentes classes d&rsquo;objets, potentiel <em>overfitting</em>, comparaison des m√©triques aux autres configurations et interpr√©tation de cette comparaison&mldr;) ;</li><li>l&rsquo;analyse qualitative de vos r√©sultats (visualisation de l&rsquo;ex√©cution du mod√®le, performances selon le contexte d&rsquo;acquisition, selon la qualit√© de l&rsquo;annotation&mldr;) ;</li><li>d&rsquo;autres id√©es que vous pourriez avoir.</li></ul><p>Vous l&rsquo;aurez compris, l&rsquo;√©valuation de votre travail ne d√©pendra pas de la performance de votre apprentissage (et donc de la configuration qui vous aura √©t√© attribu√©e), mais plut√¥t de l&rsquo;analyse que vous serez capable d&rsquo;en faire.</p><h2 id=outils-et-configuration-du-workspace>Outils et configuration du <em>workspace</em></h2><h3 id=ide-et-clone-de-yolov5----si-vous-voulez-√™tre-en-local-sur-vos-machines-cpu-only>IDE et clone de YOLOv5 &ndash; si vous voulez √™tre en local sur vos machines (CPU <em>only</em>)</h3><p>Cette section vous guide dans la configuration de votre <em>workspace</em> avec les outils dont vous disposez en salle de TP. La configuration propos√©e se base sur un environnement Ubuntu 20.04, avec l&rsquo;IDE VSCode et la cr√©ation d&rsquo;un environnement virtuel √† l&rsquo;aide de <code>python venv</code>.
Vous √™tes √©videmment libres d&rsquo;utiliser n&rsquo;importe quel IDE si vous avez d&rsquo;autres pr√©f√©rences, ou d&rsquo;utiliser Anaconda pour cr√©er votre environnement virtuel&mldr; le principal √©tant que √ßa marche !</p><p><em><strong>Let&rsquo;s go</strong></em> :</p><hr><ul><li><p>üî•üíª <mark><strong><em>√©tape 1</em></mark> : mise en place de l&rsquo;arborescence</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e>## Clonage du d√©p√¥t Github de la release 6.2 de yolov5</span>
</span></span><span style=display:flex><span>login@machine:~$ cd &lt;path/to/workspace&gt;
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;$ git clone -b v6.2 https://github.com/ultralytics/yolov5.git
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;$ cd yolov5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Configuration de l&#39;environnement virtuel nomm√© &#39;yolov5env&#39;</span>
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;/yolov5$ python3 -m venv yolov5env <span style=color:#75715e># Cr√©ation</span>
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;/yolov5$ source yolov5env/bin/activate <span style=color:#75715e># Activation</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/workspace&gt;/yolov5$ python3 -m pip install --upgrade pip <span style=color:#75715e># Mise √† jour de pip</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/workspace&gt;/yolov5$ pip3 install -r requirements.txt <span style=color:#75715e># Install libs</span>
</span></span></code></pre></div><p><em>A ce stade, toute l&rsquo;arborescence de YOLOv5 est en place, toutes les librairies sont install√©es.</em></p></li></ul><hr><ul><li><p>üî•üíª <mark><strong><em>√©tape 2</em></mark> : configuration de VS Code</strong></p><p>Dans VS Code, ouvrez le dossier <code>yolov5</code> de l&rsquo;√©tape pr√©c√©dente :</p><center><p><img src=images/open_folder_vscode.png alt="Ouverture du workspace dans VS Code"></p></center><p>Assurez-vous ensuite que l&rsquo;extension pour Python est bien install√©e. Pour cela, acc√©dez √† l&rsquo;onglet &ldquo;Extensions&rdquo; <em>via</em> le raccourci <code>Ctrl + Shift + X</code> et cherchez <code>python</code>. Installez l&rsquo;extension si elle ne l&rsquo;est pas d√©j√† :</p><center><p><img src=images/install_extension_python_vscode.png alt="Installation de l&amp;rsquo;extension pour Python"></p></center><p>S√©lectionnez ensuite l&rsquo;interpr√©teur Python de l&rsquo;environnement virtuel que vous avez cr√©√© √† l&rsquo;√©tape 1, en utilisant le raccourci <code>Ctrl + Shift + P</code> pour faire appara√Ætre la palette de commande, puis en tapant la commande <code>Python: Select Interpreter</code>. Parmi les choix propos√©s, cliquez sur celui correspondant √† l&rsquo;environnement virtuel <code>yolov5env</code> :</p><center><p><img src=images/select_interpreter_vscode.png alt="S√©lection de l&amp;rsquo;interpr√©teur Python"></p></center></li></ul><hr><ul><li><p>üî•üíª <mark><strong><em>√©tape 3</em></mark> : voyons si vous avez suivi&mldr;</strong></p><p>Si tout est correctement configur√©, vous pouvez lancer un terminal dans VS Code <em>via</em> <code>Terminal > New Terminal</code> et taper la commande suivante :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/yolov5&gt;$ python detect.py --source <span style=color:#e6db74>&#39;https://ultralytics.com/images/zidane.jpg&#39;</span>
</span></span></code></pre></div><p>Une fois la commande ex√©cut√©e, vous retrouvez le r√©sultat de l&rsquo;√©xecution du mod√®le YOLOv5-S sur l&rsquo;image <code>zidane.jpg</code> dans le dossier <code>runs/detect/exp</code> :</p><center><p><img src=images/zidane.png alt="V√©rification du fonctionnement de YOLOv5"></p></center></li></ul><hr><p>üî•üéÜüëçüåü <strong>Well done !</strong></p><h3 id=ide-et-clone-de-yolov5----pour-tourner-sur-le-serveur-gpu>IDE et clone de YOLOv5 &ndash; pour tourner sur le serveur GPU</h3><p>Une fois connect√© en SSH au serveur GPU, activez l&rsquo;environnement virtuel cr√©√© par Pierre :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>login@serveur-gpu:~$ cd &lt;path/to/workspace&gt;/yolov5
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>env<span style=color:#f92672>)</span> login@serveur-gpu:&lt;path/to/workspace&gt;/yolov5$ source /scratch/marigo/venv/bin/activate
</span></span></code></pre></div><h3 id=lancer-le-code-en-mode-debug>Lancer le code en mode <em>debug</em></h3><p>Dans l&rsquo;onglet <em>debug</em>, cliquez sur &ldquo;cr√©er un fichier launch.json&rdquo;. Vous pouvez ensuite configurer le <code>.json</code> comme ci-dessous, et adapter les param√®tres d&rsquo;appel :</p><p><img src=images/launchjson.png alt="Launch .JSON"></p><p>Vous pouvez ensuite ajouter une configuration pour le <em>debug</em> du fichier <code>train.py</code> par exemple.</p><h3 id=cvat-pour-lannotation>CVAT pour l&rsquo;annotation</h3><h3 id=serveur-de-donn√©es-pour-le-dataset>Serveur de donn√©es pour le <em>dataset</em></h3><h2 id=planning-des-s√©ances>Planning des s√©ances</h2><center><table><thead><tr><th style=text-align:center>Groupe</th><th style=text-align:center>Dates</th><th style=text-align:center>Salles</th><th style=text-align:center>Intervenant¬∑e</th></tr></thead><tbody><tr><td style=text-align:center>A1</td><td style=text-align:center>30/11/2022 \ 12h30 - 15h15<br>07/12/2022 \ 09h30 - 12h15<br>14/12/2022 \ 15h30 - 18h15</td><td style=text-align:center>GEI-111-A<br>GEI-109-A<br>GEI-111-A</td><td style=text-align:center>Claire LABIT-BONIS</td></tr><tr><td style=text-align:center>A2</td><td style=text-align:center>30/11/2022 \ 12h30 - 15h15<br>07/12/2022 \ 09h30 - 12h15<br>14/12/2022 \ 15h30 - 18h15</td><td style=text-align:center>GEI-111-B<br>GEI-109-B<br>GEI-111-B</td><td style=text-align:center>Smail AIT BOUHSAIN</td></tr><tr><td style=text-align:center>B1</td><td style=text-align:center>30/11/2022 \ 15h30 - 18h15<br>07/12/2022 \ 15h30 - 18h15<br>14/12/2022 \ 09h30 - 12h15</td><td style=text-align:center>GEI-111-A<br>GEI-111-A<br>GEI-109-A</td><td style=text-align:center>Claire LABIT-BONIS</td></tr><tr><td style=text-align:center>B2</td><td style=text-align:center>30/11/2022 \ 15h30 - 18h15<br>07/12/2022 \ 15h30 - 18h15<br>14/12/2022 \ 09h30 - 12h15</td><td style=text-align:center>GEI-111-B<br>GEI-111-B<br>GEI-109-B</td><td style=text-align:center>Pierre MARIGO</td></tr></tbody></table></center></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/clairelabitbonis/clairelabitbonis.github.io/edit/main/content/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/index.fr.md title="Am√©liorez cette page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Am√©liorez cette page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/fr/posts/teaching/deep_learning_for_cv/course_dlcv/02_lets_learn_deeply/ title="DLCV-CM-01 | Deep dive into object detection" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Pr√©c√©dent</div><div class=next-prev-text>DLCV-CM-01 | Deep dive into object detection</div></a></div><div class="col-md-6 next-article"><a href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/02_yolo/ title="DLCV-TP-01 | Le Bingo de YOLO !" class="btn btn-outline-info"><div>Suivant <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>DLCV-TP-01 | Le Bingo de YOLO !</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a><div class="dropdown languageSelector"><a class="btn dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-fr"></span>
Fran√ßais</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation><span class="flag-icon flag-icon-gb"></span>
English</a></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table des mati√®res</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#objectifs-p√©dagogiques>Objectifs p√©dagogiques</a></li><li><a href=#d√©roulement-des-s√©ances>D√©roulement des s√©ances</a></li><li><a href=#modalit√©s-d√©valuation>Modalit√©s d&rsquo;√©valuation</a></li><li><a href=#outils-et-configuration-du-workspace>Outils et configuration du <em>workspace</em></a><ul><li><a href=#ide-et-clone-de-yolov5----si-vous-voulez-√™tre-en-local-sur-vos-machines-cpu-only>IDE et clone de YOLOv5 &ndash; si vous voulez √™tre en local sur vos machines (CPU <em>only</em>)</a></li><li><a href=#ide-et-clone-de-yolov5----pour-tourner-sur-le-serveur-gpu>IDE et clone de YOLOv5 &ndash; pour tourner sur le serveur GPU</a></li><li><a href=#lancer-le-code-en-mode-debug>Lancer le code en mode <em>debug</em></a></li><li><a href=#cvat-pour-lannotation>CVAT pour l&rsquo;annotation</a></li><li><a href=#serveur-de-donn√©es-pour-le-dataset>Serveur de donn√©es pour le <em>dataset</em></a></li></ul></li><li><a href=#planning-des-s√©ances>Planning des s√©ances</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/fr/#about>A propos</a></li><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/fr/#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contactez moi :</h5><ul><li><a href=mailto:clairelabitbonis@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>clairelabitbonis@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">¬© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Aliment√© par
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script type=text/javascript src=/js/darkreader.js></script>
<script type=text/javascript src=/js/darkmode-darkreader.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script>
<script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>