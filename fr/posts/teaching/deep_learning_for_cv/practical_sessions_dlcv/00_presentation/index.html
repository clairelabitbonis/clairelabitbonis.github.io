<!doctype html><html><head><title>DLCV-TP-00 | Présentation</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png><meta property="og:title" content="DLCV-TP-00 | Présentation"><meta property="og:description" content="Objectifs pédagogiques L&rsquo;objectif de ces séances de travaux pratiques est de toucher à toutes les étapes de l&rsquo;ingénierie du deep learning, à savoir :
l&rsquo;acquisition et l&rsquo;annotation de données, l&rsquo;apprentissage de réseaux de convolution, l&rsquo;évaluation des performances de la tâche apprise, la visualisation des résultats obtenus. Pour cela, notre point de départ sera le détecteur d&rsquo;objets très largement connu et utilisé par les communautés scientifique mais aussi industrielle : YOLO (You Only Look Once)."><meta property="og:type" content="article"><meta property="og:url" content="http://clairelabitbonis.github.io/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/"><meta property="og:image" content="http://clairelabitbonis.github.io/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/featured.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-04T10:00:00+09:00"><meta property="article:modified_time" content="2022-11-04T10:00:00+09:00"><meta name=description content="DLCV-TP-00 | Présentation"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/fr><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png alt=Logo>
Claire Labit-Bonis</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-fr"></span>
Français</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation><span class="flag-icon flag-icon-gb"></span>
English</a></div></li><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/fr/search><input type=text name=keyword placeholder=Chercher data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/fr/posts data-filter=all>Articles</a></li><div class=subtree><li><a href=/fr/posts/introduction/ title=Introduction>Introduction</a></li><li><i class="fas fa-minus-circle"></i><a class=active href=/fr/posts/teaching/>Enseignements</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/fr/posts/teaching/3d_perception/>Perception 3D</a><ul><li><i class="fas fa-plus-circle"></i><a href=/fr/posts/teaching/3d_perception/practical_sessions_3d_perception/>Travaux pratiques</a><ul><li><a href=/fr/posts/teaching/3d_perception/practical_sessions_3d_perception/cc_segmentation/ title=Segmentation>Segmentation</a></li><li><a href=/fr/posts/teaching/3d_perception/practical_sessions_3d_perception/monocular_localization/ title="Localisation mono.">Localisation mono.</a></li></ul></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/fr/posts/teaching/deep_learning_for_cv/>Deep Learning & CV</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/fr/posts/teaching/deep_learning_for_cv/course_dlcv/>Cours</a><ul><li><a href=/fr/posts/teaching/deep_learning_for_cv/course_dlcv/01_why_deep_learning/ title="00 | De l'IA au DL">00 | De l'IA au DL</a></li><li><a href=/fr/posts/teaching/deep_learning_for_cv/course_dlcv/02_lets_learn_deeply/ title="01 | DD into OD">01 | DD into OD</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/>Travaux pratiques</a><ul class=active><li><a class=active href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/ title="00 | Présentation">00 | Présentation</a></li><li><a href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/02_yolo/ title="01 | YOLO !">01 | YOLO !</a></li><li><a href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/03_lets_see/ title="11 | Voyons...">11 | Voyons...</a></li></ul></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/claire_hu4fa84bbe5b0b14aeb1f21f53cdd20f7e_312167_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name>Claire Labit-Bonis</h5><p>vendredi 4 novembre 2022</p></div><div class=title><h1>DLCV-TP-00 | Présentation</h1></div><div class=post-content id=post-content><h2 id=objectifs-pédagogiques>Objectifs pédagogiques</h2><p>L&rsquo;objectif de ces séances de travaux pratiques est de toucher à toutes les étapes de l&rsquo;ingénierie du <em>deep learning</em>, à savoir :</p><ul><li>l&rsquo;<strong>acquisition</strong> et l&rsquo;<strong>annotation</strong> de données,</li><li>l&rsquo;<strong>apprentissage</strong> de réseaux de convolution,</li><li>l&rsquo;<strong>évaluation</strong> des performances de la tâche apprise,</li><li>la <strong>visualisation</strong> des résultats obtenus.</li></ul><p>Pour cela, notre point de départ sera le détecteur d&rsquo;objets très largement connu et utilisé par les communautés scientifique mais aussi industrielle : <a href=https://arxiv.org/pdf/1506.02640.pdf>YOLO</a> (You Only Look Once). Nous travaillerons avec la <a href=https://github.com/ultralytics/yolov5>version 5</a> sortie en 2020.</p><blockquote><p>A la fin des TPs, vous saurez comment utiliser YOLOv5, comment l&rsquo;entrainer sur vos propres données, et comment l&rsquo;évaluer.</p></blockquote><h2 id=déroulement-des-séances>Déroulement des séances</h2><blockquote><p><strong>Un·e pour tou·te·s, tou·te·s pour un·e !</strong></p><p>Parce que l&rsquo;union fait la force, que la joie et la bonne humeur facilitent l&rsquo;apprentissage &ndash; des humains &ndash;, les séances de travaux pratiques se dérouleront dans un contexte à la fois collectif et individuel, pas toujours sur les postes de travail, et toujours dans l&rsquo;intérêt de la compréhension. Nous aurons tou·te·s un rôle à jouer, à chacune des étapes.</p></blockquote><p>Nous allons entrainer YOLOv5 à détecter plusieurs classes d&rsquo;objets, à raison d&rsquo;une classe d&rsquo;objet par binôme. Le <em>dataset</em> que nous allons construire pour cela sera commun aux deux groupes de TPs qui se déroulent en parallèle (<em>e.g.</em>, groupes A1 & A2, groupes B1 & B2).</p><p>Pour cela, nous passerons par plusieurs étapes :</p><ul><li>🔥 <mark><strong>étape 1 - acquisition</strong></mark> : chaque binôme prendra plusieurs séquences vidéo de la classe d&rsquo;objets qu&rsquo;il aura choisie parmi une liste proposée, et la mettra sur un serveur de données commun aux groupes A1/A2 et B1/B2 ;</li><li>🔥 <mark><strong>étape 2 - annotation</strong></mark> : avec l&rsquo;outil CVAT, chaque binôme annotera ses propres séquences d&rsquo;images avec la classe d&rsquo;objet choisie, de sorte qu&rsquo;à la fin de la phase d&rsquo;annotation, le groupe entier de TP aura collectivement construit un <em>dataset</em> multi-classes dont tout le monde bénéficiera pour faire ses apprentissages ;</li><li>🔥 <mark><strong>étape 3 - prise en main du code de YOLOv5</strong></mark> : à la fin de cette étape, vous saurez appliquer sur vos propres images un modèle YOLOv5-S pré-entrainé sur COCO, entrer dans l&rsquo;architecture du réseau et identifier ses différentes couches et leurs dimensions, visualiser la sortie de détection obtenue, etc. Pour parvenir à cette prise en main, un jeu de &ldquo;où est Charlie ?&rdquo; vous sera proposé et vous poussera à décortiquer l&rsquo;exécution du code pas à pas. Vous devrez par exemple répondre à des questions comme &ldquo;quelle est la taille du tenseur en sortie de la couche 17 pour une image d&rsquo;entrée de 512x512x3 ?&rdquo; ;</li><li>🔥 <mark><strong>étape 4 - apprentissage de YOLOv5 sur notre <em>dataset</em></strong></mark> : l&rsquo;idéal pour analyser les performances d&rsquo;un jeu de paramètres donné (résolution des images d&rsquo;entrée, taille du réseau, taille de <em>batch</em>, etc.) est de lancer autant d&rsquo;apprentissages que de configurations possibles et de les comparer ensuite pour sélectionner celle qui est la meilleure. On peut ensuite afficher sur un même graphe différentes tailles de modèles, pour différentes résolutions, et comparer leur rapidité d&rsquo;exécution à la <em>mean average precision</em> qu&rsquo;ils réalisent sur un <em>dataset</em> donné, par exemple :</li></ul><center><p><img src=images/perfs_yolov5.png alt="Comparaison des performances de YOLOv5 sur COCO">
<em>Source : <a href=https://github.com/ultralytics/yolov5>https://github.com/ultralytics/yolov5</a></em></p></center><blockquote><p>Un apprentissage dure plusieurs heures. Pour pouvoir comparer toutes ces configurations, il faut soit disposer de plusieurs serveurs GPU puissants qui peuvent faire tourner en parallèle plusieurs configurations, soit disposer de beaucoup de temps et être patient&mldr;</p><p><strong>Et puis, un apprentissage, ça consomme de l&rsquo;énergie.</strong></p><p>Il n&rsquo;est donc pas envisageable que chaque binôme lance un apprentissage pour chaque jeu de paramètres puis fasse une analyse comparative des résultats.</p><p><strong>MAIS !, l&rsquo;union fait la force, une fois de plus.</strong></p><p>Chaque binôme se positionnera donc sur une configuration donnée et lancera l&rsquo;apprentissage associé sur le <em>dataset</em> du groupe de TP.</p></blockquote><ul><li>🔥 <mark><strong>étape 5 - analyse des performances</strong></mark> : une fois tous les apprentissages faits, chaque binôme pourra évaluer les performances de sa propre configuration, analyser les résultats de manière quantitative, <em>i.e.</em>, avec des chiffres, et de manière qualitative, <em>i.e.</em>, avec une visualisation &ldquo;à l&rsquo;oeil&rdquo; des cas d&rsquo;erreur et des cas qui fonctionnent. Une évaluation comparative sera également réalisable, puisque tout le monde aura accès aux résultats obtenus par les autres binômes, au travers d&rsquo;un <em>leaderboard</em> commun au groupe.</li></ul><h2 id=modalités-dévaluation>Modalités d&rsquo;évaluation</h2><p>Chaque binôme produira une capsule vidéo (c&rsquo;est-à-dire une séquence vidéo) d&rsquo;environ 5~10 minutes. Bien sûr, la pertinence du contenu importe plus que la longueur de la capsule ; libre à vous donc de décider du temps qu&rsquo;il vous faut pour aborder, par exemple :</p><ul><li>les statistiques de vos acquisitions (la classe choisie, les différents contextes d&rsquo;acquisition, le nombre d&rsquo;images annotées, la stratégie d&rsquo;annotation, le temps passé, les difficultés rencontrées&mldr;) ;</li><li>l&rsquo;analyse quantitative de vos résultats (métriques de performance, nombre d&rsquo;<em>epochs</em> pour converger, rapidité d&rsquo;exécution du modèle, répartition de la performance sur les différentes classes d&rsquo;objets, potentiel <em>overfitting</em>, comparaison des métriques aux autres configurations et interprétation de cette comparaison&mldr;) ;</li><li>l&rsquo;analyse qualitative de vos résultats (visualisation de l&rsquo;exécution du modèle, performances selon le contexte d&rsquo;acquisition, selon la qualité de l&rsquo;annotation&mldr;) ;</li><li>d&rsquo;autres idées que vous pourriez avoir.</li></ul><p>Vous l&rsquo;aurez compris, l&rsquo;évaluation de votre travail ne dépendra pas de la performance de votre apprentissage (et donc de la configuration qui vous aura été attribuée), mais plutôt de l&rsquo;analyse que vous serez capable d&rsquo;en faire.</p><h2 id=outils-et-configuration-du-workspace>Outils et configuration du <em>workspace</em></h2><h3 id=ide-et-clone-de-yolov5----si-vous-voulez-être-en-local-sur-vos-machines-cpu-only>IDE et clone de YOLOv5 &ndash; si vous voulez être en local sur vos machines (CPU <em>only</em>)</h3><p>Cette section vous guide dans la configuration de votre <em>workspace</em> avec les outils dont vous disposez en salle de TP. La configuration proposée se base sur un environnement Ubuntu 20.04, avec l&rsquo;IDE VSCode et la création d&rsquo;un environnement virtuel à l&rsquo;aide de <code>python venv</code>.
Vous êtes évidemment libres d&rsquo;utiliser n&rsquo;importe quel IDE si vous avez d&rsquo;autres préférences, ou d&rsquo;utiliser Anaconda pour créer votre environnement virtuel&mldr; le principal étant que ça marche !</p><p><em><strong>Let&rsquo;s go</strong></em> :</p><hr><ul><li><p>🔥💻 <mark><strong><em>étape 1</em></mark> : mise en place de l&rsquo;arborescence</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e>## Clonage du dépôt Github de la release 6.2 de yolov5</span>
</span></span><span style=display:flex><span>login@machine:~$ cd &lt;path/to/workspace&gt;
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;$ git clone -b v6.2 https://github.com/ultralytics/yolov5.git
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;$ cd yolov5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Configuration de l&#39;environnement virtuel nommé &#39;yolov5env&#39;</span>
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;/yolov5$ python3 -m venv yolov5env <span style=color:#75715e># Création</span>
</span></span><span style=display:flex><span>login@machine:&lt;path/to/workspace&gt;/yolov5$ source yolov5env/bin/activate <span style=color:#75715e># Activation</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/workspace&gt;/yolov5$ python3 -m pip install --upgrade pip <span style=color:#75715e># Mise à jour de pip</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/workspace&gt;/yolov5$ pip3 install -r requirements.txt <span style=color:#75715e># Install libs</span>
</span></span></code></pre></div><p><em>A ce stade, toute l&rsquo;arborescence de YOLOv5 est en place, toutes les librairies sont installées.</em></p></li></ul><hr><ul><li><p>🔥💻 <mark><strong><em>étape 2</em></mark> : configuration de VS Code</strong></p><p>Dans VS Code, ouvrez le dossier <code>yolov5</code> de l&rsquo;étape précédente :</p><center><p><img src=images/open_folder_vscode.png alt="Ouverture du workspace dans VS Code"></p></center><p>Assurez-vous ensuite que l&rsquo;extension pour Python est bien installée. Pour cela, accédez à l&rsquo;onglet &ldquo;Extensions&rdquo; <em>via</em> le raccourci <code>Ctrl + Shift + X</code> et cherchez <code>python</code>. Installez l&rsquo;extension si elle ne l&rsquo;est pas déjà :</p><center><p><img src=images/install_extension_python_vscode.png alt="Installation de l&amp;rsquo;extension pour Python"></p></center><p>Sélectionnez ensuite l&rsquo;interpréteur Python de l&rsquo;environnement virtuel que vous avez créé à l&rsquo;étape 1, en utilisant le raccourci <code>Ctrl + Shift + P</code> pour faire apparaître la palette de commande, puis en tapant la commande <code>Python: Select Interpreter</code>. Parmi les choix proposés, cliquez sur celui correspondant à l&rsquo;environnement virtuel <code>yolov5env</code> :</p><center><p><img src=images/select_interpreter_vscode.png alt="Sélection de l&amp;rsquo;interpréteur Python"></p></center></li></ul><hr><ul><li><p>🔥💻 <mark><strong><em>étape 3</em></mark> : voyons si vous avez suivi&mldr;</strong></p><p>Si tout est correctement configuré, vous pouvez lancer un terminal dans VS Code <em>via</em> <code>Terminal > New Terminal</code> et taper la commande suivante :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#f92672>(</span>yolov5env<span style=color:#f92672>)</span> login@machine:&lt;path/to/yolov5&gt;$ python detect.py --source <span style=color:#e6db74>&#39;https://ultralytics.com/images/zidane.jpg&#39;</span>
</span></span></code></pre></div><p>Une fois la commande exécutée, vous retrouvez le résultat de l&rsquo;éxecution du modèle YOLOv5-S sur l&rsquo;image <code>zidane.jpg</code> dans le dossier <code>runs/detect/exp</code> :</p><center><p><img src=images/zidane.png alt="Vérification du fonctionnement de YOLOv5"></p></center></li></ul><hr><p>🔥🎆👍🌟 <strong>Well done !</strong></p><h3 id=ide-et-clone-de-yolov5----pour-tourner-sur-le-serveur-gpu>IDE et clone de YOLOv5 &ndash; pour tourner sur le serveur GPU</h3><p>Une fois connecté en SSH au serveur GPU, activez l&rsquo;environnement virtuel créé par Pierre :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>login@serveur-gpu:~$ cd &lt;path/to/workspace&gt;/yolov5
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>env<span style=color:#f92672>)</span> login@serveur-gpu:&lt;path/to/workspace&gt;/yolov5$ source /scratch/marigo/venv/bin/activate
</span></span></code></pre></div><h3 id=lancer-le-code-en-mode-debug>Lancer le code en mode <em>debug</em></h3><p>Dans l&rsquo;onglet <em>debug</em>, cliquez sur &ldquo;créer un fichier launch.json&rdquo;. Vous pouvez ensuite configurer le <code>.json</code> comme ci-dessous, et adapter les paramètres d&rsquo;appel :</p><p><img src=images/launchjson.png alt="Launch .JSON"></p><p>Vous pouvez ensuite ajouter une configuration pour le <em>debug</em> du fichier <code>train.py</code> par exemple.</p><h3 id=cvat-pour-lannotation>CVAT pour l&rsquo;annotation</h3><h3 id=serveur-de-données-pour-le-dataset>Serveur de données pour le <em>dataset</em></h3><h2 id=planning-des-séances>Planning des séances</h2><center><table><thead><tr><th style=text-align:center>Groupe</th><th style=text-align:center>Dates</th><th style=text-align:center>Salles</th><th style=text-align:center>Intervenant·e</th></tr></thead><tbody><tr><td style=text-align:center>A1</td><td style=text-align:center>30/11/2022 \ 12h30 - 15h15<br>07/12/2022 \ 09h30 - 12h15<br>14/12/2022 \ 15h30 - 18h15</td><td style=text-align:center>GEI-111-A<br>GEI-109-A<br>GEI-111-A</td><td style=text-align:center>Claire LABIT-BONIS</td></tr><tr><td style=text-align:center>A2</td><td style=text-align:center>30/11/2022 \ 12h30 - 15h15<br>07/12/2022 \ 09h30 - 12h15<br>14/12/2022 \ 15h30 - 18h15</td><td style=text-align:center>GEI-111-B<br>GEI-109-B<br>GEI-111-B</td><td style=text-align:center>Smail AIT BOUHSAIN</td></tr><tr><td style=text-align:center>B1</td><td style=text-align:center>30/11/2022 \ 15h30 - 18h15<br>07/12/2022 \ 15h30 - 18h15<br>14/12/2022 \ 09h30 - 12h15</td><td style=text-align:center>GEI-111-A<br>GEI-111-A<br>GEI-109-A</td><td style=text-align:center>Claire LABIT-BONIS</td></tr><tr><td style=text-align:center>B2</td><td style=text-align:center>30/11/2022 \ 15h30 - 18h15<br>07/12/2022 \ 15h30 - 18h15<br>14/12/2022 \ 09h30 - 12h15</td><td style=text-align:center>GEI-111-B<br>GEI-111-B<br>GEI-109-B</td><td style=text-align:center>Pierre MARIGO</td></tr></tbody></table></center></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/clairelabitbonis/clairelabitbonis.github.io/edit/main/content/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation/index.fr.md title="Améliorez cette page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Améliorez cette page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/fr/posts/teaching/deep_learning_for_cv/course_dlcv/02_lets_learn_deeply/ title="DLCV-CM-01 | Deep dive into object detection" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Précédent</div><div class=next-prev-text>DLCV-CM-01 | Deep dive into object detection</div></a></div><div class="col-md-6 next-article"><a href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/02_yolo/ title="DLCV-TP-01 | Le Bingo de YOLO !" class="btn btn-outline-info"><div>Suivant <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>DLCV-TP-01 | Le Bingo de YOLO !</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a><div class="dropdown languageSelector"><a class="btn dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-fr"></span>
Français</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/posts/teaching/deep_learning_for_cv/practical_sessions_dlcv/00_presentation><span class="flag-icon flag-icon-gb"></span>
English</a></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table des matières</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#objectifs-pédagogiques>Objectifs pédagogiques</a></li><li><a href=#déroulement-des-séances>Déroulement des séances</a></li><li><a href=#modalités-dévaluation>Modalités d&rsquo;évaluation</a></li><li><a href=#outils-et-configuration-du-workspace>Outils et configuration du <em>workspace</em></a><ul><li><a href=#ide-et-clone-de-yolov5----si-vous-voulez-être-en-local-sur-vos-machines-cpu-only>IDE et clone de YOLOv5 &ndash; si vous voulez être en local sur vos machines (CPU <em>only</em>)</a></li><li><a href=#ide-et-clone-de-yolov5----pour-tourner-sur-le-serveur-gpu>IDE et clone de YOLOv5 &ndash; pour tourner sur le serveur GPU</a></li><li><a href=#lancer-le-code-en-mode-debug>Lancer le code en mode <em>debug</em></a></li><li><a href=#cvat-pour-lannotation>CVAT pour l&rsquo;annotation</a></li><li><a href=#serveur-de-données-pour-le-dataset>Serveur de données pour le <em>dataset</em></a></li></ul></li><li><a href=#planning-des-séances>Planning des séances</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/fr/#about>A propos</a></li><li class=nav-item><a class=smooth-scroll href=http://clairelabitbonis.github.io/fr/#publications>Publications</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contactez moi :</h5><ul><li><a href=mailto:clairelabitbonis@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>clairelabitbonis@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Alimenté par
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script type=text/javascript src=/js/darkreader.js></script>
<script type=text/javascript src=/js/darkmode-darkreader.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script>
<script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>