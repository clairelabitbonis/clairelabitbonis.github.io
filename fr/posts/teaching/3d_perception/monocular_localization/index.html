<!doctype html><html><head><title>Localisation monoculaire par PnL itérative</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_2.png><meta property="og:title" content="Localisation monoculaire par PnL itérative"><meta property="og:description" content="Iterative estimation of a camera extrinsic parameters."><meta property="og:type" content="article"><meta property="og:url" content="http://clairelabitbonis.gitlab.io/fr/posts/teaching/3d_perception/monocular_localization/"><meta property="og:image" content="http://clairelabitbonis.gitlab.io/fr/posts/teaching/3d_perception/monocular_localization/featured.png"><meta property="article:published_time" content="2022-07-16T06:15:45+06:00"><meta property="article:modified_time" content="2022-07-16T06:15:45+06:00"><meta name=description content="Iterative estimation of a camera extrinsic parameters."><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/fr><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_2.png alt=Logo>
Claire Labit-Bonis</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-fr"></span>Français</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/posts/teaching/3d_perception/monocular_localization><span class="flag-icon flag-icon-gb"></span>English</a></div></li><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/fr/search><input type=text name=keyword placeholder=Chercher data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/fr/posts data-filter=all>Articles</a></li><div class=subtree><li><a href=/fr/posts/introduction/ title=Introduction>Introduction</a></li><li><i class="fas fa-minus-circle"></i><a class=active href=/fr/posts/teaching/>Enseignements</a><ul class=active><li><i class="fas fa-minus-circle"></i><a class=active href=/fr/posts/teaching/3d_perception/>Perception 3D</a><ul class=active><li><a class=active href=/fr/posts/teaching/3d_perception/monocular_localization/ title="Monocular localization">Monocular localization</a></li><li><a href=/fr/posts/teaching/3d_perception/cc_segmentation/ title="Segmentation avec Cloud Compare">Segmentation avec Cloud Compare</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/fr/posts/teaching/3d_perception/monocular_localization/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/default-avatar_hu9bf48a08e17db6472c2e32b73058a6bb_47653_120x120_fit_box_2.png alt="Author Image"><h5 class=author-name></h5><p>:date_full</p></div><div class=title><h1>Localisation monoculaire par PnL itérative</h1></div><div class=post-content id=post-content><p>+++
title = &ldquo;[3D Perception] Monocular localization - Pikachu&rdquo;
subtitle = &ldquo;Iterative estimation of a camera extrinsic parameters.&rdquo;</p><p>date = 2016-04-20T00:00:00
lastmod = 2021-10-13T00:00:00
draft = false
math = true
highlight = true
highlight_languages = &ldquo;python&rdquo;
comments = true</p><h1 id=authors-comma-separated-list-eg-bob-smith-david-jones>Authors. Comma separated list, e.g. <code>["Bob Smith", "David Jones"]</code>.</h1><p>authors = [&ldquo;Claire Labit-Bonis&rdquo;]</p><p>tags = [&ldquo;teaching&rdquo;, &ldquo;perception&rdquo;]
summary = &ldquo;Iterative estimation of a camera extrinsic parameters.&rdquo;</p><h1 id=featured-image>Featured image</h1><h1 id=to-use-add-an-image-named-featuredjpgpng-to-your-projects-folder>To use, add an image named <code>featured.jpg/png</code> to your project&rsquo;s folder.</h1><p>[image]</p><h1 id=caption-optional>Caption (optional)</h1><h1 id=caption--image-credit-unsplashhttpsunsplashcomphotoscpkojocxduy>caption = &ldquo;Image credit: <a href=https://unsplash.com/photos/CpkOjOcXdUY><strong>Unsplash</strong></a>&rdquo;</h1><h1 id=focal-point-optional>Focal point (optional)</h1><h1 id=options-smart-center-topleft-top-topright-left-right-bottomleft-bottom-bottomright>Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight</h1><p>focal_point = ""</p><h1 id=show-image-only-in-page-previews>Show image only in page previews?</h1><p>preview_only = true</p><p>#TODO: expliquer dmatPos*matPos, lambda, détailler le calcul de matrice intrinsic inverse et stockage des segments
#l1-l2. Pourquoi ne pas utiliser Ni.(P2i-P1i) plutôt que Ni.P1i + Ni.P2i ?
#Developpement de Taylor ordre 1 pour F(X) courant : tangente à la fonction en ce nouveau point.</p><h1 id=transpoint-changer-les-noms-des-variables--p_i--p-p_f--p_transformed->Transpoint, changer les noms des variables : P_i => P, P_f => P_transformed ?</h1><h1 id=modifier-les-matrices-moda-app3d-012-035-etc>Modifier les matrices modA, App3d, [0,1,2], [0,3,5] etc.</h1><h1 id=pourquoi-err2n--moyenne>Pourquoi err/2N ? Moyenne?</h1><p>+++</p><div class="alert alert-"><strong><p>This exercise consists in filling empty functions, or parts of the <code>localisation.py</code> file:</p><ul><li><a href=#anchor-step-1>step 1</a> : <code>perspective_projection</code> and <code>transform_and_draw_model</code>,</li><li><a href=#anchor-step-2>step 2</a> : <code>calculate_normal_vector</code> and <code>calculate_error</code></li><li><a href=#anchor-step-3>step 3</a> : <code>partial_derivatives</code>,</li><li><a href=#anchor-step-4>step 4</a> : transformation towards a second point of view.</li></ul><p>This post describes the role of each function.</p></strong></div><div class="alert alert-"><strong>Do not dive headfirst into the code ! The first part &ldquo;goal description&rdquo; is just a global and theoretical presentation of the subject. Each step and its associated functions are described in details within their respective parts: <a href=#anchor-step-1>step 1</a>, <a href=#anchor-step-2>step 2</a>, <a href=#anchor-step-3>step 3</a>, <a href=#anchor-step-4>step 4</a>.</strong></div><h2 id=anchor-step-0>Goal description</h2><p>This exercise aims at finding the optimal transformation between a 3D model expressed in centimeters in an <em>object</em> coordinate system $\mathcal{R_o} (X,Y,Z)$ (sometimes called <em>world</em> coordinate system $\mathcal{R_w}$) in order to draw it as an overlay in a 2D pixel image defined in its <em>image</em> coordinate system $\mathcal{R_i} (u,v)$. The 3D model was downloaded from <a href=https://free3d.com>free3d</a> and modified within <a href=https://www.blender.org/>Blender</a> for the needs of this exercise. Points and edges were respectively exported <em>via</em> a Python script to the <code>pikachu.xyz</code> and <code>pikachu.edges</code> files.</p><p><img src=images/blender_pikachu.png alt="3D model made in Blender" title="3D model made in Blender">
<code>pikachu.xyz</code> contains 134 lines and 3 columns, corresponding to the 134 model points and their three $x, y, z$ coordinates.</p><p><code>pikachu.edges</code> contains 246 lines and 2 columns, corresponding to the 246 model edges and their 2 ends indices (for instance, the first line in <code>pikachu.edges</code> describes the first edge: its first point coordinates are at index 2 in the <code>pikachu.xyz</code> file, and its second point is at index 0).
<img src=images/edges_xyz.png alt="Coordinates and edges files" title="Coordinates and edges files"></p><p>By &ldquo;transformation&rdquo;, we mean scene rotation and translation along the $x$, $y$ and $z$ axes for each point in $\mathcal{R_o}$.</p><p>In our case, we want to perform augmented reality by positioning Pikachu&rsquo;s model over the sky-blue cube of the image below, and making it perfectly match with the virtual cube on which it is standing.</p><p><img src=images/final_objective.png alt="Goal to be reached" title="Goal to be reached"></p><details><summary>**Intrinsic parameters.** <span style=color:red>*DON'T CLICK HERE*</span></summary><blockquote><p>You clicked&mldr; Now you can learn all about intrinsic parameters.</p></blockquote><p>We use the pinhole camera model allowing to perform this operation through two successive transformations: $\mathcal{R_o} \rightarrow \mathcal{R_c}$ and $\mathcal{R_c} \rightarrow \mathcal{R_i}$. Apart from $\mathcal{R_o}$ and $\mathcal{R_i}$ coordinate systems, we then must also considerate the camera frame $\mathcal{R_c}$.</p><div class="alert alert-"><strong>The resource <em><a href=http://www.optique-ingenieur.org/fr/cours/OPI_fr_M04_C01/co/Grain_OPI_fr_M04_C01_2.html>Modélisation et calibrage d&rsquo;une caméra</a></em> describes the pinhole camera model in details, and can help understanding the exercise.</strong></div><p><img src=images/goal.png alt="Coordinate systems transformation" title="Coordinate systems transformation"></p><blockquote><p>Wikipedia defines the <a href=https://en.wikipedia.org/wiki/Pinhole_camera_model>pinhole model</a> as describing <em>the mathematical relationship between the coordinates of a point in three-dimensional space and its projection onto the image plane of an ideal pinhole camera, where the camera aperture is described as a point and no lenses are used to focus light</em>.</p></blockquote><p>The change between $\mathcal{R_c}$ and $\mathcal{R_i}$ coordinate systems is done thanks to the <strong>intrinsic</strong> camera parameters. These $(\alpha_u, \alpha_v, u_0, v_0)$ coefficients are then stored in a homogeneous transformation matrix $K_{i \leftarrow c}$ so that we can describe the relation $p_i = K_{i \leftarrow c}.P_c$ as:</p><p>$$\begin{bmatrix}u\\v\\1\end{bmatrix}_{\mathcal{R}_i} = s. \begin{bmatrix}\alpha_u & 0 & u_0\\0 & \alpha
_v & v_0\\0 & 0 & 1\end{bmatrix} . \begin{bmatrix}X\\Y\\Z\end{bmatrix}_{\mathcal{R
}_c}$$</p><p>with:</p><ul><li>$p_i$ (on the left) the pixel point within the 2D image frame $\mathcal{R_i}$,</li><li>$P_c$ (on the right) the point in centimeters within the 3D camera frame $\mathcal{R_c}$,</li><li>$s = \frac{1}{Z}$,</li><li>$\alpha_u = k_x f$, $\alpha_v = k_y f$ :<ul><li>$k_x = k_y$ the sensor number of pixels per millimeter along $x$ and $y$ directions &ndash; the equality being true only if the pixels are square,</li><li>$f$ the focal distance.</li></ul></li><li>$u_0$ and $v_0$ the image centers in pixels within $\mathcal{R}_i$.</li></ul><p>In our case, the image was captured by a Canon EOS 700D sensor of size $22.3\times14.9 mm$. The image size being $740\times480 px$ and the focal distance $18mm$, we deduce the parameters $\alpha_u = 579.8664$, $\alpha_v = 581.166$, $u_0 = 360$ and $v_0 = 240$ stored in the <code>calibration_parameters.txt</code> file.</p></details><details><summary>**Extrinsic parameters.** <span style=color:red>*DON'T CLICK HERE NEITHER*</span></summary><blockquote><p>Happy extra reading ;)</p></blockquote><p>To reach our goal <em>i.e.</em>, display our 3D model in the 2D image in pixels, we have to estimate the $\mathcal{R_o}\rightarrow\mathcal{R_c}$ transformation coefficients and establish the relation $P_c=M_{c\leftarrow o}.P_o$, with:</p><ul><li><p>$M_{c\leftarrow o}=\big[ R_{\alpha\beta\gamma} | T \big]$ the homogeneous transformation matrix of the Euler angles and $x$, $y$ and $z$ translation of the coordinate system.</p></li><li><p>$R_{\alpha\beta\gamma}$ the rotation matrix resulting from the successive applications &ndash; and thus the multiplication between them &ndash; of the rotation matrices $R_\gamma$, $R_\beta$ and $R_\alpha$:</p></li></ul><p>$$R_\alpha = \begin{bmatrix}1 & 0 & 0\\0 & \cos \alpha & -\sin \alpha\\0 & \sin \alpha & \cos \alpha\end{bmatrix}$$</p><p>$$R_\beta = \begin{bmatrix}\cos \beta & 0 & -\sin \beta\\0 & 1 & 0\\\sin \beta & 0 & \cos \beta\end{bmatrix}$$</p><p>$$R_\gamma = \begin{bmatrix}\cos \gamma & -\sin \gamma & 0\\\sin \gamma & \cos \gamma & 0\\0 & 0 & 1\end{bmatrix}$$</p><p>With the correct extrinsic parameters $(\alpha, \beta, \gamma, t_x, t_y, t_z)$, the 3D model in its coordinate system $\mathcal{R_o}$ can be transformed into $\mathcal{R_c}$ then $\mathcal{R_i}$, while respecting the following relationship for each point $P_o$:</p><p>$$ p_i = K_{i \leftarrow c} M_{c\leftarrow o} P_o$$</p><p>$$\begin{bmatrix}u\\v\\1\end{bmatrix}_{\mathcal{R}_i} = s. \begin{bmatrix}\alpha_u & 0 & u_0\\0 & \alpha
_v & v_0\\0 & 0 & 1\end{bmatrix} \bigodot \begin{bmatrix}r_{11} & r_{12} & r_{13} & t_x\\r_{21} & r_{22
} & r_{23} & t_y\\r_{31} & r_{32} & r_{33} & t_z\\0 & 0 & 0 & 1\end{bmatrix} . \begin{bmatrix}X\\Y\\Z\\1\end{bmatrix}_{\mathcal{R}_o}$$</p><blockquote><p>The $\bigodot$ operator is the multiplication between terms after having removed the homogeneous coordinate from $M_{c\leftarrow o} P_o$. It is only here so that matrices shapes match.</p></blockquote><div class="alert alert-"><strong>The instrinsic parameters are known; in order to find the extrinsic parameters, we use a localization method (<em>i.e.</em> of parameters estimation) called <strong>Perspective-n-Lines</strong>.</strong></div></details><p><strong>Extrinsic parameters estimation.</strong></p><p>Our starting point is an initial coarse estimate of $(\alpha, \beta, \gamma, t_x, t_y, t_z)$. This initial estimate is more or less accurate depending on our <em>a priori</em> knowledge of the scene, the object and the position of the camera.</p><p>In our case, the initial parameters have values $alpha = -2.1$, $beta = 0.7$, $gamma = 2.7$, $t_x = 3.1$, $t_y = 1.3$ and $t_z = 18$, <em>i.e.,</em> we know before we even start that the 3D model will have to undergo a rotation of angles $(-2.1, 0.7, 2.7)$ around its three axes, and that it should shift approximately $3cm$ in $x$, $1.5cm$ in $y$ and $18cm$ in $z$.</p><div class="alert alert-"><strong>This <em>a priori</em> knowledge of the environment comes from the fact that one is able, as a human being, to evaluate the distance of the real object from the sensor only by looking at the image.</strong></div><p><strong>Use of the programme.</strong></p><p>When the script <code>localisation.py</code> is launched, two figures open: one represents a real scene captured by the camera, the other represents the virtual model to be transformed and whose system origin is materialised by a red dot. As the objective is to copy the box on which the Pikachu rests on top of the sky-blue cube on the table, you must first select five edges belonging to the real box (by <em>clicking</em> the mouse on the edges ends), then select the corresponding edges in the same order on the 3D model (by <em>clicking</em> the mouse in the middle of the edges). The number of segments to be selected (by default 5), is fixed from the start in the variable <code>nb_segments</code>.</p><p><img src=images/edge_selection.png alt="Edge selection in the image and in the 3D model" title="Edge selection in the image and in the 3D model"></p><p>In this way, it will be possible to calculate the error on the estimation of extrinsic parameters by comparing the distance between the edges selected in the image and those of the transformed model. This error criterion is described at <a href=#anchor-step-2>step 2</a>.</p><h2 id=anchor-step-1>Step 1: Display the pattern in $\mathcal{R_i}$</h2><p>In the main program, the points of the model are stored in <code>model3d_Ro</code>, a matrix of size $[246\times6]$ corresponding to the 246 edges of the model, each defined by the 6 coordinates of its two points $P_1(x_1, y_1, z_1)$ and $P_2(x_2, y_2, z_2)$. The transformation matrix $M_{c\leftarrow o}$ is stored in <code>extrinsic_matrix</code> and the intrinsic parameters of the camera are stored in <code>intrinsic_matrix</code>.</p><p>To visualise the 3D model in the 2D image and get an idea of the accuracy of our estimate, we have to code the <code>tranform_and_draw_model</code> function which allows to apply the $K_{i \leftarrow c} M_{c\leftarrow o}$ transformation at each point $P_o$ of a set of edges <code>edges_Ro</code>, with an <code>intrinsic</code> matrix, and an <code>extrinsic</code> matrix to finally display the result in a <code>fig_axis</code> figure:</p><pre><code>def transform_and_draw_model(edges_Ro, intrinsic, extrinsic, fig_axis):
    # ********************************************************************* #
    # TO BE COMPLETED.                                                      #
    # FUNCTIONS YOU WILL HAVE TO USE :                                      #
    #   - perspective_projection                                            #  
    #   - transform_point_with_matrix                                       #
    # Input:                                                                #
    #   edges_Ro : ndarray[Nx6]                                             #
    #             N = number of edges in the model                          #
    #             6 = (X1, Y1, Z1, X2, Y2, Z2) the P1 and P2 point          #
    #                 coordinates for each edge                             #
    #   intrinsic : ndarray[3x3] - camera intrinsic parameters              #
    #   extrinsic : ndarray[4x4] - camera extrinsic parameters              #
    #   fig_axis : figure used for display                                  #
    # Output:                                                               #
    #   No return - the function only calculates and displays the           #
    #   transformed points (u1, v1) and (u2, v2)                            #
    # ********************************************************************* #

    # Part to replace #
    u_1 = np.zeros((edges_Ro.shape[0],1))
    u_2 = np.zeros((edges_Ro.shape[0],1))
    v_1 = np.zeros((edges_Ro.shape[0],1))
    v_2 = np.zeros((edges_Ro.shape[0],1))
    ############### 
    
    for p in range(edges_Ro.shape[0]):
        fig_axis.plot([u_1[p], u_2[p]], [v_1[p], v_2[p]], 'k')
</code></pre><p>The objective is to store in the points <code>[u_1, v_1]</code> and <code>[u_2, v_2]</code> the coordinates $(u, v)$ of the points $P_1$ and $P_2$ of the edges after transformation.</p><p>This function can be divided into two sub-steps:</p><ul><li><p>the transformation $\mathcal{R_o} \rightarrow \mathcal{R_c}$ of the points $P_o$ using the function <code>transform_point_with_matrix</code> provided in the <code>matTools</code> library:</p><pre><code>  P_c = matTools.transform_point_with_matrix(extrinsic, P_o)
</code></pre></li><li><p>the projection $\mathcal{R_c} \rightarrow \mathcal{R_i}$ of the newly obtained $P_c$ points using the function <code>perspective_projection</code> which must be completed :</p><pre><code>  def perspective_projection(intrinsic, P_c):
      # ***************************************************** #
      # TO BE COMPLETED.                                      #
      # Useful function:                                      #
      #   np.dot                                              #
      # Input:                                                #
      #   intrinsic : ndarray[3x3] - intrinsic parameters     #
      #   P_c : ndarray[Nx3],                                 #
      #         N = number of points to transform             #
      #         3 = (X, Y, Z) the points coordinates          #
      # Output:                                               #
      #   u, v : two ndarray[N] containing the Ri coordinates #
      #          of the transformed Pc points                 #
      # ***************************************************** #
        
      u, v = 0, 0 # Part to replace
        
      return u, v
</code></pre></li></ul><p>Once <code>perspective_projection</code> and <code>transform_and_draw_model</code> have been completed, the launch of the overall programme will project the model in the image, with the extrinsic parameters defined at the outset.</p><p><img src=images/first_rough_estimate.png alt="First projection of the model in the image" title="First projection of the model in the image"></p><blockquote><p>The transformation applied to these points is obviously not very good, the model does not match the box as one would wish. In order to be able to differentiate a &ldquo;bad&rdquo; estimate of the extrinsic parameters from a &ldquo;good&rdquo; one, and thus be able to automatically propose a new estimate closer to our objective, we must determine an error criterion characterising the distance we are from this optimal objective.</p></blockquote><h2 id=anchor-step-2>Step 2: Determine an error criterion</h2><p>The error criterion is used to assess the accuracy of our estimate. The greater the error, the poorer our extrinsic parameters. Once we have determined this error criterion, we can integrate it into an optimisation loop aimed at minimising it, and thus have the best possible extrinsic parameters for our 2D/3D matching objective.</p><p>As an example, the figure below illustrates this optimisation loop for the projection of the virtual box on the real cube. At iteration $0$, the parameters are coarse, the error is large. By changing the parameters the error will be reduced until ideally zero error and optimal transformation for a perfect projection of the 3D model into the 2D image is achieved.</p><p><img src=images/extrinsic_optim.gif alt="Optimisation of extrinsic parameters" title="Optimisation of extrinsic parameters"></p><p>In 2D/3D segment matching, the error criterion to be minimised is the scalar product of the normal to the interpretation plane for the matching. In other words, the objective is to transform the points of the model so that the segments selected in the image and those selected in the model belong to the same plane expressed in the camera frame $\mathcal{R_c}$.</p><p><img src=images/interpretation_plan.png alt="Interpretation plan relating to line matching" title="Interpretation plan relating to line matching"></p><p>As shown in the figure above, the <em>interpretation plane</em> can be defined as being formed by the segments $\mathcal{l_{i \rightarrow c}^{j,1}}$ and $\mathcal{l_{i \rightarrow c}^{j,2}}$. In this notation, $j$ corresponds to the number of edges selected when the program is launched (5 by default). For each selected edge $j$ and expressed in the <em>image</em> frame $\mathcal{R_i}$, there are two segments $l^1$ and $l^2$. The $l^1$ segment is composed of the two ends $P_{i \rightarrow c}^0$ and $P_{i \rightarrow c}^1$; the $l^2$ segment is composed of the two ends $P_{i \rightarrow c}^1$ and $P_{i \rightarrow c}^2$. Each of the $P_{i \rightarrow c}$ is a point in the <em>image</em> frame $\mathcal{R_i}$ transformed into the <em>camera</em> frame $\mathcal{R_c}$. They are known: they are the ones that have been selected by mouse click.</p><p>Once these segments have been calculated, we can calculate the normal to the interpretation plane $N_c^j$. As a reminder:</p><p>$$N = \frac{l^1 \wedge l^2}{||l^1 \wedge l^2||}$$</p><p>In the segment selection function <code>utils.select_segments()</code>, as you make edge selections, the normals are calculated and stored in the <code>normal_vectors</code> matrix thanks to the <code>calculate_normal_vector</code> function which must be completed in the <code>localisation.py</code> file:</p><pre><code>def calculate_normal_vector(p1_Ri, p2_Ri, intrinsic):
    # ********************************************************* #
    # TO BE COMPLETED.                                          #
    # Useful functions:                                         #
    #   np.dot, np.cross, np.linalg.norm, np.linalg.inv         #
    # Input:                                                    #
    #   p1_Ri : list[3]                                         #
    #           3 = (u, v, 1) of the first selected point       #
    #   p2_Ri : list[3]                                         #
    #           3 = (u, v, 1) of the second selected point      #
    #   intrinsic : ndarray[3x3] of intrinsic                   #
    # Output:                                                   #
    #   normal_vector : ndarray[3] containing the normal to     #
    #                   L1_c and L2_c segments, deducted from   #
    #                   the selected image points               #
    # ********************************************************* #

    normal_vector = np.zeros((len(p1_Ri),)) # Part to replace

    return normal_vector
</code></pre><p>Then, for each segment $j \in [1, &mldr;, 5]$, the distance between the plane linked to the image segments and the points of the 3D model can be calculated using the scalar product of the $N_c^j$ and the $P_{o\rightarrow c}^j$. If the scalar product is null, both vectors are orthogonal and the point $P_{o\rightarrow c}^j$ belongs to the same plane as $P_{i \rightarrow c}^j$. The larger the scalar product, the further the extrinsic parameters move away from the correct transformation.</p><p>To summarize, the parameter optimization criterion is $\sum_{j=1}^{2n} F^j(X)^2$ (the square removes negative values), with $F^j(X) = N^{j ; \text{mod} ; 2}.P_
{o\rightarrow c}^{j ; \text{mod} ; 2, 1|2}$, depending on whether one is on the point $P^{1}$ or $P^{2}$.</p><blockquote><p>In the sum that runs through the points from $j$ to $2n$, $j ; \text{mod} ; 2$ is the segment index for the current point. In other words, we cumulate the $(N^i.P^{i, 1})^2$ and $(N^i.P^{i, 2})^2$ for all $i$ segments.</p></blockquote><div class="alert alert-"><strong>Here, $X$ designates the set of parameters $(\alpha, \beta, \gamma, t_x, t_y, t_z)$ and not a coordinate.</strong></div><p>Remember that every $P_{o\rightarrow c} = \big[ R_{\alpha\beta\gamma} | T \big]. P_o$ ; the value of the error thus actually depends on the value of the extrinsic parameters. Each time the optimization loop is run, changing the weights of these parameters will influence the $F(X)$ criterion.</p><p>The function <code>calculate_error</code> calculates the error criterion. It takes as input the number of selected edges <code>nb_segments</code>, the <code>normal_vectors</code>, and the <code>Rc_segments</code> selected and then transformed by the matrix of extrinsic parameters and expressed in $\mathcal{R_c}$.</p><pre><code>def calculate_error(nb_segments, normal_vectors, segments_Rc):
    # ***************************************************************** #
    # TO BE COMPLETED.                                                  #
    # Input:                                                            #
    #   nb_segments : default 5 = number of selected segments           #
    #   normal_vectors : ndarray[Nx3] - normal vectors to the           #
    #                    interpretation plane of selected segments      #
    #                    N = number of segments                         #
    #                    3 = (X,Y,Z) normal coordinates in Rc           #
    #   segments_Rc : ndarray[Nx6] = selected segments in Ro            #
    #                 and transformed in Rc                             #
    #                 N = number of segments                            #
    #                 6 = (X1, Y1, Z1, X2, Y2, Z2) of points P1 and     #
    #                 P2 of the transformed edges in Rc                 #
    # Output:                                                           #
    #   err : float64 - cumulated error of observed vs. expected dist.  #
    # ***************************************************************** #
    err = 0
    for p in range(nb_segments):
        # Part to replace with the error calculation
        err = err + 0
    
    err = np.sqrt(err / 2 * nb_segments)
    return err
</code></pre><blockquote><p>From a mathematical point of view, the sum is written for $j$ ranging from $1$ to $2n$, <em>i.e.,</em> the number of points. From an implementation point of view and given the construction of our variables, it is simpler to express the criterion through a sum running along the edges: $\sum_{j=1}^{n} F^j(X)$ with $F^{j}(X) = F^{j, 1}(X)^2 + F^{j, 2}(X)^2$.</p></blockquote><blockquote><p>Thus, $F^{j, 1}(X) = N^j.P_{o\rightarrow c}^{j, 1}$ and $F^{j, 2}(X) = N^j.P_{o\rightarrow c}^{j, 2}$.</p></blockquote><h2 id=anchor-step-3>Step 3: Estimation of parameters by ordinary least squares method</h2><p>At each iteration $k$, we look for a set of parameters $X_{k}(\alpha, \beta, \gamma, t_x, t_y, t_z)$ such as the criterion $F(X)$ is equal to the criterion for the previous parameter set $X_0$ (before update) incremented by a delta weighted by the Jacobian of the function.</p><p>We thus seek to solve a system of the form :</p><p>$$F(X) \approx F(X_0) + J \Delta X$$</p><div class="alert alert-"><strong>The Jacobian $J$ contains on its lines the partial derivatives of the $F$ function for each selected point and according to each of the $X$ parameters. It reflects the trend of the criterion (ascending/descending) and the speed at which it increases or decreases according to the value of each of its parameters.</strong></div><p>Since our objective is to reach a criterion $F(X) = 0$, we translate the problem to be solved by :</p><p>$$\begin{align}
0 &= F(X_0) + J \Delta X\\<br>\llap{\Leftrightarrow \qquad} -F(X_0) &= J \Delta X
\end{align}$$</p><p>The advantage of working with successive increments is that the increment values are so small in relation to the last iteration that the variation of these parameters can be approximated as 0. As a reminder, the error criterion is expressed as follows for each segment $j$ :</p><p>$$
\begin{align}
F^{j, 1}(X) &= N^j.P_{o\rightarrow c}^{j, 1} \text{ avec } P_{o\rightarrow c}^{j, 1} = \big[ R_{\alpha\beta\gamma
} | T \big] . P_o^{j, 1}\\\<br>F^{j, 2}(X) &= N^{j}.P_{o\rightarrow c}^{j, 2} \text{ avec } P_{o\rightarrow c}^{j, 2} = \big[ R_{\alpha\beta\gamma
} | T \big] . P_o^{j, 2}
\end{align}$$</p><p>Because of always having $\Delta X \approx 0$, the calculation of the Jacobian matrix is considerably simplified, since the partial derivatives $(\frac{\partial F^j}{\partial \alpha}, \frac{\partial F^j}{\partial \beta}, \frac{\partial F^j}{\partial \gamma}, \frac{\partial F^j}{\partial t_x}, \frac{\partial F^j}{\partial t_y}, \frac{\partial F^j}{\partial t_z})$ are the same at each iteration.</p><p>The detail of the derivation is given for the first parameter $\alpha$, the following are to be demonstrated :</p><p>$$\begin{align}
\frac{\partial F^j}{\partial \alpha} &= N^j . [R_\gamma . R_\beta . \frac{\partial R_\alpha}{\partial \alpha} | T] . P_o^j\\\<br>\end{align}$$</p><div class="alert alert-"><strong><p>$R_\gamma$ and $R_\beta$ disappear from the derivation because for $\gamma \approx 0$ and $\beta \approx 0$, we have :</p><p>$$
R_{\gamma \approx 0} = \begin{bmatrix}
\cos 0 & -\sin 0 & 0\\\
\sin 0 & \cos 0 & 0\\\
0 & 0 & 1 \end{bmatrix} = \begin{bmatrix}
1 & 0 & 0\\\
0 & 1 & 0\\\
0 & 0 & 1 \end{bmatrix} = I_3
$$</p><p>$$
R_{\beta \approx 0} = \begin{bmatrix}
\cos 0 & 0 & -\sin 0\\\<br>0 & 1 & 0\\\<br>\sin 0 & 0 & \cos 0 \end{bmatrix} = \begin{bmatrix}
1 & 0 & 0\\\
0 & 1 & 0\\\
0 & 0 & 1 \end{bmatrix} = I_3
$$</p><p>$T$ disappears since $t_x$, $t_y$, $t_z \approx 0$.</p><p><strong>Reminder of derivation rules :</strong> $(\text{constant } a)' \rightarrow 0 \text{, } (\sin)' \rightarrow \cos \text{, } (\cos)' \rightarrow -\sin$.</p></strong></div><p>$$\begin{align}
\frac{\partial F^j}{\partial \alpha} &= N^j . [I_3 . I_3 . \frac{\partial R_\alpha}{\partial \alpha} | 0] .
P_o^j\\\<br>&= N^j . \frac{\partial \begin{bmatrix}1 & 0 & 0\\\ 0 & \cos (\alpha \approx 0) & -\sin (\alpha \approx 0)\\\ 0 & \sin (\alpha \approx 0) & \cos (\alpha \approx 0) \end{bmatrix}}{\partial \alpha} . \begin{bmatrix}X^j\\\ Y^j\\\ Z^j\end{bmatrix}_o\\\<br>&= N^j . \begin{bmatrix}0 & 0 & 0\\\ 0 & -\sin (\alpha \approx 0) & -\cos (\alpha \approx 0)\\\ 0 & \cos (\alpha \approx 0) & -\sin (\alpha \approx 0) \end{bmatrix} . \begin{bmatrix}X^j\\\ Y^j\\\ Z^j\end{bmatrix}_o\\\<br>&= N^j . \begin{bmatrix}0 & 0 & 0\\\ 0 & 0 & -1\\\ 0 & 1 & 0 \end{bmatrix} . \begin{bmatrix}X^j\\\ Y^j\\\ Z^j\end{bmatrix}_o\\\<br>&= N^j . \begin{bmatrix}0\\\ -Z^j\\\ Y^j\end{bmatrix}_o\\\<br>\end{align}$$</p><p>The reasoning is the same for $\frac{\partial F^j}{\partial \beta}, \frac{\partial F^j}{\partial \gamma}, \frac{\partial F^j}{\partial t_x}, \frac{\partial F^j}{\partial t_y}, \frac{\partial F^j}{\partial t_z}$ and we get :</p><p>$$
\frac{\partial F^j}{\partial \beta} = N^j . \begin{bmatrix}Z^j\\\ 0\\\ -X^j\end{bmatrix}_o \text{, }
\frac{\partial F^j}{\partial \gamma} = N^j . \begin{bmatrix}-Y^j\\\ X^j\\\ 0\end{bmatrix}_o
$$
$$
\frac{\partial F^j}{\partial t_x} = N_x^j \text{, }
\frac{\partial F^j}{\partial t_y} = N_y^j \text{, }
\frac{\partial F^j}{\partial t_z} = N_z^j
$$</p><p>Each of these partial derivatives is to be implemented in the <code>partial_derivatives</code> function:</p><pre><code>def partial_derivatives(normal_vector, P_c):
    # ********************************************************************* #
    # TO BE COMPLETED.                                                      #
    # Input:                                                                #
    #   normal_vector : ndarray[3] contains the normal of the segment       #
    #                   to which P_c belongs                                #
    #   P_c : ndarray[3] the object point transformed to Rc                 #
    # Output:                                                               #
    #   partial_derivative : ndarray[6] partial derivative of the criterion #
    #                        for each extrinsic parameter                   #
    #   crit_X0 : float64 - criterion value for the current parameters,     #
    #                       which will be used as initial value before      #
    #                       the update and recalculation of the error       #
    # ********************************************************************* #
    X, Y, Z = P_c[0], P_c[1], P_c[2]
    partial_derivative = np.zeros((6))
    
    # Part to replace #######
    partial_derivative[0] = 0
    partial_derivative[1] = 0
    partial_derivative[2] = 0
    partial_derivative[3] = 0
    partial_derivative[4] = 0
    partial_derivative[5] = 0
    #########################

    # ********************************************************************
    crit_X0 = normal_vector[0] * X + normal_vector[1] * Y + normal_vector[2] * Z
    return partial_derivative, crit_X0
</code></pre><p>The problem can thus be formalised as :</p><p>$$
F = J \Delta X \text{ with } F = \begin{bmatrix}-F^1(X_0) \\\ \vdots \\\ -F^{2n}(X_0)\end{bmatrix}_{2n\times 1}
$$
$$
\text{ and } J = \begin{bmatrix}\frac{\partial F^1}{\partial \alpha} & \frac{\partial F^1}{\partial \beta} & \frac{\partial F^1}{\partial \gamma} & \frac{\partial F^1}{\partial t_x} & \frac{\partial F^1}{\partial t_y} & \frac{\partial F^1}{\partial t_z}\\\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\\ \frac{\partial F^{2n}}{\partial \alpha} & \frac{\partial F^{2n}}{\partial \beta} & \frac{\partial F^{2n}}{\partial \gamma} & \frac{\partial F^{2n}}{\partial t_x} & \frac{\partial F^{2n}}{\partial t_y} & \frac{\partial F^{2n}}{\partial t_z}\end{bmatrix}_{2n\times 6}
$$
$$
\text{ and } \Delta X = \begin{bmatrix}\Delta \alpha \\\ \vdots \\\ \Delta t_z\end{bmatrix}_{6 \times 1}
$$</p><div class="alert alert-"><strong>$2n$ is the number of selected points, $n$ is the number of edges (one edge = two ends).</strong></div><p>$F$ is known, $J$ is known, all that remains is to estimate $\Delta X$ so that the equality $F = J \Delta X$ is &ldquo;as true as possible&rdquo;. This is done by minimising the distance between the two sides of equality:</p><p>$$
\begin{align}
\min_{\Delta X} ||F - J\Delta X||^2 & \\\<br>\llap{\Leftrightarrow \qquad} \frac{\partial ||F - J\Delta X||^2}{\partial \Delta X} &= 0
\end{align}
$$</p><blockquote><p>Indeed, if the derivative of the function to be minimized is 0, then the curve of the function has definitely reached a minimum.</p></blockquote><div class="alert alert-"><strong><p><strong>Reminder of matrices operations:</strong></p><p>$(A+B)^T = A^T + B^T$</p><p>$(AB)^T = B^T A^T$</p><p>$A\times B \neq B\times A$</p><p>$A^2 = A^T A$</p></strong></div><p>By developing $(F - J \Delta X)^2$, we arrive at :</p><p>$$
\begin{align}
(F - J \Delta X)^2 & = (F - J \Delta X)^T(F - J \Delta X)\\\<br>&= (F^T - \Delta X^T J^T)(F - J \Delta X)\\\<br>&= F^TF - F^TJ\Delta X - \Delta X^T J^T F + \Delta X^T J^T J \Delta X
\end{align}
$$</p><p>We use matrices properties to show $F^TJ\Delta X = ((J\Delta X)^T F)^T = (\Delta X^T J^T F)^T$. Let&rsquo;s consider matrices shapes for both sides of this equality:
$$
\begin{align}
F^TJ\Delta X &\rightarrow [1\times2n][2n\times 6][6\times 1] \rightarrow [1\times 1]\\\<br>(\Delta X^T J^T F)^T &\rightarrow [1\times6][6\times 2n][2n\times 1] \rightarrow [1\times 1]
\end{align}
$$</p><p>Given that each of these terms is a $[1\times 1]$ matrix in the end, we can write $(\Delta X^T J^T F)^T = \Delta X^T J^T F$, and thus $F^TJ\Delta X = \Delta X^T J^T F$.</p><p>We deduce from this:
$$
\begin{align}
(F - J \Delta X)^2 & = F^TF - 2\Delta X^T J^T F + \Delta X^T J^T J \Delta X\\\<br>\end{align}
$$</p><div class="alert alert-"><strong><p><strong>Some rules of derivation:</strong></p><p>$\frac{\partial AX}{\partial X} = A^T$, $\frac{\partial X^TA^T}{\partial X} = A^T$, $\frac{\partial X^TAX}{\partial X} = 2AX$.</p></strong></div><p>$$
\begin{align}
\frac{\partial (F - J \Delta X)^2}{\partial \Delta X} & = -2J^T F + 2 J^T J \Delta X\\\<br>&= -J^T F + J^T J \Delta X\\\<br>\llap{\Leftrightarrow \qquad} J^T F &= J^T J \Delta X \\\<br>(J^TJ)^{-1} J^T F &= \Delta X \\\<br>\end{align}
$$</p><p><strong>In summary:</strong></p><ul><li>minimizing the distance between $F$ and $J\Delta X$ means that $\frac{\partial (F - J \Delta X)^2}{\partial \Delta X} = 0$</li><li>the solution is $\Delta X = (J^TJ)^{-1} J^T F$.</li></ul><div class="alert alert-"><strong>The matrix $J^+ = (J^TJ)^{-1} J^T$ is called the <em>pseudo-inverse</em> of $J$.</strong></div><p>In the Python code, we can thus implement the parameter update in the optimization loop. $\Delta X$ corresponds to the variable named <code>delta_solution</code>. We can then pass <code>delta_solution</code> to the function <code>matTools.construct_matrix_from_vec</code> which returns a matrix of $X$ increments in the same form as <code>extrinsic</code> (the extrinsic parameter matrix).</p><p>Each element of <code>extrinsic</code> is incremented by multiplying it by <code>delta_extrinsic</code>.</p><pre><code># ********************************************************************* #
# TO BE COMPLETED.                                                      #
# delta_solution = ...                                                  #
# delta_extrinsic = matTools.construct_matrix_from_vec(delta_solution)  #
# extrinsic = ...                                                       #
# ********************************************************************* #
</code></pre><p>Once the optimisation loop is operational, the model transformation can be visualised without the virtual box by removing the first 12 points of <code>model3D_Ro</code> :</p><p><img src=images/final_objective_without_box.png alt="Transformation of the model after estimation of the camera pose" title="Transformation of the model after estimation of the camera pose"></p><h2 id=anchor-step-4>Step 4 : projection of the estimated pose on the image taken from a different point of view</h2><p>A second photo has been captured from a different point of view, and the passage matrix between the first and second views is stored and loaded at the beginning of the programme from the corresponding <code>.txt</code> file. This allows the model to be re-projected into the image from the second camera, and the result to be displayed:</p><pre><code>fig5 = plt.figure(5)
ax5 = fig5.add_subplot(111)
ax5.set_xlim(0.720)
ax5.set_ylim(480)
plt.imshow(image_2)

# To be completed with the passage matrix from Ro to Rc2
# Ro -&gt; Rc and Rc -&gt; Rc2
Ro_to_Rc2 = ...

transform_and_draw_model(model3D_Ro[12:], intrinsic_matrix, Ro_to_Rc2, ax5)
plt.show(block = False)
</code></pre><p><img src=images/left_to_right.png alt="Projection in the second image" title="Projection in the second image"></p><h2 id=bonus>Bonus for the end</h2><p>Thanks to the estimated extrinsic parameter matrix, and as long as the origin of the object reference does not change, elements can be added to the 3D scene and projected into the image in an identical way. <code>model3D_Ro_final</code> contains the points of a scene with Pikachu and a dinosaur in it.</p><p><img src=images/pika_dino_blender.png alt="Adding elements to the 3D scene" title="Adding elements to the 3D scene"></p><p>The last lines of the display part are uncommented to get the final result:</p><pre><code>fig6 = plt.figure(6)
ax6, lines = utils.plot_3d_model(model3D_Ro_final, fig6)

fig7 = plt.figure(7)
ax7 = fig7.add_subplot(111)
ax7.set_xlim(0, 720)
plt.imshow(image)
transform_and_draw_model(model3D_Ro_final[12:], intrinsic_matrix, extrinsic_matrix, ax7)
plt.show(block=True)
</code></pre><p><img src=images/pika_dino.png alt="New projected scene" title="New projected scene"></p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/clairelabitbonis/clairelabitbonis.github.io/edit/main/content/posts/teaching/3d_perception/monocular_localization/index.fr.md title="Améliorez cette page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>Améliorez cette page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/fr/posts/introduction/ title=Introduction class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>Précédent</div><div class=next-prev-text>Introduction</div></a></div><div class="col-md-6 next-article"><a href=/fr/posts/teaching/3d_perception/cc_segmentation/ title="Segmentation de nuages de points 3D par capteur à lumière structurée RGB-D" class="btn btn-outline-info"><div>Suivant <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Segmentation de nuages de points 3D par capteur à lumière structurée RGB-D</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a><div class="dropdown languageSelector"><a class="btn dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-fr"></span>Français</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/posts/teaching/3d_perception/monocular_localization><span class="flag-icon flag-icon-gb"></span>English</a></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table des matières</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#anchor-step-0>Goal description</a></li><li><a href=#anchor-step-1>Step 1: Display the pattern in $\mathcal{R_i}$</a></li><li><a href=#anchor-step-2>Step 2: Determine an error criterion</a></li><li><a href=#anchor-step-3>Step 3: Estimation of parameters by ordinary least squares method</a></li><li><a href=#anchor-step-4>Step 4 : projection of the estimated pose on the image taken from a different point of view</a></li><li><a href=#bonus>Bonus for the end</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Alimenté par
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script><script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body);>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"\\[",right:"\\]",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false}]});</script></body></html>