<!doctype html><html><head><title>Segmentation de nuages de points 3D par capteur à lumière structurée RGB-D avec CloudCompare</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_2.png><meta property="og:title" content="Segmentation de nuages de points 3D par capteur à lumière structurée RGB-D avec CloudCompare"><meta property="og:description" content="Contrôle qualité d'objets 3D."><meta property="og:type" content="article"><meta property="og:url" content="http://clairelabitbonis.gitlab.io/fr/posts/teaching/3d_perception/cc_segmentation/"><meta property="og:image" content="http://clairelabitbonis.gitlab.io/fr/posts/teaching/3d_perception/cc_segmentation/featured.png"><meta property="article:published_time" content="2022-07-16T00:00:00+00:00"><meta property="article:modified_time" content="2022-07-16T00:00:00+00:00"><meta name=description content="Contrôle qualité d'objets 3D."><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/fr><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_2.png alt=Logo>
Claire Labit-Bonis</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-fr"></span>Français</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/posts/teaching/3d_perception/cc_segmentation><span class="flag-icon flag-icon-gb"></span>English</a></div></li><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu0a17c6d8ab77c1c9df3e22d1e0f9b13d_86416_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu2230ff0c1688347dd01f9a568f627fa9_80011_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/fr/search><input type=text name=keyword placeholder=Chercher data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/fr/posts data-filter=all>Articles</a></li><div class=subtree><li><a href=/fr/posts/introduction/ title=Introduction>Introduction</a></li><li><i class="fas fa-minus-circle"></i><a class=active href=/fr/posts/teaching/>Enseignements</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/fr/posts/teaching/deep_learning_for_cv/>Deep Learning & CV</a><ul><li><a href=/fr/posts/teaching/deep_learning_for_cv/overview_dlcv/ title="Aperçu général">Aperçu général</a></li><li><a href=/fr/posts/teaching/deep_learning_for_cv/course/ title=Cours>Cours</a></li><li><a href=/fr/posts/teaching/deep_learning_for_cv/practical_sessions/ title="Travaux pratiques">Travaux pratiques</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/fr/posts/teaching/3d_perception/>Perception 3D</a><ul class=active><li><a href=/fr/posts/teaching/3d_perception/overview_perception/ title="Aperçu général">Aperçu général</a></li><li><a class=active href=/fr/posts/teaching/3d_perception/cc_segmentation/ title=Segmentation>Segmentation</a></li><li><a href=/fr/posts/teaching/3d_perception/monocular_localization/ title="Localisation mono.">Localisation mono.</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/fr/posts/teaching/3d_perception/cc_segmentation/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/default-avatar_hu9bf48a08e17db6472c2e32b73058a6bb_47653_120x120_fit_box_2.png alt="Author Image"><h5 class=author-name></h5><p>:date_full</p></div><div class=title><h1>Segmentation de nuages de points 3D par capteur à lumière structurée RGB-D avec CloudCompare</h1></div><div class=post-content id=post-content><h2 id=anchor-step-0>Définition des objectifs</h2><p>Ce TP vise à effectuer une segmentation 3D et un contrôle de forme sur des objets polyédriques, c&rsquo;est-à-dire à vérifier si ces objets sont corrects par rapport à un modèle géométrique de référence et/ou présentent des défauts (trous, résidus, etc.).</p><p><img src=images/objets_a_comparer.png alt="Objets à comparer" class=center><div style=margin-top:1rem></div></p><p>Pour cela, il faut au préalable construire ce modèle de référence à partir d&rsquo;une image RGB-D d&rsquo;un objet sans défaut. Ensuite, pour toute vue d&rsquo;un objet inconnu (dit &ldquo;de test&rdquo;), nous devons le segmenter à partir de l&rsquo;arrière-plan et le comparer avec le modèle de référence. Ce processus de vérification de la forme doit être indépendant du point de vue et,
exige donc l&rsquo;enregistrement de chaque nuage de points associé par rapport à celui de référence.</p><div class="alert alert-info"><strong><p>Nous proposons de décomposer cet objectif en trois étapes :</p><ul><li><a href=#anchor-step-1>étape 1</a> : <strong>extraire les points 3D</strong> des objets à comparer (à la fois objets de référence et de test) en supprimant tous les points de la scène n&rsquo;appartenant pas à l&rsquo;objet. Pour éviter un processus redondant, cette étape sera à réaliser seulement sur la scène de référence contenue dans <code>data01.xyz</code> ; cela a déjà été réalisé sur les objets à contrôler, et stocké dans les fichiers <code>data02_object.xyz</code> et <code>data03_object.xyz</code>.</li><li><a href=#anchor-step-2>étape 2</a> : <strong>enregistrer</strong> les points de chaque objet de test vers le modèle de référence afin de les comparer <em>i.e.</em> aligner leurs nuages de points 3D respectifs sur le repère de coordonnées de référence.</li><li><a href=#anchor-step-3>étape 3</a> : <strong>comparer</strong> les modèles de contrôle et de référence et conclure sur les potentiels défauts des modèles de contrôle.</li></ul></strong></div><h2 id=etape-1--extraction-du-modèle-3d-de-la-scène-de-référence><em><strong>Etape 1</strong></em> : extraction du modèle 3D de la scène de référence</h2><p>La première étape du TP consiste à extraire le nuage de points du modèle de référence à partir de la scène RGB-D acquise avec une Kinect :</p><p><img src=images/extraction_objet.png alt="Extraction du modèle de référence" class=center><div style=margin-top:1rem></div></p><p>Cette étape vise à calquer une surface plane sur le plan du sol, et à ne garder que la boite du centre en calculant la distance de chacun de ses points par rapport à ce plan et en y appliquant un seuil de filtrage.</p><p>Pour cela, ouvrez CloudCompare (le logiciel principal, pas le viewer) et importez les points de la scène <code>data01.xyz</code>. Sélectionnez le nuage en cliquant dessus dans le <em>workspace</em>.
A l&rsquo;aide de l&rsquo;outil de segmentation (<strong>Edit > Segment</strong>, ou bien directement le raccourci &ldquo;ciseaux&rdquo; dans la barre des raccourcis), divisez le nuage en trois sous-ensembles afin d&rsquo;en extraire le plan du sol et une zone grossière autour de la boite.
Le résultat obtenu est illustré par la figure suivante :</p><p><img src=images/extraction_modele.gif alt="Division de la scène en trois nuages" class=center><div style=margin-top:1rem></div></p><div class="alert alert-warning"><strong><p>Dans CloudCompare, pour travailler sur un nuage de points, il faut que la ligne lui correspondant soit sélectionnée dans le <em>workspace</em>. Vous savez si le nuage est sélectionné lorsqu&rsquo;une boite jaune s&rsquo;affiche autour.</p><p>Le fait de cocher la case ne sélectionne pas le nuage, elle le rend simplement visible/invisible dans l&rsquo;affichage.</p></strong></div><p>Créez une surface calquée sur le nuage du plan du sol à l&rsquo;aide de l&rsquo;outil <strong>Tools > Fit > Plane</strong>.
En sélectionnant le plan nouvellement créé et le nuage qui contient la boite, il est maintenant possible de calculer, pour chacun des points de ce nuage, sa distance au plan à l&rsquo;aide de l&rsquo;outil <strong>Tools > Distances > Cloud/Mesh Distance</strong> :</p><p><img src=images/fit_plane_compute_distance.png alt="Surface au plan et distance" class=center><div style=margin-top:1rem></div></p><p>L&rsquo;outil de distance ajoute un quatrième champ à chacun des points du nuage : la distance nouvellement calculée. En allant dans les propriétés du nuage, filtrez les points par rapport à ce champ scalaire pour ne garder que les points appartenant à la boite :</p><p><img src=images/filter_by_value.gif alt="Filtrage par la distance au plan" class=center><div style=margin-top:1rem></div></p><p>En cliquant sur <em>split</em>, deux nuages sont créés, correspondant aux deux côtés du filtrage :</p><p><img src=images/split.png alt="Boite extraite" class=center><div style=margin-top:1rem></div></p><p>Assurez-vous que le nuage nouvellement créé contient environ 10,000 points (le nombre de points est accessible dans le panneau des propriétés sur la gauche).</p><p>Supprimez le champ scalaire de la distance <em>via</em> <strong>Edit > Scalar fields > Delete</strong>. Sélectionnez seulement le nuage de la boite avant de l&rsquo;enregistrer au format ASCII Cloud sous le nom <code>data01_segmented.xyz</code> dans le dossier <code>data</code> du TP.</p><div class="alert alert-info"><strong>Par précaution, sauvegardez votre projet CloudCompare : pensez à <strong>sélectionner tous les nuages de points</strong>, et à sauvegarder le projet au format CloudCompare.</strong></div><h2 id=etape-2--enregistrement-des-points-3d><em><strong>Etape 2</strong></em> : enregistrement des points 3D</h2><p>Si vous avez ouvert les scènes complètes <code>data02.xyz</code> et <code>data03.xyz</code> dans CloudCompare, vous aurez remarqué que chaque scène a été prise d&rsquo;un point de vue légèrement différent, et que les objets eux-mêmes ont bougé :</p><p><img src=images/points_vue_diff.gif alt="Points de vue différents des scènes" class=center><div style=margin-top:1rem></div></p><p>Pour pouvoir comparer les modèles entre eux, on propose de les superposer et de calculer leur distance cumulée point à point. Plus cette distance est faible, plus les modèles se superposent et se ressemblent ; plus elle est grande, plus les modèles diffèrent.
L&rsquo;exemple suivant montre la superposition du modèle correct sur le modèle de référence précédemment extrait :</p><p><img src=images/plot_after_icp.png alt="Application d'ICP au modèle correct" class=center><div style=margin-top:1rem></div></p><p>Le fait de transformer les points d&rsquo;un modèle via une matrice de rotation/translation pour venir le superposer sur un autre nuage s&rsquo;appelle <em>l&rsquo;enregistrement des points</em>.
L&rsquo;algorithme <em><strong>Iterative Closest Point</strong></em> permet cet enregistrement, et nous proposons de l&rsquo;utiliser en Python.
Le code à modifier se situe uniquement dans <code>qualitycheck.py</code>, l&rsquo;objectif étant d&rsquo;appliquer ICP à la fois sur le modèle correct <code>data02_object.xyz</code>, et sur le modèle défectueux <code>data03_object.xyz</code>.</p><h3 id=chargement-des-modèles>Chargement des modèles</h3><p>La première partie du code charge les modèles <code>.xyz</code> extraits avec CloudCompare, stocke le modèle de référence dans la variable <code>ref</code> et le modèle à comparer dans la variable <code>data</code>.
Pour exécuter le code soit sur <code>data02_object</code>, soit sur <code>data03_object</code>, il suffit de commenter la ligne correspondante.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Load pre-processed model point cloud</span>
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Extracting MODEL object...&#34;</span>)
model <span style=color:#f92672>=</span> datatools<span style=color:#f92672>.</span>load_XYZ_data_to_vec(<span style=color:#e6db74>&#39;data/data01_segmented.xyz&#39;</span>)

<span style=color:#75715e># Load raw data point cloud</span>
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Extracting DATA02 object...&#34;</span>)
data02_object <span style=color:#f92672>=</span> datatools<span style=color:#f92672>.</span>load_XYZ_data_to_vec(<span style=color:#e6db74>&#39;data/data02_object.xyz&#39;</span>)

<span style=color:#75715e># Load raw data point cloud</span>
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Extracting DATA03 object...&#34;</span>)
data03_object <span style=color:#f92672>=</span> datatools<span style=color:#f92672>.</span>load_XYZ_data_to_vec(<span style=color:#e6db74>&#39;data/data03_object.xyz&#39;</span>)

ref <span style=color:#f92672>=</span> model_object
data <span style=color:#f92672>=</span> data02_object
<span style=color:#75715e># data = data03_object</span>
</code></pre></div><h3 id=appel-à-icp>Appel à ICP</h3><p>La deuxième partie du code consiste à coder l&rsquo;appel à la fonction <code>icp</code> de la librairie <code>icp</code>&mldr;</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e>##########################################################################</span>
<span style=color:#75715e># Run ICP to get data transformation w.r.t the model, final error and execution time</span>

<span style=color:#75715e>#**************** To be completed ****************</span>
T <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>eye(<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>4</span>)
errors <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>100</span>))
iterations <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
total_time<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
<span style=color:#75715e>#*************************************************</span>

<span style=color:#75715e># Draw results</span>
fig <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>figure(<span style=color:#ae81ff>1</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>5</span>))
ax <span style=color:#f92672>=</span> fig<span style=color:#f92672>.</span>add_subplot(<span style=color:#ae81ff>131</span>, projection<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;3d&#39;</span>)
<span style=color:#75715e># Draw reference</span>
datatools<span style=color:#f92672>.</span>draw_data(ref, title<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Reference&#39;</span>, ax<span style=color:#f92672>=</span>ax)

ax <span style=color:#f92672>=</span> fig<span style=color:#f92672>.</span>add_subplot(<span style=color:#ae81ff>132</span>, projection<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;3d&#39;</span>)
<span style=color:#75715e># Draw original data and reference</span>
datatools<span style=color:#f92672>.</span>draw_data_and_ref(data, ref<span style=color:#f92672>=</span>ref, title<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Raw data&#39;</span>, ax<span style=color:#f92672>=</span>ax)
</code></pre></div><p>&mldr;et à stocker le retour de la fonction dans les variables <code>T</code>, <code>errors</code>, <code>iterations</code> et <code>total_time</code> comme défini par l&rsquo;en-tête de définition de la fonction dans le fichier <code>icp.py</code> :</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>icp</span>(data, ref, init_pose<span style=color:#f92672>=</span>None, max_iterations<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>, tolerance<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>):
    <span style=color:#e6db74>&#39;&#39;&#39;
</span><span style=color:#e6db74>    The Iterative Closest Point method: finds best-fit transform that maps points A on to points B
</span><span style=color:#e6db74>    Input:
</span><span style=color:#e6db74>        A: Nxm numpy array of source mD points
</span><span style=color:#e6db74>        B: Nxm numpy array of destination mD point
</span><span style=color:#e6db74>        init_pose: (m+1)x(m+1) homogeneous transformation
</span><span style=color:#e6db74>        max_iterations: exit algorithm after max_iterations
</span><span style=color:#e6db74>        tolerance: convergence criteria
</span><span style=color:#e6db74>    Output:
</span><span style=color:#e6db74>        T: final homogeneous transformation that maps A on to B
</span><span style=color:#e6db74>        errors: Euclidean distances (errors) for max_iterations iterations in a (max_iterations+1) vector. distances[0] is the initial distance.
</span><span style=color:#e6db74>        i: number of iterations to converge
</span><span style=color:#e6db74>        total_time : execution time
</span><span style=color:#e6db74>    &#39;&#39;&#39;</span>
</code></pre></div><h3 id=transformation-du-modèle>Transformation du modèle</h3><p>La matrice <code>T</code> de transformation issue d&rsquo;ICP est la matrice de passage homogène permettant de calquer le modèle <code>data</code>, passé en paramètre de la fonction <code>icp</code>, sur le modèle <code>ref</code>.
Pour rappel, l&rsquo;application d&rsquo;une matrice homogène pour transformer un ensemble de points d&rsquo;un repère initial \(\mathcal{R_i}\) vers un repère final \(\mathcal{R_f}\) s&rsquo;effectue de la manière suivante :</p><p>$$P_f^{(4 \times N)} = T^{(4 \times 4)} . P_i^{(4 \times N)}$$</p><p>Dans le code, la troisième partie consiste donc à appliquer la matrice de transformation au modèle à comparer. Un exemple de l&rsquo;application d&rsquo;une matrice de rotation homogène à une autre matrice est donné ci-après :</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>  EXAMPLE: How to transform a 3D matrix with a rotation on its x-axis:
</span><span style=color:#e6db74>&#34;&#34;&#34;</span>
<span style=color:#75715e># Construct a homogeneous matrix from the original one</span>
homogeneous <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>ones((original<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], <span style=color:#ae81ff>4</span>))
homogeneous[:,:<span style=color:#ae81ff>3</span>] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>copy(original)

<span style=color:#75715e># Define the rotation matrix</span>
theta <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>radians(<span style=color:#ae81ff>36</span>)
c, s <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>cos(theta), np<span style=color:#f92672>.</span>sin(theta)
rotation_matrix <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>,  <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>),
                             <span style=color:#ae81ff>0</span>, c, <span style=color:#f92672>-</span>s, <span style=color:#ae81ff>0</span>),
                             <span style=color:#ae81ff>0</span>, s,  c, <span style=color:#ae81ff>0</span>),
                             <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>,  <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>)))

<span style=color:#75715e># Apply the rotation to the original point cloud</span>
rotated_matrix <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(rotation_matrix, homogeneous<span style=color:#f92672>.</span>T)<span style=color:#f92672>.</span>T

<span style=color:#75715e># Delete the homogeneous coordinate to get back to the original shape</span>
rotated_matrix <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>delete(homogeneous, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>)
</code></pre></div><div class="alert alert-info"><strong><p>La variable <code>original</code> est un tableau de taille \(N \times 3\), \(N\) étant le nombre de points du modèle et 3 ses coordonnées \(X\), \(Y\) et \(Z\).</p><p>Il faut veiller à lui ajouter une coordonnée homogène et appliquer les transposées nécessaires pour que la multiplication de matrices fonctionne.
Aidez-vous de l&rsquo;exemple donné dans le code pour réaliser cette étape.</p></strong></div><p>Vous pouvez ensuite afficher le résultat en décommentant et complétant la ligne <code>datatools.draw_data...</code>.</p><h3 id=affichage-de-lerreur>Affichage de l&rsquo;erreur</h3><p>Décommentez et affichez l&rsquo;erreur dans la dernière partie du code, en changeant les &ldquo;&mldr;&rdquo; par les variables correspondantes :</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Display error progress over time</span>
<span style=color:#75715e># **************** To be uncommented and completed ****************</span>
<span style=color:#75715e># fig1 = plt.figure(2, figsize=(20, 3))</span>
<span style=color:#75715e># it = np.arange(0, len(errors), 1)</span>
<span style=color:#75715e># plt.plot(it, ...)</span>
<span style=color:#75715e># plt.ylabel(&#39;Residual distance&#39;)</span>
<span style=color:#75715e># plt.xlabel(&#39;Iterations&#39;)</span>
<span style=color:#75715e># plt.title(&#39;Total elapsed time :&#39; + str(...) + &#39; s.&#39;)</span>
<span style=color:#75715e># plt.show()</span>
</code></pre></div><h2 id=etape-3--comparaison-des-modèles><em><strong>Etape 3</strong></em> : comparaison des modèles</h2><p>Comparez l&rsquo;application d&rsquo;ICP sur les modèles <code>data02</code> et <code>data03</code>, remarquez l&rsquo;évolution de l&rsquo;erreur et les différences de valeurs.
Que représente cette erreur ? Que peut-on dire des deux modèles ? En vous appuyant sur les erreurs, quel seuil de décision pourriez-vous choisir pour déterminer si un modèle est défectueux ou non ?</p><h3 id=icp-dans-cloudcompare>ICP dans CloudCompare</h3><p>L&rsquo;algorithme ICP peut également être utilisé directement dans CloudCompare. Ouvrez-le et importez <code>data01_segmented.xyz</code>, <code>data02_object.xyz</code> et <code>data03_object.xyz</code>.</p><p>Sélectionnez par exemple les nuages de <code>data01_segmented</code> et <code>data02_object</code>, utilisez l&rsquo;outil <strong>Tools > Registration > Fine registration (ICP)</strong>. Assurez-vous que la référence est bien <code>data01</code> et appliquez ICP.
Son exécution vous renvoie la matrice de transformation calculée par l&rsquo;algorithme, et l&rsquo;applique à l&rsquo;objet.</p><p><img src=images/registration_cc.png alt="ICP dans CloudCompare" class=center><div style=margin-top:1rem></div></p><p>On peut ainsi, toujours en sélectionnant les deux nuages, calculer la distance entre les points avec <strong>Tools > Distance > Cloud/Cloud Distance</strong>. Assurez-vous que la référence est bien <code>data01</code> et cliquez sur OK/Compute/OK.
Sélectionnez <code>data02_object</code> et affichez l&rsquo;histogramme de ses distances au nuage de référence <em>via</em> <strong>Edit > Scalar fields > Show histogram</strong>.</p><p><img src=images/histogram_dists.png alt="Histogramme des distances point à point de data02 vs. data01" class=center><div style=margin-top:1rem></div></p><p>Faites la même chose avec <code>data03_object</code> et comparez les histogrammes. Comment les interprétez-vous ? Comment pouvez-vous les comparer ?</p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/clairelabitbonis/clairelabitbonis.github.io/edit/main/content/posts/teaching/3d_perception/cc_segmentation/index.fr.md title="Améliorez cette page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>Améliorez cette page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/fr/posts/teaching/3d_perception/overview_perception/ title="Aperçu général" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>Précédent</div><div class=next-prev-text>Aperçu général</div></a></div><div class="col-md-6 next-article"><a href=/fr/posts/teaching/3d_perception/monocular_localization/ title="Localisation monoculaire par PnL itérative" class="btn btn-outline-info"><div>Suivant <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Localisation monoculaire par PnL itérative</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a><div class="dropdown languageSelector"><a class="btn dropdown-toggle" href=# id=languageSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-fr"></span>Français</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/posts/teaching/3d_perception/cc_segmentation><span class="flag-icon flag-icon-gb"></span>English</a></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table des matières</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#anchor-step-0>Définition des objectifs</a></li><li><a href=#etape-1--extraction-du-modèle-3d-de-la-scène-de-référence><em><strong>Etape 1</strong></em> : extraction du modèle 3D de la scène de référence</a></li><li><a href=#etape-2--enregistrement-des-points-3d><em><strong>Etape 2</strong></em> : enregistrement des points 3D</a><ul><li><a href=#chargement-des-modèles>Chargement des modèles</a></li><li><a href=#appel-à-icp>Appel à ICP</a></li><li><a href=#transformation-du-modèle>Transformation du modèle</a></li><li><a href=#affichage-de-lerreur>Affichage de l&rsquo;erreur</a></li></ul></li><li><a href=#etape-3--comparaison-des-modèles><em><strong>Etape 3</strong></em> : comparaison des modèles</a><ul><li><a href=#icp-dans-cloudcompare>ICP dans CloudCompare</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Alimenté par
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript src=/js/darkreader.js></script><script type=text/javascript src=/js/darkmode-darkreader.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=stylesheet href=/katex/katex.min.css><script type=text/javascript defer src=/katex/katex.min.js></script><script type=text/javascript defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body);>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"\\[",right:"\\]",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false}]});</script></body></html>